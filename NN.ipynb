{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c4008ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Don't mind about this \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7be4216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_trining = pd.read_csv(\"training_features.txt\")\n",
    "Y_train = pd.read_csv(\"training_labels.txt\")\n",
    "X_test = pd.read_csv(\"test_features.txt\")\n",
    "Y_test = pd.read_csv(\"test_labels.txt\")\n",
    "X_trining['label']=Y_train\n",
    "X_trining=shuffle(X_trining)\n",
    "Y_train=X_trining['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "784f877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr= np.array(X_trining)\n",
    "X_train_arr=X_train_arr[:,:-1]\n",
    "X_test_arr= np.array(X_test)\n",
    "Y_train_arr= np.array(Y_train)\n",
    "Y_test_arr= np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0769a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_y=np.zeros((Y.size, Y.max()))  \n",
    "    one_hot_y[np.arange(Y.size), Y-1] = 1\n",
    "    one_hot_y= one_hot_y.T\n",
    "    return one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b7ef2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-1*z)))\n",
    "\n",
    "    def _diff_sigmoid(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "\n",
    "    def _tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def _diff_tanh(self, z):\n",
    "        return 1-np.square(self._tanh(z))\n",
    "\n",
    "    def relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def _diff_relu(self, z):\n",
    "        a= np.zeros_like(z,dtype='int')\n",
    "        a[z>0] = 1\n",
    "        return a\n",
    "\n",
    "    def softmax(self,z):\n",
    "        exp = np.exp(z)\n",
    "        tot= exp.sum(axis=0)\n",
    "        t= exp/tot\n",
    "        return t\n",
    "    \n",
    "    def _diff_softmax(self, z,y):\n",
    "        yhat_r = self.softmax(z)\n",
    "        one_yi = y *-1*(1-yhat_r)\n",
    "        z=(1-y)*yhat_r\n",
    "        return one_yi +z\n",
    "\n",
    "    \n",
    "    def __init__(self,n_neurons,n_inputs, activation='relu'):\n",
    "        self.n_neurons = n_neurons #number of neurons \n",
    "        self.n_inputs = n_inputs #past number of samples\n",
    "        self.activation_name = activation\n",
    "\n",
    "        self.W= np.random.randn(self.n_neurons,self.n_inputs)*np.sqrt(2/self.n_inputs)\n",
    "        self.b= np.random.randn(self.n_neurons,1)*np.sqrt(2/self.n_inputs)\n",
    "\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    \n",
    "    def _set_target(self, t):\n",
    "        self.target=t\n",
    "\n",
    "    def forward(self,Ai):\n",
    "        z = np.dot(self.W,Ai)+ self.b\n",
    "        if (self.activation_name=='relu'):\n",
    "            A = self.relu(z)\n",
    "        elif(self.activation_name=='softmax'):\n",
    "            A = self.softmax(z)\n",
    "        else:\n",
    "            A = self._tanh(z)\n",
    "        self.Ai = Ai\n",
    "        self.Z = z\n",
    "        return A\n",
    "    \n",
    "    def backward(self,inp):\n",
    "        if (self.activation_name=='relu'):\n",
    "            act_diff = self._diff_relu(self.Z)\n",
    "        elif(self.activation_name=='softmax'):\n",
    "            act_diff = self._diff_softmax(self.Z,self.target)\n",
    "        else:\n",
    "            act_diff = self._diff_tanh(self.Z) \n",
    "        e = np.ones((self.Ai.shape[1],1))\n",
    "        bet= inp * act_diff\n",
    "        self.dW= self.dW+ np.dot(bet,self.Ai.T)\n",
    "        self.db= self.db+ np.dot(bet,e)\n",
    "        return np.dot(self.W.T, bet)\n",
    "    \n",
    "    def zeroing_diff(self):\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65546f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,lr):\n",
    "        self.alpha = lr\n",
    "        self.layers = []\n",
    "        self.t_loss=[]\n",
    "        self.v_loss=[]\n",
    "\n",
    "    def forward(self,X):\n",
    "        a = X   \n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self,inp):\n",
    "        gd = inp\n",
    "        for layer in self.layers[::-1]:\n",
    "            gd = layer.backward(gd)\n",
    "            \n",
    "    def add_layer(self,n_neurons,n_inputs,act):\n",
    "        self.layers.append(Layer(n_neurons,n_inputs,act))\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        onehotY= y\n",
    "        yhat_r = np.max(onehotY*y_hat, axis=0,keepdims=True)\n",
    "        return (1/(y.shape[1]))*-1*np.sum(np.log(yhat_r))\n",
    "    \n",
    "    def fit(self,x_train,y_train,x_val, y_val, epochs=5):\n",
    "        M = x_train.shape[1]\n",
    "        for i in range(epochs):\n",
    "            y_hat = self.forward(x_train)       \n",
    "            gd=self.loss(y_train, y_hat)\n",
    "            print(\"Training loss: {}....\".format(gd))\n",
    "            self.t_loss.append(gd)\n",
    "            self.layers[-1]._set_target(y_train)\n",
    "            self.backward(1)   \n",
    "            # apply GD\n",
    "            for i in range(len(self.layers)):\n",
    "                self.layers[i].W = self.layers[i].W - (self.alpha * (self.layers[i].dW/M))\n",
    "                self.layers[i].b = self.layers[i].b - (self.alpha * (self.layers[i].db/M))\n",
    "            \n",
    "            y_hat_v = self.forward(x_val)\n",
    "            gd=self.loss(y_val, y_hat_v)\n",
    "            self.v_loss.append(gd)\n",
    "            print(\"Validation loss: {}....\".format(gd))\n",
    "            print(\"-----------------------------------\")       \n",
    "            \n",
    "                \n",
    "            for layer in self.layers:\n",
    "                layer.zeroing_diff()\n",
    "        return self.t_loss, self.v_loss\n",
    "                \n",
    "    def predict(self,data): \n",
    "        y_hat= self.forward(data)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93193286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model,X,Y_train_arr,n=3):\n",
    "    skfolds = StratifiedKFold(n_splits=n)\n",
    "    for train_index, test_index in skfolds.split(X,Y_train_arr):\n",
    "        X_train_folds = np.array(X[train_index]).T\n",
    "        y_train_folds = one_hot(np.array(Y_train_arr[train_index]).T)\n",
    "        X_val_fold = np.array(X[test_index]).T\n",
    "        y_val_fold = np.array(Y_train_arr[test_index])\n",
    "        Y_val_arr_one_hot= one_hot(y_val_fold)\n",
    "        train_loss , valid_loss = model.fit(X_train_folds,y_train_folds,X_val_fold,Y_val_arr_one_hot, epochs=750)\n",
    "        Y_predicted = model.predict(X_val_fold)\n",
    "        numbs=[]\n",
    "        for i in range(len(y_val_fold)):\n",
    "            s=np.argmax(Y_predicted[:,i])\n",
    "            s+=1\n",
    "            numbs.append(s)\n",
    "    return train_loss,valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6565c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.5374994383733256....\n",
      "Validation loss: 2.5487260374819076....\n",
      "-----------------------------------\n",
      "Training loss: 2.5282529472522564....\n",
      "Validation loss: 2.5399340300992783....\n",
      "-----------------------------------\n",
      "Training loss: 2.519244061107889....\n",
      "Validation loss: 2.531358113121786....\n",
      "-----------------------------------\n",
      "Training loss: 2.510453963647212....\n",
      "Validation loss: 2.5229723750498088....\n",
      "-----------------------------------\n",
      "Training loss: 2.5018643879623634....\n",
      "Validation loss: 2.5147852534122204....\n",
      "-----------------------------------\n",
      "Training loss: 2.493479393734405....\n",
      "Validation loss: 2.506776345919246....\n",
      "-----------------------------------\n",
      "Training loss: 2.4852961971321874....\n",
      "Validation loss: 2.4989440964132865....\n",
      "-----------------------------------\n",
      "Training loss: 2.4772971859843365....\n",
      "Validation loss: 2.491276008616149....\n",
      "-----------------------------------\n",
      "Training loss: 2.4694710996152445....\n",
      "Validation loss: 2.483752644631275....\n",
      "-----------------------------------\n",
      "Training loss: 2.4618018153852863....\n",
      "Validation loss: 2.4763951412200336....\n",
      "-----------------------------------\n",
      "Training loss: 2.4542934962882272....\n",
      "Validation loss: 2.469184270778855....\n",
      "-----------------------------------\n",
      "Training loss: 2.4469285739421447....\n",
      "Validation loss: 2.4621105702971864....\n",
      "-----------------------------------\n",
      "Training loss: 2.439701881849007....\n",
      "Validation loss: 2.4551788966113217....\n",
      "-----------------------------------\n",
      "Training loss: 2.4326191511050053....\n",
      "Validation loss: 2.4483718469399838....\n",
      "-----------------------------------\n",
      "Training loss: 2.425671466640568....\n",
      "Validation loss: 2.4416823355394213....\n",
      "-----------------------------------\n",
      "Training loss: 2.4188454646636974....\n",
      "Validation loss: 2.435104724803068....\n",
      "-----------------------------------\n",
      "Training loss: 2.412136776801172....\n",
      "Validation loss: 2.428644742599896....\n",
      "-----------------------------------\n",
      "Training loss: 2.405546524632969....\n",
      "Validation loss: 2.422288073734449....\n",
      "-----------------------------------\n",
      "Training loss: 2.3990618398224357....\n",
      "Validation loss: 2.416036636791216....\n",
      "-----------------------------------\n",
      "Training loss: 2.392681500547504....\n",
      "Validation loss: 2.4098844752801214....\n",
      "-----------------------------------\n",
      "Training loss: 2.3864049400609355....\n",
      "Validation loss: 2.403825732428403....\n",
      "-----------------------------------\n",
      "Training loss: 2.3802298580224406....\n",
      "Validation loss: 2.3978553996596323....\n",
      "-----------------------------------\n",
      "Training loss: 2.3741487427794357....\n",
      "Validation loss: 2.39197321456567....\n",
      "-----------------------------------\n",
      "Training loss: 2.3681530539956666....\n",
      "Validation loss: 2.3861807170127305....\n",
      "-----------------------------------\n",
      "Training loss: 2.362247761117951....\n",
      "Validation loss: 2.3804814888859798....\n",
      "-----------------------------------\n",
      "Training loss: 2.3564352195963956....\n",
      "Validation loss: 2.374871666465034....\n",
      "-----------------------------------\n",
      "Training loss: 2.3507144154178934....\n",
      "Validation loss: 2.3693435137968644....\n",
      "-----------------------------------\n",
      "Training loss: 2.3450864970913936....\n",
      "Validation loss: 2.363891296234585....\n",
      "-----------------------------------\n",
      "Training loss: 2.339529537079493....\n",
      "Validation loss: 2.358504705393149....\n",
      "-----------------------------------\n",
      "Training loss: 2.3340408005289843....\n",
      "Validation loss: 2.3531906721301277....\n",
      "-----------------------------------\n",
      "Training loss: 2.328633557196564....\n",
      "Validation loss: 2.34793425967037....\n",
      "-----------------------------------\n",
      "Training loss: 2.323286651030011....\n",
      "Validation loss: 2.3427348558097....\n",
      "-----------------------------------\n",
      "Training loss: 2.3179977373132967....\n",
      "Validation loss: 2.337595700918645....\n",
      "-----------------------------------\n",
      "Training loss: 2.3127702725090575....\n",
      "Validation loss: 2.3325175326393874....\n",
      "-----------------------------------\n",
      "Training loss: 2.3076073467378735....\n",
      "Validation loss: 2.327502853027737....\n",
      "-----------------------------------\n",
      "Training loss: 2.302507320219131....\n",
      "Validation loss: 2.3225447630169236....\n",
      "-----------------------------------\n",
      "Training loss: 2.2974693536880713....\n",
      "Validation loss: 2.317656421251004....\n",
      "-----------------------------------\n",
      "Training loss: 2.2924984354255318....\n",
      "Validation loss: 2.3128274142073635....\n",
      "-----------------------------------\n",
      "Training loss: 2.287587553859183....\n",
      "Validation loss: 2.3080533502350935....\n",
      "-----------------------------------\n",
      "Training loss: 2.282726706774717....\n",
      "Validation loss: 2.303340785236081....\n",
      "-----------------------------------\n",
      "Training loss: 2.277920927103441....\n",
      "Validation loss: 2.298692742525181....\n",
      "-----------------------------------\n",
      "Training loss: 2.2731735059845724....\n",
      "Validation loss: 2.294096311222453....\n",
      "-----------------------------------\n",
      "Training loss: 2.2684778038928....\n",
      "Validation loss: 2.289554641132483....\n",
      "-----------------------------------\n",
      "Training loss: 2.263835727565794....\n",
      "Validation loss: 2.2850658481212576....\n",
      "-----------------------------------\n",
      "Training loss: 2.2592503891002393....\n",
      "Validation loss: 2.280631615397022....\n",
      "-----------------------------------\n",
      "Training loss: 2.254715823159138....\n",
      "Validation loss: 2.27624399314137....\n",
      "-----------------------------------\n",
      "Training loss: 2.2502181224217....\n",
      "Validation loss: 2.27189170191414....\n",
      "-----------------------------------\n",
      "Training loss: 2.245759602311397....\n",
      "Validation loss: 2.2675795385614106....\n",
      "-----------------------------------\n",
      "Training loss: 2.2413440097342154....\n",
      "Validation loss: 2.2633159367778073....\n",
      "-----------------------------------\n",
      "Training loss: 2.236976650919379....\n",
      "Validation loss: 2.259094782482879....\n",
      "-----------------------------------\n",
      "Training loss: 2.2326547913935664....\n",
      "Validation loss: 2.2549221102280472....\n",
      "-----------------------------------\n",
      "Training loss: 2.2283838326793886....\n",
      "Validation loss: 2.250792131788213....\n",
      "-----------------------------------\n",
      "Training loss: 2.224155198777977....\n",
      "Validation loss: 2.2467007562468457....\n",
      "-----------------------------------\n",
      "Training loss: 2.21997140450879....\n",
      "Validation loss: 2.2426453583036543....\n",
      "-----------------------------------\n",
      "Training loss: 2.215824317833276....\n",
      "Validation loss: 2.238619831858409....\n",
      "-----------------------------------\n",
      "Training loss: 2.21171224819413....\n",
      "Validation loss: 2.234628540827056....\n",
      "-----------------------------------\n",
      "Training loss: 2.207638029322937....\n",
      "Validation loss: 2.230674852921442....\n",
      "-----------------------------------\n",
      "Training loss: 2.2036009445965052....\n",
      "Validation loss: 2.2267603792719703....\n",
      "-----------------------------------\n",
      "Training loss: 2.1996047478020073....\n",
      "Validation loss: 2.2228793953441732....\n",
      "-----------------------------------\n",
      "Training loss: 2.1956414261574824....\n",
      "Validation loss: 2.2190234049266517....\n",
      "-----------------------------------\n",
      "Training loss: 2.191706063379674....\n",
      "Validation loss: 2.2151967818886122....\n",
      "-----------------------------------\n",
      "Training loss: 2.18780158409442....\n",
      "Validation loss: 2.211405948773606....\n",
      "-----------------------------------\n",
      "Training loss: 2.183928782108592....\n",
      "Validation loss: 2.2076382455384738....\n",
      "-----------------------------------\n",
      "Training loss: 2.1800806608959715....\n",
      "Validation loss: 2.2038982809982115....\n",
      "-----------------------------------\n",
      "Training loss: 2.176258858144416....\n",
      "Validation loss: 2.2001837041544015....\n",
      "-----------------------------------\n",
      "Training loss: 2.172456763941315....\n",
      "Validation loss: 2.1964948639800506....\n",
      "-----------------------------------\n",
      "Training loss: 2.1686783060900425....\n",
      "Validation loss: 2.192827108982384....\n",
      "-----------------------------------\n",
      "Training loss: 2.1649219319514543....\n",
      "Validation loss: 2.1891845271475168....\n",
      "-----------------------------------\n",
      "Training loss: 2.1611912757508245....\n",
      "Validation loss: 2.1855690174754554....\n",
      "-----------------------------------\n",
      "Training loss: 2.157485719332373....\n",
      "Validation loss: 2.181975219637587....\n",
      "-----------------------------------\n",
      "Training loss: 2.153804885171389....\n",
      "Validation loss: 2.1784033269743786....\n",
      "-----------------------------------\n",
      "Training loss: 2.150146667629622....\n",
      "Validation loss: 2.174851307417796....\n",
      "-----------------------------------\n",
      "Training loss: 2.1465121466843518....\n",
      "Validation loss: 2.171330800701454....\n",
      "-----------------------------------\n",
      "Training loss: 2.142906742590613....\n",
      "Validation loss: 2.167835227597446....\n",
      "-----------------------------------\n",
      "Training loss: 2.1393223766444898....\n",
      "Validation loss: 2.1643613036165927....\n",
      "-----------------------------------\n",
      "Training loss: 2.135759274763555....\n",
      "Validation loss: 2.160905715104147....\n",
      "-----------------------------------\n",
      "Training loss: 2.1322147190518232....\n",
      "Validation loss: 2.1574715557755626....\n",
      "-----------------------------------\n",
      "Training loss: 2.1286913406978587....\n",
      "Validation loss: 2.154054879591131....\n",
      "-----------------------------------\n",
      "Training loss: 2.125187469254059....\n",
      "Validation loss: 2.1506555761189636....\n",
      "-----------------------------------\n",
      "Training loss: 2.12169922139155....\n",
      "Validation loss: 2.1472778177998806....\n",
      "-----------------------------------\n",
      "Training loss: 2.118228013923817....\n",
      "Validation loss: 2.143920940130159....\n",
      "-----------------------------------\n",
      "Training loss: 2.1147754362640816....\n",
      "Validation loss: 2.1405837317347087....\n",
      "-----------------------------------\n",
      "Training loss: 2.1113430600746828....\n",
      "Validation loss: 2.137265714361944....\n",
      "-----------------------------------\n",
      "Training loss: 2.1079301637343426....\n",
      "Validation loss: 2.1339591755043403....\n",
      "-----------------------------------\n",
      "Training loss: 2.104531578005925....\n",
      "Validation loss: 2.130669877884555....\n",
      "-----------------------------------\n",
      "Training loss: 2.101151275195212....\n",
      "Validation loss: 2.1274018709500706....\n",
      "-----------------------------------\n",
      "Training loss: 2.097787985081115....\n",
      "Validation loss: 2.1241495611311874....\n",
      "-----------------------------------\n",
      "Training loss: 2.094437205049478....\n",
      "Validation loss: 2.120907591969167....\n",
      "-----------------------------------\n",
      "Training loss: 2.0911012983398254....\n",
      "Validation loss: 2.1176814000697486....\n",
      "-----------------------------------\n",
      "Training loss: 2.0877834375096693....\n",
      "Validation loss: 2.114470155735362....\n",
      "-----------------------------------\n",
      "Training loss: 2.084478729747605....\n",
      "Validation loss: 2.111272040303531....\n",
      "-----------------------------------\n",
      "Training loss: 2.081187084018705....\n",
      "Validation loss: 2.1080882132234513....\n",
      "-----------------------------------\n",
      "Training loss: 2.0779113822914....\n",
      "Validation loss: 2.1049211356004016....\n",
      "-----------------------------------\n",
      "Training loss: 2.074650981394041....\n",
      "Validation loss: 2.1017648656168895....\n",
      "-----------------------------------\n",
      "Training loss: 2.0714008277129023....\n",
      "Validation loss: 2.098620692574345....\n",
      "-----------------------------------\n",
      "Training loss: 2.0681654485840206....\n",
      "Validation loss: 2.0954894306777043....\n",
      "-----------------------------------\n",
      "Training loss: 2.064946136557459....\n",
      "Validation loss: 2.09237019760144....\n",
      "-----------------------------------\n",
      "Training loss: 2.061739822723604....\n",
      "Validation loss: 2.0892617885416143....\n",
      "-----------------------------------\n",
      "Training loss: 2.0585434229983717....\n",
      "Validation loss: 2.0861631044786235....\n",
      "-----------------------------------\n",
      "Training loss: 2.0553592197374377....\n",
      "Validation loss: 2.083075316637832....\n",
      "-----------------------------------\n",
      "Training loss: 2.052187148158245....\n",
      "Validation loss: 2.0799934679340186....\n",
      "-----------------------------------\n",
      "Training loss: 2.049023065170811....\n",
      "Validation loss: 2.0769158905025416....\n",
      "-----------------------------------\n",
      "Training loss: 2.045866225088099....\n",
      "Validation loss: 2.073849440937974....\n",
      "-----------------------------------\n",
      "Training loss: 2.0427200174868645....\n",
      "Validation loss: 2.0707892442016327....\n",
      "-----------------------------------\n",
      "Training loss: 2.039583185910523....\n",
      "Validation loss: 2.0677358825413563....\n",
      "-----------------------------------\n",
      "Training loss: 2.036456928470717....\n",
      "Validation loss: 2.0646925918846737....\n",
      "-----------------------------------\n",
      "Training loss: 2.0333430487587436....\n",
      "Validation loss: 2.061659336472698....\n",
      "-----------------------------------\n",
      "Training loss: 2.030242089175888....\n",
      "Validation loss: 2.05863632395585....\n",
      "-----------------------------------\n",
      "Training loss: 2.0271503973884744....\n",
      "Validation loss: 2.055617314818019....\n",
      "-----------------------------------\n",
      "Training loss: 2.0240644416512157....\n",
      "Validation loss: 2.0526064611422594....\n",
      "-----------------------------------\n",
      "Training loss: 2.0209877214746896....\n",
      "Validation loss: 2.049604971313059....\n",
      "-----------------------------------\n",
      "Training loss: 2.0179192492939557....\n",
      "Validation loss: 2.0466080678043075....\n",
      "-----------------------------------\n",
      "Training loss: 2.014856825031247....\n",
      "Validation loss: 2.0436168720004666....\n",
      "-----------------------------------\n",
      "Training loss: 2.0118027589841945....\n",
      "Validation loss: 2.0406288375951998....\n",
      "-----------------------------------\n",
      "Training loss: 2.008754549913014....\n",
      "Validation loss: 2.037645890091862....\n",
      "-----------------------------------\n",
      "Training loss: 2.005710342507855....\n",
      "Validation loss: 2.034664765571217....\n",
      "-----------------------------------\n",
      "Training loss: 2.002669397125397....\n",
      "Validation loss: 2.031689351459915....\n",
      "-----------------------------------\n",
      "Training loss: 1.9996350201841524....\n",
      "Validation loss: 2.028721834172064....\n",
      "-----------------------------------\n",
      "Training loss: 1.9966086713689597....\n",
      "Validation loss: 2.025761321745059....\n",
      "-----------------------------------\n",
      "Training loss: 1.9935907188689215....\n",
      "Validation loss: 2.0228047169653625....\n",
      "-----------------------------------\n",
      "Training loss: 1.990580718140709....\n",
      "Validation loss: 2.0198525929756577....\n",
      "-----------------------------------\n",
      "Training loss: 1.9875776630389552....\n",
      "Validation loss: 2.0169068345265044....\n",
      "-----------------------------------\n",
      "Training loss: 1.9845802526727818....\n",
      "Validation loss: 2.013966190828499....\n",
      "-----------------------------------\n",
      "Training loss: 1.9815856300593921....\n",
      "Validation loss: 2.011025700676086....\n",
      "-----------------------------------\n",
      "Training loss: 1.9785941694476443....\n",
      "Validation loss: 2.0080901527187676....\n",
      "-----------------------------------\n",
      "Training loss: 1.9756084189401477....\n",
      "Validation loss: 2.005160077877327....\n",
      "-----------------------------------\n",
      "Training loss: 1.9726225750908433....\n",
      "Validation loss: 2.0022332156869522....\n",
      "-----------------------------------\n",
      "Training loss: 1.969635352420515....\n",
      "Validation loss: 1.999306376421048....\n",
      "-----------------------------------\n",
      "Training loss: 1.9666462072421715....\n",
      "Validation loss: 1.996383989088096....\n",
      "-----------------------------------\n",
      "Training loss: 1.9636604168113878....\n",
      "Validation loss: 1.9934643064591397....\n",
      "-----------------------------------\n",
      "Training loss: 1.960680131116654....\n",
      "Validation loss: 1.990547485994555....\n",
      "-----------------------------------\n",
      "Training loss: 1.957703463420972....\n",
      "Validation loss: 1.9876299879274935....\n",
      "-----------------------------------\n",
      "Training loss: 1.9547253200836507....\n",
      "Validation loss: 1.9847132562339083....\n",
      "-----------------------------------\n",
      "Training loss: 1.9517489532192742....\n",
      "Validation loss: 1.9817975769606664....\n",
      "-----------------------------------\n",
      "Training loss: 1.9487732426278133....\n",
      "Validation loss: 1.9788856902403218....\n",
      "-----------------------------------\n",
      "Training loss: 1.9458014404122674....\n",
      "Validation loss: 1.9759736422074545....\n",
      "-----------------------------------\n",
      "Training loss: 1.9428287956723678....\n",
      "Validation loss: 1.9730625352077482....\n",
      "-----------------------------------\n",
      "Training loss: 1.939858263157555....\n",
      "Validation loss: 1.9701538530241842....\n",
      "-----------------------------------\n",
      "Training loss: 1.9368910516909248....\n",
      "Validation loss: 1.96724428708797....\n",
      "-----------------------------------\n",
      "Training loss: 1.9339245624165335....\n",
      "Validation loss: 1.9643364393741074....\n",
      "-----------------------------------\n",
      "Training loss: 1.9309577563632412....\n",
      "Validation loss: 1.9614313041585454....\n",
      "-----------------------------------\n",
      "Training loss: 1.927991058881521....\n",
      "Validation loss: 1.9585301518510028....\n",
      "-----------------------------------\n",
      "Training loss: 1.9250275387073217....\n",
      "Validation loss: 1.9556291626151434....\n",
      "-----------------------------------\n",
      "Training loss: 1.9220625001101692....\n",
      "Validation loss: 1.952728290878305....\n",
      "-----------------------------------\n",
      "Training loss: 1.9190972905673527....\n",
      "Validation loss: 1.9498276068968774....\n",
      "-----------------------------------\n",
      "Training loss: 1.916129724555698....\n",
      "Validation loss: 1.9469272822200778....\n",
      "-----------------------------------\n",
      "Training loss: 1.9131632125940288....\n",
      "Validation loss: 1.9440276634924625....\n",
      "-----------------------------------\n",
      "Training loss: 1.9101987858983547....\n",
      "Validation loss: 1.9411304935691664....\n",
      "-----------------------------------\n",
      "Training loss: 1.9072376053880278....\n",
      "Validation loss: 1.9382329166748773....\n",
      "-----------------------------------\n",
      "Training loss: 1.9042768402355748....\n",
      "Validation loss: 1.9353371075833676....\n",
      "-----------------------------------\n",
      "Training loss: 1.9013181204844265....\n",
      "Validation loss: 1.932440860769251....\n",
      "-----------------------------------\n",
      "Training loss: 1.8983607410110195....\n",
      "Validation loss: 1.9295450499548095....\n",
      "-----------------------------------\n",
      "Training loss: 1.8954053291677702....\n",
      "Validation loss: 1.9266497464618628....\n",
      "-----------------------------------\n",
      "Training loss: 1.8924500229487795....\n",
      "Validation loss: 1.9237550199003004....\n",
      "-----------------------------------\n",
      "Training loss: 1.889493933767576....\n",
      "Validation loss: 1.9208620777443526....\n",
      "-----------------------------------\n",
      "Training loss: 1.8865381272887052....\n",
      "Validation loss: 1.9179678195344068....\n",
      "-----------------------------------\n",
      "Training loss: 1.8835802398065133....\n",
      "Validation loss: 1.915074088285237....\n",
      "-----------------------------------\n",
      "Training loss: 1.8806211716256938....\n",
      "Validation loss: 1.9121832687902343....\n",
      "-----------------------------------\n",
      "Training loss: 1.8776643448684078....\n",
      "Validation loss: 1.9092912660811325....\n",
      "-----------------------------------\n",
      "Training loss: 1.8747091415382595....\n",
      "Validation loss: 1.9063988156960243....\n",
      "-----------------------------------\n",
      "Training loss: 1.8717547080215085....\n",
      "Validation loss: 1.9035061225699719....\n",
      "-----------------------------------\n",
      "Training loss: 1.8688046448182445....\n",
      "Validation loss: 1.9006104251173492....\n",
      "-----------------------------------\n",
      "Training loss: 1.8658551652851656....\n",
      "Validation loss: 1.897717473387849....\n",
      "-----------------------------------\n",
      "Training loss: 1.8629100401162608....\n",
      "Validation loss: 1.8948221193421513....\n",
      "-----------------------------------\n",
      "Training loss: 1.859967133782339....\n",
      "Validation loss: 1.8919255079293293....\n",
      "-----------------------------------\n",
      "Training loss: 1.8570236960092277....\n",
      "Validation loss: 1.8890275228090578....\n",
      "-----------------------------------\n",
      "Training loss: 1.8540807765658684....\n",
      "Validation loss: 1.886128939023579....\n",
      "-----------------------------------\n",
      "Training loss: 1.8511376667552073....\n",
      "Validation loss: 1.8832292855331494....\n",
      "-----------------------------------\n",
      "Training loss: 1.848194679711087....\n",
      "Validation loss: 1.8803284628365315....\n",
      "-----------------------------------\n",
      "Training loss: 1.8452506628547418....\n",
      "Validation loss: 1.8774232652385452....\n",
      "-----------------------------------\n",
      "Training loss: 1.8423038691687241....\n",
      "Validation loss: 1.874517345557111....\n",
      "-----------------------------------\n",
      "Training loss: 1.8393554372409713....\n",
      "Validation loss: 1.871610689786342....\n",
      "-----------------------------------\n",
      "Training loss: 1.8364054203575684....\n",
      "Validation loss: 1.8687016842192448....\n",
      "-----------------------------------\n",
      "Training loss: 1.8334497252988062....\n",
      "Validation loss: 1.865791590290165....\n",
      "-----------------------------------\n",
      "Training loss: 1.830493958597461....\n",
      "Validation loss: 1.8628771588629065....\n",
      "-----------------------------------\n",
      "Training loss: 1.8275350934358232....\n",
      "Validation loss: 1.85996297832419....\n",
      "-----------------------------------\n",
      "Training loss: 1.8245766922616926....\n",
      "Validation loss: 1.857047036832445....\n",
      "-----------------------------------\n",
      "Training loss: 1.8216180572373282....\n",
      "Validation loss: 1.8541227841669785....\n",
      "-----------------------------------\n",
      "Training loss: 1.818656229103182....\n",
      "Validation loss: 1.8511954987481773....\n",
      "-----------------------------------\n",
      "Training loss: 1.8156933685477419....\n",
      "Validation loss: 1.848266588489562....\n",
      "-----------------------------------\n",
      "Training loss: 1.8127284529620837....\n",
      "Validation loss: 1.8453377608174102....\n",
      "-----------------------------------\n",
      "Training loss: 1.809763595834429....\n",
      "Validation loss: 1.8424103861352163....\n",
      "-----------------------------------\n",
      "Training loss: 1.8067973471319396....\n",
      "Validation loss: 1.8394787701890618....\n",
      "-----------------------------------\n",
      "Training loss: 1.8038253663145656....\n",
      "Validation loss: 1.8365439489297266....\n",
      "-----------------------------------\n",
      "Training loss: 1.800850010987471....\n",
      "Validation loss: 1.833607827230312....\n",
      "-----------------------------------\n",
      "Training loss: 1.7978737337335897....\n",
      "Validation loss: 1.8306696696329703....\n",
      "-----------------------------------\n",
      "Training loss: 1.7948963454566618....\n",
      "Validation loss: 1.8277313453482562....\n",
      "-----------------------------------\n",
      "Training loss: 1.7919175464702133....\n",
      "Validation loss: 1.8247882719740032....\n",
      "-----------------------------------\n",
      "Training loss: 1.7889322166732238....\n",
      "Validation loss: 1.8218444228820327....\n",
      "-----------------------------------\n",
      "Training loss: 1.7859434224721833....\n",
      "Validation loss: 1.818897308662583....\n",
      "-----------------------------------\n",
      "Training loss: 1.7829474645627266....\n",
      "Validation loss: 1.8159478015603607....\n",
      "-----------------------------------\n",
      "Training loss: 1.7799482716925128....\n",
      "Validation loss: 1.8130000507683843....\n",
      "-----------------------------------\n",
      "Training loss: 1.7769490051804535....\n",
      "Validation loss: 1.8100504070736387....\n",
      "-----------------------------------\n",
      "Training loss: 1.7739495583601372....\n",
      "Validation loss: 1.8071017784844436....\n",
      "-----------------------------------\n",
      "Training loss: 1.7709482539296155....\n",
      "Validation loss: 1.8041510863854804....\n",
      "-----------------------------------\n",
      "Training loss: 1.7679438131062442....\n",
      "Validation loss: 1.8011975214542268....\n",
      "-----------------------------------\n",
      "Training loss: 1.7649382458423524....\n",
      "Validation loss: 1.7982381386925235....\n",
      "-----------------------------------\n",
      "Training loss: 1.7619287302382782....\n",
      "Validation loss: 1.7952778265606593....\n",
      "-----------------------------------\n",
      "Training loss: 1.758919498411049....\n",
      "Validation loss: 1.7923159953418977....\n",
      "-----------------------------------\n",
      "Training loss: 1.7559069650724752....\n",
      "Validation loss: 1.7893543627101398....\n",
      "-----------------------------------\n",
      "Training loss: 1.752893519532363....\n",
      "Validation loss: 1.786393419186782....\n",
      "-----------------------------------\n",
      "Training loss: 1.7498795793104593....\n",
      "Validation loss: 1.7834313973090317....\n",
      "-----------------------------------\n",
      "Training loss: 1.7468628120244873....\n",
      "Validation loss: 1.7804690600112623....\n",
      "-----------------------------------\n",
      "Training loss: 1.7438479221978684....\n",
      "Validation loss: 1.777508670271288....\n",
      "-----------------------------------\n",
      "Training loss: 1.7408372778142553....\n",
      "Validation loss: 1.7745495213014997....\n",
      "-----------------------------------\n",
      "Training loss: 1.7378245700927133....\n",
      "Validation loss: 1.7715917602085305....\n",
      "-----------------------------------\n",
      "Training loss: 1.734810233907598....\n",
      "Validation loss: 1.7686331996867695....\n",
      "-----------------------------------\n",
      "Training loss: 1.7317934711501544....\n",
      "Validation loss: 1.7656723498466054....\n",
      "-----------------------------------\n",
      "Training loss: 1.7287719977409883....\n",
      "Validation loss: 1.7627065879445636....\n",
      "-----------------------------------\n",
      "Training loss: 1.7257437550689125....\n",
      "Validation loss: 1.7597429099824153....\n",
      "-----------------------------------\n",
      "Training loss: 1.7227130156306447....\n",
      "Validation loss: 1.7567810321794028....\n",
      "-----------------------------------\n",
      "Training loss: 1.7196807892294315....\n",
      "Validation loss: 1.7538134390857358....\n",
      "-----------------------------------\n",
      "Training loss: 1.7166423099928498....\n",
      "Validation loss: 1.75084184544132....\n",
      "-----------------------------------\n",
      "Training loss: 1.7135995204507322....\n",
      "Validation loss: 1.7478695588943376....\n",
      "-----------------------------------\n",
      "Training loss: 1.7105579220122604....\n",
      "Validation loss: 1.7448999304044497....\n",
      "-----------------------------------\n",
      "Training loss: 1.7075168130729605....\n",
      "Validation loss: 1.741930483106216....\n",
      "-----------------------------------\n",
      "Training loss: 1.7044722159225807....\n",
      "Validation loss: 1.7389631925973428....\n",
      "-----------------------------------\n",
      "Training loss: 1.7014236180641653....\n",
      "Validation loss: 1.7359952104085414....\n",
      "-----------------------------------\n",
      "Training loss: 1.6983739177016939....\n",
      "Validation loss: 1.7330285462195352....\n",
      "-----------------------------------\n",
      "Training loss: 1.6953252630308022....\n",
      "Validation loss: 1.7300622492214714....\n",
      "-----------------------------------\n",
      "Training loss: 1.6922745310684961....\n",
      "Validation loss: 1.727092354813334....\n",
      "-----------------------------------\n",
      "Training loss: 1.6892262207875934....\n",
      "Validation loss: 1.7241223513510922....\n",
      "-----------------------------------\n",
      "Training loss: 1.6861797248750527....\n",
      "Validation loss: 1.7211530241489486....\n",
      "-----------------------------------\n",
      "Training loss: 1.6831328610905403....\n",
      "Validation loss: 1.7181855520180187....\n",
      "-----------------------------------\n",
      "Training loss: 1.6800845670168745....\n",
      "Validation loss: 1.715218926615711....\n",
      "-----------------------------------\n",
      "Training loss: 1.6770355919458348....\n",
      "Validation loss: 1.7122477478985698....\n",
      "-----------------------------------\n",
      "Training loss: 1.6739880849501736....\n",
      "Validation loss: 1.7092746910966785....\n",
      "-----------------------------------\n",
      "Training loss: 1.670940029632477....\n",
      "Validation loss: 1.7062991078139358....\n",
      "-----------------------------------\n",
      "Training loss: 1.6678918921948889....\n",
      "Validation loss: 1.7033238871445946....\n",
      "-----------------------------------\n",
      "Training loss: 1.6648460378980894....\n",
      "Validation loss: 1.700350551036345....\n",
      "-----------------------------------\n",
      "Training loss: 1.6618015173679144....\n",
      "Validation loss: 1.6973811523040636....\n",
      "-----------------------------------\n",
      "Training loss: 1.65875606756927....\n",
      "Validation loss: 1.6944128985396938....\n",
      "-----------------------------------\n",
      "Training loss: 1.6557098257619411....\n",
      "Validation loss: 1.6914437503940236....\n",
      "-----------------------------------\n",
      "Training loss: 1.6526628132639407....\n",
      "Validation loss: 1.6884742354157265....\n",
      "-----------------------------------\n",
      "Training loss: 1.6496158296338272....\n",
      "Validation loss: 1.6855069827704396....\n",
      "-----------------------------------\n",
      "Training loss: 1.646570327655065....\n",
      "Validation loss: 1.6825431341138437....\n",
      "-----------------------------------\n",
      "Training loss: 1.643526582154498....\n",
      "Validation loss: 1.6795815261833018....\n",
      "-----------------------------------\n",
      "Training loss: 1.640485044311033....\n",
      "Validation loss: 1.6766224843809583....\n",
      "-----------------------------------\n",
      "Training loss: 1.6374476623433911....\n",
      "Validation loss: 1.6736669072028167....\n",
      "-----------------------------------\n",
      "Training loss: 1.6344134380678956....\n",
      "Validation loss: 1.6707132830969453....\n",
      "-----------------------------------\n",
      "Training loss: 1.6313793050925718....\n",
      "Validation loss: 1.667757229055097....\n",
      "-----------------------------------\n",
      "Training loss: 1.6283452215878982....\n",
      "Validation loss: 1.6648024570210178....\n",
      "-----------------------------------\n",
      "Training loss: 1.6253127751641117....\n",
      "Validation loss: 1.6618488972346726....\n",
      "-----------------------------------\n",
      "Training loss: 1.6222839271228537....\n",
      "Validation loss: 1.6588994179479746....\n",
      "-----------------------------------\n",
      "Training loss: 1.619258772499131....\n",
      "Validation loss: 1.6559520273031523....\n",
      "-----------------------------------\n",
      "Training loss: 1.616234138186566....\n",
      "Validation loss: 1.653003125664307....\n",
      "-----------------------------------\n",
      "Training loss: 1.613210497578256....\n",
      "Validation loss: 1.650052969216657....\n",
      "-----------------------------------\n",
      "Training loss: 1.6101877024015352....\n",
      "Validation loss: 1.6471024924643902....\n",
      "-----------------------------------\n",
      "Training loss: 1.6071692560815687....\n",
      "Validation loss: 1.6441563735501386....\n",
      "-----------------------------------\n",
      "Training loss: 1.6041546756069631....\n",
      "Validation loss: 1.641212532810922....\n",
      "-----------------------------------\n",
      "Training loss: 1.6011436555301328....\n",
      "Validation loss: 1.6382706447870419....\n",
      "-----------------------------------\n",
      "Training loss: 1.5981335357508175....\n",
      "Validation loss: 1.6353301836503056....\n",
      "-----------------------------------\n",
      "Training loss: 1.5951231011545297....\n",
      "Validation loss: 1.6323920200800317....\n",
      "-----------------------------------\n",
      "Training loss: 1.592115651501221....\n",
      "Validation loss: 1.6294590057718266....\n",
      "-----------------------------------\n",
      "Training loss: 1.589113387541699....\n",
      "Validation loss: 1.6265274459525145....\n",
      "-----------------------------------\n",
      "Training loss: 1.5861155259056787....\n",
      "Validation loss: 1.6235986087212773....\n",
      "-----------------------------------\n",
      "Training loss: 1.583122049174172....\n",
      "Validation loss: 1.6206733157991107....\n",
      "-----------------------------------\n",
      "Training loss: 1.5801337040378027....\n",
      "Validation loss: 1.6177547512058696....\n",
      "-----------------------------------\n",
      "Training loss: 1.5771502067993863....\n",
      "Validation loss: 1.6148397681496842....\n",
      "-----------------------------------\n",
      "Training loss: 1.5741687255139187....\n",
      "Validation loss: 1.6119305282595737....\n",
      "-----------------------------------\n",
      "Training loss: 1.5711942191456918....\n",
      "Validation loss: 1.609028374068907....\n",
      "-----------------------------------\n",
      "Training loss: 1.5682271886890464....\n",
      "Validation loss: 1.6061277675053198....\n",
      "-----------------------------------\n",
      "Training loss: 1.5652616677593256....\n",
      "Validation loss: 1.603227565554777....\n",
      "-----------------------------------\n",
      "Training loss: 1.5622975281679898....\n",
      "Validation loss: 1.6003309748365615....\n",
      "-----------------------------------\n",
      "Training loss: 1.5593381617410207....\n",
      "Validation loss: 1.5974372484512964....\n",
      "-----------------------------------\n",
      "Training loss: 1.5563829390208268....\n",
      "Validation loss: 1.59454641438357....\n",
      "-----------------------------------\n",
      "Training loss: 1.553433591897504....\n",
      "Validation loss: 1.5916574879029746....\n",
      "-----------------------------------\n",
      "Training loss: 1.5504891516384842....\n",
      "Validation loss: 1.5887703488233775....\n",
      "-----------------------------------\n",
      "Training loss: 1.5475469990368926....\n",
      "Validation loss: 1.5858865036072338....\n",
      "-----------------------------------\n",
      "Training loss: 1.5446100128329667....\n",
      "Validation loss: 1.5830070339524986....\n",
      "-----------------------------------\n",
      "Training loss: 1.5416773479553467....\n",
      "Validation loss: 1.5801301878231397....\n",
      "-----------------------------------\n",
      "Training loss: 1.5387473181215505....\n",
      "Validation loss: 1.5772589285141472....\n",
      "-----------------------------------\n",
      "Training loss: 1.5358246281016747....\n",
      "Validation loss: 1.5743947630758148....\n",
      "-----------------------------------\n",
      "Training loss: 1.5329090809589716....\n",
      "Validation loss: 1.57153505300784....\n",
      "-----------------------------------\n",
      "Training loss: 1.5299977320203688....\n",
      "Validation loss: 1.5686780637537892....\n",
      "-----------------------------------\n",
      "Training loss: 1.5270898802926185....\n",
      "Validation loss: 1.5658259605761695....\n",
      "-----------------------------------\n",
      "Training loss: 1.5241872977175788....\n",
      "Validation loss: 1.5629786260595255....\n",
      "-----------------------------------\n",
      "Training loss: 1.521288418348556....\n",
      "Validation loss: 1.5601363059330018....\n",
      "-----------------------------------\n",
      "Training loss: 1.5183934842668967....\n",
      "Validation loss: 1.5572981596678823....\n",
      "-----------------------------------\n",
      "Training loss: 1.5155029818382104....\n",
      "Validation loss: 1.554464125274466....\n",
      "-----------------------------------\n",
      "Training loss: 1.5126158864384975....\n",
      "Validation loss: 1.5516346922068407....\n",
      "-----------------------------------\n",
      "Training loss: 1.5097342107753362....\n",
      "Validation loss: 1.5488098189303163....\n",
      "-----------------------------------\n",
      "Training loss: 1.5068577569813142....\n",
      "Validation loss: 1.545989574334661....\n",
      "-----------------------------------\n",
      "Training loss: 1.5039875411343469....\n",
      "Validation loss: 1.5431735964419633....\n",
      "-----------------------------------\n",
      "Training loss: 1.5011223141089047....\n",
      "Validation loss: 1.5403612988433606....\n",
      "-----------------------------------\n",
      "Training loss: 1.49826055088194....\n",
      "Validation loss: 1.537554114372455....\n",
      "-----------------------------------\n",
      "Training loss: 1.4954033533070463....\n",
      "Validation loss: 1.5347520248246764....\n",
      "-----------------------------------\n",
      "Training loss: 1.4925504512599737....\n",
      "Validation loss: 1.531955517241117....\n",
      "-----------------------------------\n",
      "Training loss: 1.489701676104528....\n",
      "Validation loss: 1.5291645968075385....\n",
      "-----------------------------------\n",
      "Training loss: 1.4868580572945889....\n",
      "Validation loss: 1.5263778852228658....\n",
      "-----------------------------------\n",
      "Training loss: 1.4840195369462013....\n",
      "Validation loss: 1.5235938292974625....\n",
      "-----------------------------------\n",
      "Training loss: 1.4811833361528903....\n",
      "Validation loss: 1.520813696716882....\n",
      "-----------------------------------\n",
      "Training loss: 1.4783511921805907....\n",
      "Validation loss: 1.5180385087051347....\n",
      "-----------------------------------\n",
      "Training loss: 1.4755240584777225....\n",
      "Validation loss: 1.5152686694329058....\n",
      "-----------------------------------\n",
      "Training loss: 1.4727027293026174....\n",
      "Validation loss: 1.5125041949284428....\n",
      "-----------------------------------\n",
      "Training loss: 1.469886425547607....\n",
      "Validation loss: 1.5097438168682755....\n",
      "-----------------------------------\n",
      "Training loss: 1.4670743679538387....\n",
      "Validation loss: 1.5069861137781089....\n",
      "-----------------------------------\n",
      "Training loss: 1.4642650792157834....\n",
      "Validation loss: 1.5042350094467647....\n",
      "-----------------------------------\n",
      "Training loss: 1.4614604183086184....\n",
      "Validation loss: 1.5014881878761588....\n",
      "-----------------------------------\n",
      "Training loss: 1.458661850967139....\n",
      "Validation loss: 1.498745117119194....\n",
      "-----------------------------------\n",
      "Training loss: 1.4558693920937533....\n",
      "Validation loss: 1.4960076143023597....\n",
      "-----------------------------------\n",
      "Training loss: 1.4530827039855096....\n",
      "Validation loss: 1.4932743054803683....\n",
      "-----------------------------------\n",
      "Training loss: 1.4502995349296366....\n",
      "Validation loss: 1.4905442469650771....\n",
      "-----------------------------------\n",
      "Training loss: 1.4475199417978122....\n",
      "Validation loss: 1.4878190696088727....\n",
      "-----------------------------------\n",
      "Training loss: 1.4447448439354065....\n",
      "Validation loss: 1.4850974862639665....\n",
      "-----------------------------------\n",
      "Training loss: 1.4419733290657062....\n",
      "Validation loss: 1.482380761466149....\n",
      "-----------------------------------\n",
      "Training loss: 1.439207661767321....\n",
      "Validation loss: 1.4796688199456831....\n",
      "-----------------------------------\n",
      "Training loss: 1.4364468540612536....\n",
      "Validation loss: 1.476961437874943....\n",
      "-----------------------------------\n",
      "Training loss: 1.4336916791277552....\n",
      "Validation loss: 1.474259014465003....\n",
      "-----------------------------------\n",
      "Training loss: 1.4309403406699854....\n",
      "Validation loss: 1.4715600490640792....\n",
      "-----------------------------------\n",
      "Training loss: 1.4281932188721904....\n",
      "Validation loss: 1.4688647955349248....\n",
      "-----------------------------------\n",
      "Training loss: 1.425451387496423....\n",
      "Validation loss: 1.4661715944711218....\n",
      "-----------------------------------\n",
      "Training loss: 1.4227124126077337....\n",
      "Validation loss: 1.4634834975744158....\n",
      "-----------------------------------\n",
      "Training loss: 1.4199778371927254....\n",
      "Validation loss: 1.460801375066237....\n",
      "-----------------------------------\n",
      "Training loss: 1.4172476030991978....\n",
      "Validation loss: 1.4581242660303033....\n",
      "-----------------------------------\n",
      "Training loss: 1.4145216822166915....\n",
      "Validation loss: 1.4554507694727477....\n",
      "-----------------------------------\n",
      "Training loss: 1.4117981357265494....\n",
      "Validation loss: 1.4527839722341132....\n",
      "-----------------------------------\n",
      "Training loss: 1.4090816687134093....\n",
      "Validation loss: 1.4501230588035956....\n",
      "-----------------------------------\n",
      "Training loss: 1.4063709313542165....\n",
      "Validation loss: 1.4474655178151061....\n",
      "-----------------------------------\n",
      "Training loss: 1.4036620235972121....\n",
      "Validation loss: 1.4448103197653193....\n",
      "-----------------------------------\n",
      "Training loss: 1.4009576901132033....\n",
      "Validation loss: 1.4421595967328238....\n",
      "-----------------------------------\n",
      "Training loss: 1.3982587326991551....\n",
      "Validation loss: 1.4395136971564408....\n",
      "-----------------------------------\n",
      "Training loss: 1.3955630558478183....\n",
      "Validation loss: 1.4368722965219973....\n",
      "-----------------------------------\n",
      "Training loss: 1.3928726529865578....\n",
      "Validation loss: 1.4342353912243462....\n",
      "-----------------------------------\n",
      "Training loss: 1.3901878817232227....\n",
      "Validation loss: 1.4316027476680493....\n",
      "-----------------------------------\n",
      "Training loss: 1.3875083976869507....\n",
      "Validation loss: 1.4289730534956961....\n",
      "-----------------------------------\n",
      "Training loss: 1.3848329846575749....\n",
      "Validation loss: 1.426347917355631....\n",
      "-----------------------------------\n",
      "Training loss: 1.3821606637961406....\n",
      "Validation loss: 1.4237274694715552....\n",
      "-----------------------------------\n",
      "Training loss: 1.3794919004495783....\n",
      "Validation loss: 1.4211130409978192....\n",
      "-----------------------------------\n",
      "Training loss: 1.3768282623908217....\n",
      "Validation loss: 1.418502833660627....\n",
      "-----------------------------------\n",
      "Training loss: 1.374168614736202....\n",
      "Validation loss: 1.4158991738169444....\n",
      "-----------------------------------\n",
      "Training loss: 1.371513439430585....\n",
      "Validation loss: 1.4132999032241178....\n",
      "-----------------------------------\n",
      "Training loss: 1.3688618697061938....\n",
      "Validation loss: 1.4107053756484076....\n",
      "-----------------------------------\n",
      "Training loss: 1.366214668932359....\n",
      "Validation loss: 1.4081165109028848....\n",
      "-----------------------------------\n",
      "Training loss: 1.3635724175744404....\n",
      "Validation loss: 1.4055318535792458....\n",
      "-----------------------------------\n",
      "Training loss: 1.3609339378696517....\n",
      "Validation loss: 1.402951724948911....\n",
      "-----------------------------------\n",
      "Training loss: 1.358299176907801....\n",
      "Validation loss: 1.400376245128483....\n",
      "-----------------------------------\n",
      "Training loss: 1.3556691692063738....\n",
      "Validation loss: 1.39780532342069....\n",
      "-----------------------------------\n",
      "Training loss: 1.3530451977646993....\n",
      "Validation loss: 1.3952406318830013....\n",
      "-----------------------------------\n",
      "Training loss: 1.3504268281445293....\n",
      "Validation loss: 1.3926823793725567....\n",
      "-----------------------------------\n",
      "Training loss: 1.347813481758489....\n",
      "Validation loss: 1.3901298397354858....\n",
      "-----------------------------------\n",
      "Training loss: 1.3452066457274598....\n",
      "Validation loss: 1.3875834690691824....\n",
      "-----------------------------------\n",
      "Training loss: 1.342606814691675....\n",
      "Validation loss: 1.3850413652597942....\n",
      "-----------------------------------\n",
      "Training loss: 1.3400132634178745....\n",
      "Validation loss: 1.382505096845419....\n",
      "-----------------------------------\n",
      "Training loss: 1.3374258109912023....\n",
      "Validation loss: 1.3799744592611676....\n",
      "-----------------------------------\n",
      "Training loss: 1.3348426645380556....\n",
      "Validation loss: 1.3774469274109322....\n",
      "-----------------------------------\n",
      "Training loss: 1.3322624820034732....\n",
      "Validation loss: 1.3749260163887504....\n",
      "-----------------------------------\n",
      "Training loss: 1.3296883817511758....\n",
      "Validation loss: 1.3724094980484487....\n",
      "-----------------------------------\n",
      "Training loss: 1.3271185188338075....\n",
      "Validation loss: 1.3698991913816059....\n",
      "-----------------------------------\n",
      "Training loss: 1.3245561371469265....\n",
      "Validation loss: 1.367393121585847....\n",
      "-----------------------------------\n",
      "Training loss: 1.3219985534039578....\n",
      "Validation loss: 1.364891103220044....\n",
      "-----------------------------------\n",
      "Training loss: 1.3194453479685282....\n",
      "Validation loss: 1.362395299964377....\n",
      "-----------------------------------\n",
      "Training loss: 1.316897507107304....\n",
      "Validation loss: 1.3599038537564623....\n",
      "-----------------------------------\n",
      "Training loss: 1.3143557865622466....\n",
      "Validation loss: 1.357417915262846....\n",
      "-----------------------------------\n",
      "Training loss: 1.3118197727695524....\n",
      "Validation loss: 1.35493755904122....\n",
      "-----------------------------------\n",
      "Training loss: 1.3092900172251996....\n",
      "Validation loss: 1.3524599339611283....\n",
      "-----------------------------------\n",
      "Training loss: 1.3067644998896404....\n",
      "Validation loss: 1.349987988609766....\n",
      "-----------------------------------\n",
      "Training loss: 1.3042439335438032....\n",
      "Validation loss: 1.347521255767258....\n",
      "-----------------------------------\n",
      "Training loss: 1.3017276921839407....\n",
      "Validation loss: 1.3450596070956027....\n",
      "-----------------------------------\n",
      "Training loss: 1.2992173696470597....\n",
      "Validation loss: 1.3426038215774652....\n",
      "-----------------------------------\n",
      "Training loss: 1.296712227482664....\n",
      "Validation loss: 1.3401524332596564....\n",
      "-----------------------------------\n",
      "Training loss: 1.2942100705092996....\n",
      "Validation loss: 1.3377056694303546....\n",
      "-----------------------------------\n",
      "Training loss: 1.2917119747045216....\n",
      "Validation loss: 1.3352616302813025....\n",
      "-----------------------------------\n",
      "Training loss: 1.2892172793779297....\n",
      "Validation loss: 1.3328232651962262....\n",
      "-----------------------------------\n",
      "Training loss: 1.286727590617494....\n",
      "Validation loss: 1.3303900276866187....\n",
      "-----------------------------------\n",
      "Training loss: 1.2842441261914477....\n",
      "Validation loss: 1.3279627629379742....\n",
      "-----------------------------------\n",
      "Training loss: 1.2817672433715974....\n",
      "Validation loss: 1.325539965695939....\n",
      "-----------------------------------\n",
      "Training loss: 1.2792962532576733....\n",
      "Validation loss: 1.3231225355242495....\n",
      "-----------------------------------\n",
      "Training loss: 1.2768321182695728....\n",
      "Validation loss: 1.3207088718435684....\n",
      "-----------------------------------\n",
      "Training loss: 1.2743747090832833....\n",
      "Validation loss: 1.318300828705074....\n",
      "-----------------------------------\n",
      "Training loss: 1.2719230292125383....\n",
      "Validation loss: 1.3158980764467625....\n",
      "-----------------------------------\n",
      "Training loss: 1.2694772171105206....\n",
      "Validation loss: 1.3135014834551453....\n",
      "-----------------------------------\n",
      "Training loss: 1.2670372454379732....\n",
      "Validation loss: 1.3111080448958827....\n",
      "-----------------------------------\n",
      "Training loss: 1.2646015039806753....\n",
      "Validation loss: 1.3087189689108418....\n",
      "-----------------------------------\n",
      "Training loss: 1.262170531543046....\n",
      "Validation loss: 1.3063342909405955....\n",
      "-----------------------------------\n",
      "Training loss: 1.2597450263956913....\n",
      "Validation loss: 1.303955406734245....\n",
      "-----------------------------------\n",
      "Training loss: 1.25732495159359....\n",
      "Validation loss: 1.3015816586349815....\n",
      "-----------------------------------\n",
      "Training loss: 1.254910978826842....\n",
      "Validation loss: 1.299212222606798....\n",
      "-----------------------------------\n",
      "Training loss: 1.2525018097368807....\n",
      "Validation loss: 1.2968477477056572....\n",
      "-----------------------------------\n",
      "Training loss: 1.250098131559039....\n",
      "Validation loss: 1.2944880537732515....\n",
      "-----------------------------------\n",
      "Training loss: 1.2477006280231335....\n",
      "Validation loss: 1.2921333105891977....\n",
      "-----------------------------------\n",
      "Training loss: 1.2453086515832812....\n",
      "Validation loss: 1.2897841477012182....\n",
      "-----------------------------------\n",
      "Training loss: 1.242922451224383....\n",
      "Validation loss: 1.2874413019653566....\n",
      "-----------------------------------\n",
      "Training loss: 1.2405420994522622....\n",
      "Validation loss: 1.2851041450259333....\n",
      "-----------------------------------\n",
      "Training loss: 1.2381678712228283....\n",
      "Validation loss: 1.282772064373798....\n",
      "-----------------------------------\n",
      "Training loss: 1.2357989682522619....\n",
      "Validation loss: 1.2804464502124049....\n",
      "-----------------------------------\n",
      "Training loss: 1.2334368147070345....\n",
      "Validation loss: 1.2781270027084444....\n",
      "-----------------------------------\n",
      "Training loss: 1.2310799035581534....\n",
      "Validation loss: 1.275813725527272....\n",
      "-----------------------------------\n",
      "Training loss: 1.2287286624503202....\n",
      "Validation loss: 1.2735063303393586....\n",
      "-----------------------------------\n",
      "Training loss: 1.2263832652737077....\n",
      "Validation loss: 1.2712035685866427....\n",
      "-----------------------------------\n",
      "Training loss: 1.2240421282871938....\n",
      "Validation loss: 1.2689067526029507....\n",
      "-----------------------------------\n",
      "Training loss: 1.2217046001626617....\n",
      "Validation loss: 1.2666141643767244....\n",
      "-----------------------------------\n",
      "Training loss: 1.2193710974722576....\n",
      "Validation loss: 1.264327815305905....\n",
      "-----------------------------------\n",
      "Training loss: 1.2170437900644113....\n",
      "Validation loss: 1.2620471313012847....\n",
      "-----------------------------------\n",
      "Training loss: 1.2147234061120098....\n",
      "Validation loss: 1.2597706706048433....\n",
      "-----------------------------------\n",
      "Training loss: 1.2124091989634953....\n",
      "Validation loss: 1.2575007258591286....\n",
      "-----------------------------------\n",
      "Training loss: 1.2101015891022648....\n",
      "Validation loss: 1.2552365447113483....\n",
      "-----------------------------------\n",
      "Training loss: 1.2078005598583506....\n",
      "Validation loss: 1.25297831813865....\n",
      "-----------------------------------\n",
      "Training loss: 1.2055053245091414....\n",
      "Validation loss: 1.2507261463519534....\n",
      "-----------------------------------\n",
      "Training loss: 1.2032171382879202....\n",
      "Validation loss: 1.2484790061458635....\n",
      "-----------------------------------\n",
      "Training loss: 1.2009354621735286....\n",
      "Validation loss: 1.2462368956877883....\n",
      "-----------------------------------\n",
      "Training loss: 1.1986590596394033....\n",
      "Validation loss: 1.2439996568595317....\n",
      "-----------------------------------\n",
      "Training loss: 1.196386953660302....\n",
      "Validation loss: 1.241767025670938....\n",
      "-----------------------------------\n",
      "Training loss: 1.194119375629901....\n",
      "Validation loss: 1.2395402345978743....\n",
      "-----------------------------------\n",
      "Training loss: 1.1918570368758703....\n",
      "Validation loss: 1.237317619061636....\n",
      "-----------------------------------\n",
      "Training loss: 1.189598764190795....\n",
      "Validation loss: 1.235099638551652....\n",
      "-----------------------------------\n",
      "Training loss: 1.1873464119530097....\n",
      "Validation loss: 1.2328864730984792....\n",
      "-----------------------------------\n",
      "Training loss: 1.185099804485561....\n",
      "Validation loss: 1.2306790506029477....\n",
      "-----------------------------------\n",
      "Training loss: 1.1828591659813439....\n",
      "Validation loss: 1.228477652230486....\n",
      "-----------------------------------\n",
      "Training loss: 1.1806243693104923....\n",
      "Validation loss: 1.2262814081224365....\n",
      "-----------------------------------\n",
      "Training loss: 1.178395770754982....\n",
      "Validation loss: 1.2240915765866396....\n",
      "-----------------------------------\n",
      "Training loss: 1.17617295695387....\n",
      "Validation loss: 1.2219066007790362....\n",
      "-----------------------------------\n",
      "Training loss: 1.1739558978307112....\n",
      "Validation loss: 1.2197261579300305....\n",
      "-----------------------------------\n",
      "Training loss: 1.1717432103339254....\n",
      "Validation loss: 1.2175517080079918....\n",
      "-----------------------------------\n",
      "Training loss: 1.1695352494167701....\n",
      "Validation loss: 1.215383515549803....\n",
      "-----------------------------------\n",
      "Training loss: 1.1673339241521912....\n",
      "Validation loss: 1.2132219622689786....\n",
      "-----------------------------------\n",
      "Training loss: 1.1651392472261377....\n",
      "Validation loss: 1.2110661267028469....\n",
      "-----------------------------------\n",
      "Training loss: 1.1629497103419122....\n",
      "Validation loss: 1.2089155418144766....\n",
      "-----------------------------------\n",
      "Training loss: 1.160765985178503....\n",
      "Validation loss: 1.2067707539874906....\n",
      "-----------------------------------\n",
      "Training loss: 1.158588465386199....\n",
      "Validation loss: 1.2046309981892096....\n",
      "-----------------------------------\n",
      "Training loss: 1.1564167153477083....\n",
      "Validation loss: 1.202495325376939....\n",
      "-----------------------------------\n",
      "Training loss: 1.154250467299899....\n",
      "Validation loss: 1.2003644871398516....\n",
      "-----------------------------------\n",
      "Training loss: 1.152089878619048....\n",
      "Validation loss: 1.198240334035089....\n",
      "-----------------------------------\n",
      "Training loss: 1.1499352713746391....\n",
      "Validation loss: 1.1961226359425845....\n",
      "-----------------------------------\n",
      "Training loss: 1.147786676087131....\n",
      "Validation loss: 1.1940108216595595....\n",
      "-----------------------------------\n",
      "Training loss: 1.1456436234180907....\n",
      "Validation loss: 1.1919049383729234....\n",
      "-----------------------------------\n",
      "Training loss: 1.1435067223697315....\n",
      "Validation loss: 1.189804702070482....\n",
      "-----------------------------------\n",
      "Training loss: 1.1413754561467384....\n",
      "Validation loss: 1.1877084614975428....\n",
      "-----------------------------------\n",
      "Training loss: 1.1392502246590839....\n",
      "Validation loss: 1.1856187804603464....\n",
      "-----------------------------------\n",
      "Training loss: 1.1371316439932355....\n",
      "Validation loss: 1.1835363517062394....\n",
      "-----------------------------------\n",
      "Training loss: 1.1350190211772893....\n",
      "Validation loss: 1.1814601641301128....\n",
      "-----------------------------------\n",
      "Training loss: 1.1329127646121013....\n",
      "Validation loss: 1.179391156612386....\n",
      "-----------------------------------\n",
      "Training loss: 1.1308127585624614....\n",
      "Validation loss: 1.17732753849829....\n",
      "-----------------------------------\n",
      "Training loss: 1.1287186965033642....\n",
      "Validation loss: 1.1752698290874588....\n",
      "-----------------------------------\n",
      "Training loss: 1.1266303451105382....\n",
      "Validation loss: 1.1732182371277537....\n",
      "-----------------------------------\n",
      "Training loss: 1.124548608263739....\n",
      "Validation loss: 1.1711714983801658....\n",
      "-----------------------------------\n",
      "Training loss: 1.1224723955842466....\n",
      "Validation loss: 1.1691304791708057....\n",
      "-----------------------------------\n",
      "Training loss: 1.1204014204343886....\n",
      "Validation loss: 1.167094384629643....\n",
      "-----------------------------------\n",
      "Training loss: 1.118334778256674....\n",
      "Validation loss: 1.165063663789536....\n",
      "-----------------------------------\n",
      "Training loss: 1.116272098489567....\n",
      "Validation loss: 1.163037385987544....\n",
      "-----------------------------------\n",
      "Training loss: 1.1142149632343892....\n",
      "Validation loss: 1.1610167863701717....\n",
      "-----------------------------------\n",
      "Training loss: 1.112163604310212....\n",
      "Validation loss: 1.159001104452429....\n",
      "-----------------------------------\n",
      "Training loss: 1.1101178581916704....\n",
      "Validation loss: 1.1569900158224733....\n",
      "-----------------------------------\n",
      "Training loss: 1.1080777230540995....\n",
      "Validation loss: 1.154984063337304....\n",
      "-----------------------------------\n",
      "Training loss: 1.1060425099392135....\n",
      "Validation loss: 1.1529847093621708....\n",
      "-----------------------------------\n",
      "Training loss: 1.1040134859111976....\n",
      "Validation loss: 1.15099150969725....\n",
      "-----------------------------------\n",
      "Training loss: 1.101990629690514....\n",
      "Validation loss: 1.149003850104612....\n",
      "-----------------------------------\n",
      "Training loss: 1.0999729358327275....\n",
      "Validation loss: 1.1470210814814847....\n",
      "-----------------------------------\n",
      "Training loss: 1.09796042066223....\n",
      "Validation loss: 1.1450431732466715....\n",
      "-----------------------------------\n",
      "Training loss: 1.0959537402959154....\n",
      "Validation loss: 1.143071270899687....\n",
      "-----------------------------------\n",
      "Training loss: 1.093952614298992....\n",
      "Validation loss: 1.1411055610895227....\n",
      "-----------------------------------\n",
      "Training loss: 1.0919574039073074....\n",
      "Validation loss: 1.139145503001492....\n",
      "-----------------------------------\n",
      "Training loss: 1.0899680298871033....\n",
      "Validation loss: 1.1371909624546608....\n",
      "-----------------------------------\n",
      "Training loss: 1.0879841450451404....\n",
      "Validation loss: 1.1352405088087665....\n",
      "-----------------------------------\n",
      "Training loss: 1.0860048366608999....\n",
      "Validation loss: 1.1332950840797722....\n",
      "-----------------------------------\n",
      "Training loss: 1.084030335720298....\n",
      "Validation loss: 1.1313557078918939....\n",
      "-----------------------------------\n",
      "Training loss: 1.0820614647761302....\n",
      "Validation loss: 1.1294223563821486....\n",
      "-----------------------------------\n",
      "Training loss: 1.0800977814712371....\n",
      "Validation loss: 1.1274951900049455....\n",
      "-----------------------------------\n",
      "Training loss: 1.0781399181470432....\n",
      "Validation loss: 1.1255731067143402....\n",
      "-----------------------------------\n",
      "Training loss: 1.0761875090566022....\n",
      "Validation loss: 1.1236566041323697....\n",
      "-----------------------------------\n",
      "Training loss: 1.0742408087672108....\n",
      "Validation loss: 1.1217449775783739....\n",
      "-----------------------------------\n",
      "Training loss: 1.0722990383642583....\n",
      "Validation loss: 1.1198381871418037....\n",
      "-----------------------------------\n",
      "Training loss: 1.070361690476463....\n",
      "Validation loss: 1.1179365900131206....\n",
      "-----------------------------------\n",
      "Training loss: 1.0684301656347326....\n",
      "Validation loss: 1.1160415910910877....\n",
      "-----------------------------------\n",
      "Training loss: 1.066505570661352....\n",
      "Validation loss: 1.1141525747224377....\n",
      "-----------------------------------\n",
      "Training loss: 1.064586513409442....\n",
      "Validation loss: 1.1122667669867188....\n",
      "-----------------------------------\n",
      "Training loss: 1.0626707342475452....\n",
      "Validation loss: 1.1103859232127362....\n",
      "-----------------------------------\n",
      "Training loss: 1.0607607156503385....\n",
      "Validation loss: 1.1085090414935967....\n",
      "-----------------------------------\n",
      "Training loss: 1.0588558889928985....\n",
      "Validation loss: 1.1066367352792574....\n",
      "-----------------------------------\n",
      "Training loss: 1.056955992393916....\n",
      "Validation loss: 1.1047692251183534....\n",
      "-----------------------------------\n",
      "Training loss: 1.0550620238113064....\n",
      "Validation loss: 1.1029071512683015....\n",
      "-----------------------------------\n",
      "Training loss: 1.053172542041548....\n",
      "Validation loss: 1.1010503104953322....\n",
      "-----------------------------------\n",
      "Training loss: 1.051287623981518....\n",
      "Validation loss: 1.0991989507386282....\n",
      "-----------------------------------\n",
      "Training loss: 1.0494084174280092....\n",
      "Validation loss: 1.0973539908461405....\n",
      "-----------------------------------\n",
      "Training loss: 1.0475350136833599....\n",
      "Validation loss: 1.095513915141306....\n",
      "-----------------------------------\n",
      "Training loss: 1.0456659619314224....\n",
      "Validation loss: 1.0936785194781868....\n",
      "-----------------------------------\n",
      "Training loss: 1.0438002282264598....\n",
      "Validation loss: 1.0918485588025086....\n",
      "-----------------------------------\n",
      "Training loss: 1.0419394304260696....\n",
      "Validation loss: 1.0900238011718217....\n",
      "-----------------------------------\n",
      "Training loss: 1.0400838484139245....\n",
      "Validation loss: 1.0882038009263642....\n",
      "-----------------------------------\n",
      "Training loss: 1.0382332138164145....\n",
      "Validation loss: 1.0863883935996557....\n",
      "-----------------------------------\n",
      "Training loss: 1.036386744480648....\n",
      "Validation loss: 1.0845784453680485....\n",
      "-----------------------------------\n",
      "Training loss: 1.0345456100652985....\n",
      "Validation loss: 1.0827739208483063....\n",
      "-----------------------------------\n",
      "Training loss: 1.0327097525336355....\n",
      "Validation loss: 1.0809752214522605....\n",
      "-----------------------------------\n",
      "Training loss: 1.0308798169817865....\n",
      "Validation loss: 1.0791814350078013....\n",
      "-----------------------------------\n",
      "Training loss: 1.0290538665367768....\n",
      "Validation loss: 1.077393665337898....\n",
      "-----------------------------------\n",
      "Training loss: 1.0272331273318487....\n",
      "Validation loss: 1.0756109465269375....\n",
      "-----------------------------------\n",
      "Training loss: 1.025417635648648....\n",
      "Validation loss: 1.0738330680076913....\n",
      "-----------------------------------\n",
      "Training loss: 1.023606672747666....\n",
      "Validation loss: 1.072059645274454....\n",
      "-----------------------------------\n",
      "Training loss: 1.0217998177619354....\n",
      "Validation loss: 1.0702912194948058....\n",
      "-----------------------------------\n",
      "Training loss: 1.0199981313125286....\n",
      "Validation loss: 1.0685277081467721....\n",
      "-----------------------------------\n",
      "Training loss: 1.0182017314581182....\n",
      "Validation loss: 1.0667690582471492....\n",
      "-----------------------------------\n",
      "Training loss: 1.0164112679855557....\n",
      "Validation loss: 1.0650158699718877....\n",
      "-----------------------------------\n",
      "Training loss: 1.0146254558722685....\n",
      "Validation loss: 1.0632675392309343....\n",
      "-----------------------------------\n",
      "Training loss: 1.0128441592883262....\n",
      "Validation loss: 1.0615237446433499....\n",
      "-----------------------------------\n",
      "Training loss: 1.0110679284821549....\n",
      "Validation loss: 1.0597845832024884....\n",
      "-----------------------------------\n",
      "Training loss: 1.0092968510839917....\n",
      "Validation loss: 1.058050594677033....\n",
      "-----------------------------------\n",
      "Training loss: 1.007531293033374....\n",
      "Validation loss: 1.0563224908819775....\n",
      "-----------------------------------\n",
      "Training loss: 1.0057716143318647....\n",
      "Validation loss: 1.0545990502234845....\n",
      "-----------------------------------\n",
      "Training loss: 1.0040168705085168....\n",
      "Validation loss: 1.0528804353696553....\n",
      "-----------------------------------\n",
      "Training loss: 1.0022675192448705....\n",
      "Validation loss: 1.051166977793596....\n",
      "-----------------------------------\n",
      "Training loss: 1.000523246430774....\n",
      "Validation loss: 1.049458955144079....\n",
      "-----------------------------------\n",
      "Training loss: 0.9987841179802018....\n",
      "Validation loss: 1.0477558382181136....\n",
      "-----------------------------------\n",
      "Training loss: 0.9970505214814465....\n",
      "Validation loss: 1.046057877755999....\n",
      "-----------------------------------\n",
      "Training loss: 0.9953211060301583....\n",
      "Validation loss: 1.0443642075731352....\n",
      "-----------------------------------\n",
      "Training loss: 0.9935964150321904....\n",
      "Validation loss: 1.0426758159330953....\n",
      "-----------------------------------\n",
      "Training loss: 0.9918767806045228....\n",
      "Validation loss: 1.0409924897314577....\n",
      "-----------------------------------\n",
      "Training loss: 0.9901618478235983....\n",
      "Validation loss: 1.0393145487100273....\n",
      "-----------------------------------\n",
      "Training loss: 0.98845150300184....\n",
      "Validation loss: 1.0376422814691018....\n",
      "-----------------------------------\n",
      "Training loss: 0.986746043131298....\n",
      "Validation loss: 1.0359752859030353....\n",
      "-----------------------------------\n",
      "Training loss: 0.9850460529704538....\n",
      "Validation loss: 1.0343126110516034....\n",
      "-----------------------------------\n",
      "Training loss: 0.9833511144134404....\n",
      "Validation loss: 1.0326551172023484....\n",
      "-----------------------------------\n",
      "Training loss: 0.9816612558841323....\n",
      "Validation loss: 1.0310034321158028....\n",
      "-----------------------------------\n",
      "Training loss: 0.9799765067355746....\n",
      "Validation loss: 1.0293562186967493....\n",
      "-----------------------------------\n",
      "Training loss: 0.9782960990665257....\n",
      "Validation loss: 1.0277137888585257....\n",
      "-----------------------------------\n",
      "Training loss: 0.976619831784899....\n",
      "Validation loss: 1.0260765603939135....\n",
      "-----------------------------------\n",
      "Training loss: 0.9749483485530428....\n",
      "Validation loss: 1.0244442689401567....\n",
      "-----------------------------------\n",
      "Training loss: 0.9732820642791731....\n",
      "Validation loss: 1.0228175355762508....\n",
      "-----------------------------------\n",
      "Training loss: 0.9716211406102225....\n",
      "Validation loss: 1.0211955399318238....\n",
      "-----------------------------------\n",
      "Training loss: 0.9699645407874593....\n",
      "Validation loss: 1.0195783890784882....\n",
      "-----------------------------------\n",
      "Training loss: 0.9683127946638782....\n",
      "Validation loss: 1.0179655883209096....\n",
      "-----------------------------------\n",
      "Training loss: 0.9666658825012159....\n",
      "Validation loss: 1.0163568220419237....\n",
      "-----------------------------------\n",
      "Training loss: 0.9650235815100418....\n",
      "Validation loss: 1.0147536795465382....\n",
      "-----------------------------------\n",
      "Training loss: 0.9633864342500633....\n",
      "Validation loss: 1.0131556340850105....\n",
      "-----------------------------------\n",
      "Training loss: 0.9617539232034976....\n",
      "Validation loss: 1.0115617839576194....\n",
      "-----------------------------------\n",
      "Training loss: 0.960125485166544....\n",
      "Validation loss: 1.0099727521998239....\n",
      "-----------------------------------\n",
      "Training loss: 0.9585019757977007....\n",
      "Validation loss: 1.0083890173809842....\n",
      "-----------------------------------\n",
      "Training loss: 0.9568835618640049....\n",
      "Validation loss: 1.0068098037114093....\n",
      "-----------------------------------\n",
      "Training loss: 0.9552701525216839....\n",
      "Validation loss: 1.0052343546060074....\n",
      "-----------------------------------\n",
      "Training loss: 0.9536605198738578....\n",
      "Validation loss: 1.003663941785085....\n",
      "-----------------------------------\n",
      "Training loss: 0.9520557798335537....\n",
      "Validation loss: 1.0020985138006155....\n",
      "-----------------------------------\n",
      "Training loss: 0.9504554553093923....\n",
      "Validation loss: 1.0005371766968036....\n",
      "-----------------------------------\n",
      "Training loss: 0.9488591819241874....\n",
      "Validation loss: 0.9989800189902148....\n",
      "-----------------------------------\n",
      "Training loss: 0.9472670405864938....\n",
      "Validation loss: 0.9974278717074682....\n",
      "-----------------------------------\n",
      "Training loss: 0.9456798175148988....\n",
      "Validation loss: 0.9958807543938454....\n",
      "-----------------------------------\n",
      "Training loss: 0.9440971019893476....\n",
      "Validation loss: 0.9943388736055693....\n",
      "-----------------------------------\n",
      "Training loss: 0.9425194104075317....\n",
      "Validation loss: 0.9928012751014099....\n",
      "-----------------------------------\n",
      "Training loss: 0.9409453882442286....\n",
      "Validation loss: 0.9912679618703774....\n",
      "-----------------------------------\n",
      "Training loss: 0.9393763123826913....\n",
      "Validation loss: 0.9897395160482849....\n",
      "-----------------------------------\n",
      "Training loss: 0.9378123776922282....\n",
      "Validation loss: 0.9882161448562128....\n",
      "-----------------------------------\n",
      "Training loss: 0.9362532655362554....\n",
      "Validation loss: 0.9866971791191654....\n",
      "-----------------------------------\n",
      "Training loss: 0.9346986553375011....\n",
      "Validation loss: 0.9851818217472266....\n",
      "-----------------------------------\n",
      "Training loss: 0.9331487603562835....\n",
      "Validation loss: 0.983671153858208....\n",
      "-----------------------------------\n",
      "Training loss: 0.9316040704634481....\n",
      "Validation loss: 0.9821652560621963....\n",
      "-----------------------------------\n",
      "Training loss: 0.930063396160495....\n",
      "Validation loss: 0.9806639342182527....\n",
      "-----------------------------------\n",
      "Training loss: 0.9285272854871353....\n",
      "Validation loss: 0.9791673905374785....\n",
      "-----------------------------------\n",
      "Training loss: 0.9269958236212896....\n",
      "Validation loss: 0.9776747068473755....\n",
      "-----------------------------------\n",
      "Training loss: 0.9254691202684774....\n",
      "Validation loss: 0.9761862640674456....\n",
      "-----------------------------------\n",
      "Training loss: 0.9239477205135668....\n",
      "Validation loss: 0.9747020059910049....\n",
      "-----------------------------------\n",
      "Training loss: 0.9224308155402234....\n",
      "Validation loss: 0.9732220537503613....\n",
      "-----------------------------------\n",
      "Training loss: 0.9209182882186476....\n",
      "Validation loss: 0.9717473888739041....\n",
      "-----------------------------------\n",
      "Training loss: 0.9194103424458624....\n",
      "Validation loss: 0.9702780859114153....\n",
      "-----------------------------------\n",
      "Training loss: 0.9179070086317331....\n",
      "Validation loss: 0.9688135016784913....\n",
      "-----------------------------------\n",
      "Training loss: 0.9164080602690293....\n",
      "Validation loss: 0.967353216110188....\n",
      "-----------------------------------\n",
      "Training loss: 0.9149130513682794....\n",
      "Validation loss: 0.9658970428418788....\n",
      "-----------------------------------\n",
      "Training loss: 0.9134221606925814....\n",
      "Validation loss: 0.9644452922686999....\n",
      "-----------------------------------\n",
      "Training loss: 0.9119356376690233....\n",
      "Validation loss: 0.9629979322353953....\n",
      "-----------------------------------\n",
      "Training loss: 0.9104527841656019....\n",
      "Validation loss: 0.9615554907007362....\n",
      "-----------------------------------\n",
      "Training loss: 0.9089741704827143....\n",
      "Validation loss: 0.9601161939354614....\n",
      "-----------------------------------\n",
      "Training loss: 0.9074988986083593....\n",
      "Validation loss: 0.9586814010455289....\n",
      "-----------------------------------\n",
      "Training loss: 0.9060282055284128....\n",
      "Validation loss: 0.9572514109183714....\n",
      "-----------------------------------\n",
      "Training loss: 0.9045625808636572....\n",
      "Validation loss: 0.9558253845001157....\n",
      "-----------------------------------\n",
      "Training loss: 0.903101606908147....\n",
      "Validation loss: 0.954403661122309....\n",
      "-----------------------------------\n",
      "Training loss: 0.9016450876332858....\n",
      "Validation loss: 0.952986533120382....\n",
      "-----------------------------------\n",
      "Training loss: 0.9001928691916397....\n",
      "Validation loss: 0.9515740660355804....\n",
      "-----------------------------------\n",
      "Training loss: 0.8987444630517726....\n",
      "Validation loss: 0.9501651278275216....\n",
      "-----------------------------------\n",
      "Training loss: 0.8972999872621379....\n",
      "Validation loss: 0.9487601313467462....\n",
      "-----------------------------------\n",
      "Training loss: 0.8958602024931439....\n",
      "Validation loss: 0.9473590272866329....\n",
      "-----------------------------------\n",
      "Training loss: 0.8944247515295326....\n",
      "Validation loss: 0.9459621878745617....\n",
      "-----------------------------------\n",
      "Training loss: 0.8929936742377628....\n",
      "Validation loss: 0.9445693714283485....\n",
      "-----------------------------------\n",
      "Training loss: 0.8915665973671499....\n",
      "Validation loss: 0.9431805262084397....\n",
      "-----------------------------------\n",
      "Training loss: 0.8901435939655359....\n",
      "Validation loss: 0.9417965789972407....\n",
      "-----------------------------------\n",
      "Training loss: 0.8887252021419605....\n",
      "Validation loss: 0.9404166528982246....\n",
      "-----------------------------------\n",
      "Training loss: 0.8873112288975251....\n",
      "Validation loss: 0.9390413703697814....\n",
      "-----------------------------------\n",
      "Training loss: 0.8859018220792242....\n",
      "Validation loss: 0.9376706828110176....\n",
      "-----------------------------------\n",
      "Training loss: 0.8844967728423953....\n",
      "Validation loss: 0.9363037262958959....\n",
      "-----------------------------------\n",
      "Training loss: 0.8830960801839612....\n",
      "Validation loss: 0.9349399842060174....\n",
      "-----------------------------------\n",
      "Training loss: 0.8816992830947356....\n",
      "Validation loss: 0.9335804257271137....\n",
      "-----------------------------------\n",
      "Training loss: 0.8803066019873274....\n",
      "Validation loss: 0.9322248148485623....\n",
      "-----------------------------------\n",
      "Training loss: 0.8789179175081662....\n",
      "Validation loss: 0.9308730405261868....\n",
      "-----------------------------------\n",
      "Training loss: 0.8775328443692765....\n",
      "Validation loss: 0.9295253521205666....\n",
      "-----------------------------------\n",
      "Training loss: 0.876151490400334....\n",
      "Validation loss: 0.9281814403788616....\n",
      "-----------------------------------\n",
      "Training loss: 0.8747736102368427....\n",
      "Validation loss: 0.9268410136576395....\n",
      "-----------------------------------\n",
      "Training loss: 0.8733989941349871....\n",
      "Validation loss: 0.9255046149356599....\n",
      "-----------------------------------\n",
      "Training loss: 0.8720284913403327....\n",
      "Validation loss: 0.9241716285531361....\n",
      "-----------------------------------\n",
      "Training loss: 0.8706615378995328....\n",
      "Validation loss: 0.9228420221282396....\n",
      "-----------------------------------\n",
      "Training loss: 0.8692982766472204....\n",
      "Validation loss: 0.9215160027273779....\n",
      "-----------------------------------\n",
      "Training loss: 0.8679388930751903....\n",
      "Validation loss: 0.9201934405770077....\n",
      "-----------------------------------\n",
      "Training loss: 0.8665825950250398....\n",
      "Validation loss: 0.9188742743191219....\n",
      "-----------------------------------\n",
      "Training loss: 0.8652295088735008....\n",
      "Validation loss: 0.9175587997459247....\n",
      "-----------------------------------\n",
      "Training loss: 0.8638803330884508....\n",
      "Validation loss: 0.916246703861327....\n",
      "-----------------------------------\n",
      "Training loss: 0.8625346568258674....\n",
      "Validation loss: 0.9149382756246519....\n",
      "-----------------------------------\n",
      "Training loss: 0.8611928223038254....\n",
      "Validation loss: 0.9136335697242176....\n",
      "-----------------------------------\n",
      "Training loss: 0.8598551447232454....\n",
      "Validation loss: 0.9123326714650349....\n",
      "-----------------------------------\n",
      "Training loss: 0.8585211523337416....\n",
      "Validation loss: 0.9110350686889309....\n",
      "-----------------------------------\n",
      "Training loss: 0.8571901147243323....\n",
      "Validation loss: 0.9097408099265336....\n",
      "-----------------------------------\n",
      "Training loss: 0.855861949775348....\n",
      "Validation loss: 0.9084506324382433....\n",
      "-----------------------------------\n",
      "Training loss: 0.8545377069747137....\n",
      "Validation loss: 0.9071643469336859....\n",
      "-----------------------------------\n",
      "Training loss: 0.8532173574828....\n",
      "Validation loss: 0.9058824029872852....\n",
      "-----------------------------------\n",
      "Training loss: 0.8519014728040122....\n",
      "Validation loss: 0.9046045630591923....\n",
      "-----------------------------------\n",
      "Training loss: 0.8505895810760473....\n",
      "Validation loss: 0.9033304016626995....\n",
      "-----------------------------------\n",
      "Training loss: 0.8492812781968356....\n",
      "Validation loss: 0.902060347742899....\n",
      "-----------------------------------\n",
      "Training loss: 0.8479768604083793....\n",
      "Validation loss: 0.9007944028664958....\n",
      "-----------------------------------\n",
      "Training loss: 0.8466760866456157....\n",
      "Validation loss: 0.8995316917918136....\n",
      "-----------------------------------\n",
      "Training loss: 0.8453784802030166....\n",
      "Validation loss: 0.8982727915608865....\n",
      "-----------------------------------\n",
      "Training loss: 0.8440841624113814....\n",
      "Validation loss: 0.8970170708969774....\n",
      "-----------------------------------\n",
      "Training loss: 0.8427929414469821....\n",
      "Validation loss: 0.8957654306145155....\n",
      "-----------------------------------\n",
      "Training loss: 0.8415055116446543....\n",
      "Validation loss: 0.8945174885629942....\n",
      "-----------------------------------\n",
      "Training loss: 0.8402216880047478....\n",
      "Validation loss: 0.8932732651544856....\n",
      "-----------------------------------\n",
      "Training loss: 0.8389417226879664....\n",
      "Validation loss: 0.8920328482131131....\n",
      "-----------------------------------\n",
      "Training loss: 0.8376658922729152....\n",
      "Validation loss: 0.8907965127217631....\n",
      "-----------------------------------\n",
      "Training loss: 0.836394410326561....\n",
      "Validation loss: 0.8895641954542051....\n",
      "-----------------------------------\n",
      "Training loss: 0.8351268439118923....\n",
      "Validation loss: 0.888335459572028....\n",
      "-----------------------------------\n",
      "Training loss: 0.8338625212981461....\n",
      "Validation loss: 0.8871105240760311....\n",
      "-----------------------------------\n",
      "Training loss: 0.8326015351478548....\n",
      "Validation loss: 0.8858898451686699....\n",
      "-----------------------------------\n",
      "Training loss: 0.8313447193834174....\n",
      "Validation loss: 0.8846730822428545....\n",
      "-----------------------------------\n",
      "Training loss: 0.8300917094455859....\n",
      "Validation loss: 0.8834591644117282....\n",
      "-----------------------------------\n",
      "Training loss: 0.8288418735892382....\n",
      "Validation loss: 0.882248818953236....\n",
      "-----------------------------------\n",
      "Training loss: 0.8275957642659927....\n",
      "Validation loss: 0.8810427036620402....\n",
      "-----------------------------------\n",
      "Training loss: 0.8263540198312243....\n",
      "Validation loss: 0.8798408698431012....\n",
      "-----------------------------------\n",
      "Training loss: 0.8251164511111873....\n",
      "Validation loss: 0.8786426794656482....\n",
      "-----------------------------------\n",
      "Training loss: 0.8238827145329953....\n",
      "Validation loss: 0.8774482038453922....\n",
      "-----------------------------------\n",
      "Training loss: 0.8226527451788251....\n",
      "Validation loss: 0.8762570935891494....\n",
      "-----------------------------------\n",
      "Training loss: 0.8214261145587594....\n",
      "Validation loss: 0.8750693804254106....\n",
      "-----------------------------------\n",
      "Training loss: 0.8202030729228759....\n",
      "Validation loss: 0.8738849478465431....\n",
      "-----------------------------------\n",
      "Training loss: 0.818983165009773....\n",
      "Validation loss: 0.8727038908078983....\n",
      "-----------------------------------\n",
      "Training loss: 0.8177674086165966....\n",
      "Validation loss: 0.8715263447285956....\n",
      "-----------------------------------\n",
      "Training loss: 0.8165558011388706....\n",
      "Validation loss: 0.8703526725570477....\n",
      "-----------------------------------\n",
      "Training loss: 0.8153479456301281....\n",
      "Validation loss: 0.8691824082499081....\n",
      "-----------------------------------\n",
      "Training loss: 0.814143428512864....\n",
      "Validation loss: 0.868015737928129....\n",
      "-----------------------------------\n",
      "Training loss: 0.8129419821141735....\n",
      "Validation loss: 0.866853201216755....\n",
      "-----------------------------------\n",
      "Training loss: 0.8117441412445948....\n",
      "Validation loss: 0.8656936441007629....\n",
      "-----------------------------------\n",
      "Training loss: 0.8105492837996636....\n",
      "Validation loss: 0.8645372558932088....\n",
      "-----------------------------------\n",
      "Training loss: 0.8093578093814477....\n",
      "Validation loss: 0.863384681505915....\n",
      "-----------------------------------\n",
      "Training loss: 0.8081701978493606....\n",
      "Validation loss: 0.8622357848074153....\n",
      "-----------------------------------\n",
      "Training loss: 0.8069858491316358....\n",
      "Validation loss: 0.8610903731496846....\n",
      "-----------------------------------\n",
      "Training loss: 0.805805418413726....\n",
      "Validation loss: 0.8599485413733015....\n",
      "-----------------------------------\n",
      "Training loss: 0.8046285067966289....\n",
      "Validation loss: 0.8588094385380071....\n",
      "-----------------------------------\n",
      "Training loss: 0.8034548976347196....\n",
      "Validation loss: 0.857673566478555....\n",
      "-----------------------------------\n",
      "Training loss: 0.8022850409718744....\n",
      "Validation loss: 0.8565410519651571....\n",
      "-----------------------------------\n",
      "Training loss: 0.801118866008451....\n",
      "Validation loss: 0.855411822424643....\n",
      "-----------------------------------\n",
      "Training loss: 0.7999556177762132....\n",
      "Validation loss: 0.8542863682692744....\n",
      "-----------------------------------\n",
      "Training loss: 0.7987960478113022....\n",
      "Validation loss: 0.85316468842147....\n",
      "-----------------------------------\n",
      "Training loss: 0.7976398981916982....\n",
      "Validation loss: 0.8520461449637721....\n",
      "-----------------------------------\n",
      "Training loss: 0.7964868116846685....\n",
      "Validation loss: 0.8509304320876111....\n",
      "-----------------------------------\n",
      "Training loss: 0.7953365914480747....\n",
      "Validation loss: 0.8498185023723743....\n",
      "-----------------------------------\n",
      "Training loss: 0.7941899663723067....\n",
      "Validation loss: 0.8487099329054159....\n",
      "-----------------------------------\n",
      "Training loss: 0.7930468430459321....\n",
      "Validation loss: 0.8476050015734014....\n",
      "-----------------------------------\n",
      "Training loss: 0.7919069067315394....\n",
      "Validation loss: 0.8465028630774207....\n",
      "-----------------------------------\n",
      "Training loss: 0.7907696139714163....\n",
      "Validation loss: 0.8454038302535635....\n",
      "-----------------------------------\n",
      "Training loss: 0.7896360783334658....\n",
      "Validation loss: 0.8443082466590917....\n",
      "-----------------------------------\n",
      "Training loss: 0.7885058404107713....\n",
      "Validation loss: 0.8432160527597052....\n",
      "-----------------------------------\n",
      "Training loss: 0.7873789627813452....\n",
      "Validation loss: 0.8421263900067943....\n",
      "-----------------------------------\n",
      "Training loss: 0.786254630515362....\n",
      "Validation loss: 0.8410397691315155....\n",
      "-----------------------------------\n",
      "Training loss: 0.7851332421769881....\n",
      "Validation loss: 0.8399559570929866....\n",
      "-----------------------------------\n",
      "Training loss: 0.7840151284902339....\n",
      "Validation loss: 0.8388752050132005....\n",
      "-----------------------------------\n",
      "Training loss: 0.7828995191378768....\n",
      "Validation loss: 0.8377974870159255....\n",
      "-----------------------------------\n",
      "Training loss: 0.7817866140559352....\n",
      "Validation loss: 0.8367229198308944....\n",
      "-----------------------------------\n",
      "Training loss: 0.7806766995933573....\n",
      "Validation loss: 0.8356510213506867....\n",
      "-----------------------------------\n",
      "Training loss: 0.7795700568797987....\n",
      "Validation loss: 0.8345823483502363....\n",
      "-----------------------------------\n",
      "Training loss: 0.7784669925399736....\n",
      "Validation loss: 0.8335164622433305....\n",
      "-----------------------------------\n",
      "Training loss: 0.7773670603925688....\n",
      "Validation loss: 0.8324535770819265....\n",
      "-----------------------------------\n",
      "Training loss: 0.7762708023133775....\n",
      "Validation loss: 0.8313941282465835....\n",
      "-----------------------------------\n",
      "Training loss: 0.7751778848484671....\n",
      "Validation loss: 0.8303376015834534....\n",
      "-----------------------------------\n",
      "Training loss: 0.7740880621305326....\n",
      "Validation loss: 0.8292843339930464....\n",
      "-----------------------------------\n",
      "Training loss: 0.7730013033396402....\n",
      "Validation loss: 0.8282340736705273....\n",
      "-----------------------------------\n",
      "Training loss: 0.7719175829851039....\n",
      "Validation loss: 0.8271865757289297....\n",
      "-----------------------------------\n",
      "Training loss: 0.7708370004495486....\n",
      "Validation loss: 0.8261422489717549....\n",
      "-----------------------------------\n",
      "Training loss: 0.7697594660086282....\n",
      "Validation loss: 0.8251009807228272....\n",
      "-----------------------------------\n",
      "Training loss: 0.7686848332241316....\n",
      "Validation loss: 0.824063131222571....\n",
      "-----------------------------------\n",
      "Training loss: 0.7676136625951369....\n",
      "Validation loss: 0.8230289644666319....\n",
      "-----------------------------------\n",
      "Training loss: 0.7665460284982....\n",
      "Validation loss: 0.8219978943884748....\n",
      "-----------------------------------\n",
      "Training loss: 0.7654810627311971....\n",
      "Validation loss: 0.8209702085028061....\n",
      "-----------------------------------\n",
      "Training loss: 0.7644190789900974....\n",
      "Validation loss: 0.819945978157446....\n",
      "-----------------------------------\n",
      "Training loss: 0.7633603830151737....\n",
      "Validation loss: 0.8189246424649962....\n",
      "-----------------------------------\n",
      "Training loss: 0.7623045861434775....\n",
      "Validation loss: 0.8179062830194095....\n",
      "-----------------------------------\n",
      "Training loss: 0.7612518595798781....\n",
      "Validation loss: 0.8168905439587285....\n",
      "-----------------------------------\n",
      "Training loss: 0.7602022432684483....\n",
      "Validation loss: 0.8158778226732736....\n",
      "-----------------------------------\n",
      "Training loss: 0.7591556315527781....\n",
      "Validation loss: 0.8148680572713431....\n",
      "-----------------------------------\n",
      "Training loss: 0.7581122797137801....\n",
      "Validation loss: 0.8138612106930345....\n",
      "-----------------------------------\n",
      "Training loss: 0.7570722569580491....\n",
      "Validation loss: 0.8128575054621585....\n",
      "-----------------------------------\n",
      "Training loss: 0.7560356657140748....\n",
      "Validation loss: 0.8118568701989776....\n",
      "-----------------------------------\n",
      "Training loss: 0.7550019602504316....\n",
      "Validation loss: 0.8108595354393168....\n",
      "-----------------------------------\n",
      "Training loss: 0.7539713382977293....\n",
      "Validation loss: 0.8098655092563514....\n",
      "-----------------------------------\n",
      "Training loss: 0.7529439161872518....\n",
      "Validation loss: 0.8088747609463776....\n",
      "-----------------------------------\n",
      "Training loss: 0.7519195095717512....\n",
      "Validation loss: 0.8078866654090061....\n",
      "-----------------------------------\n",
      "Training loss: 0.7508981423056869....\n",
      "Validation loss: 0.8069008810769777....\n",
      "-----------------------------------\n",
      "Training loss: 0.7498797217967029....\n",
      "Validation loss: 0.8059180672731122....\n",
      "-----------------------------------\n",
      "Training loss: 0.7488644092778239....\n",
      "Validation loss: 0.8049383339236943....\n",
      "-----------------------------------\n",
      "Training loss: 0.747852594186275....\n",
      "Validation loss: 0.8039617948811953....\n",
      "-----------------------------------\n",
      "Training loss: 0.7468440597190223....\n",
      "Validation loss: 0.8029875787473751....\n",
      "-----------------------------------\n",
      "Training loss: 0.7458382064582779....\n",
      "Validation loss: 0.80201598913699....\n",
      "-----------------------------------\n",
      "Training loss: 0.7448351491419246....\n",
      "Validation loss: 0.8010469995074564....\n",
      "-----------------------------------\n",
      "Training loss: 0.7438347111757008....\n",
      "Validation loss: 0.8000810154147502....\n",
      "-----------------------------------\n",
      "Training loss: 0.7428366160556739....\n",
      "Validation loss: 0.7991178204277305....\n",
      "-----------------------------------\n",
      "Training loss: 0.7418409569554338....\n",
      "Validation loss: 0.7981573996642244....\n",
      "-----------------------------------\n",
      "Training loss: 0.7408481583530167....\n",
      "Validation loss: 0.7971996922704478....\n",
      "-----------------------------------\n",
      "Training loss: 0.7398580835232861....\n",
      "Validation loss: 0.7962448017420788....\n",
      "-----------------------------------\n",
      "Training loss: 0.7388707131178787....\n",
      "Validation loss: 0.7952927418392838....\n",
      "-----------------------------------\n",
      "Training loss: 0.737886340572211....\n",
      "Validation loss: 0.7943432110784724....\n",
      "-----------------------------------\n",
      "Training loss: 0.7369044550119811....\n",
      "Validation loss: 0.7933961754089228....\n",
      "-----------------------------------\n",
      "Training loss: 0.7359248063566491....\n",
      "Validation loss: 0.7924516644658243....\n",
      "-----------------------------------\n",
      "Training loss: 0.7349473568912669....\n",
      "Validation loss: 0.7915098283148493....\n",
      "-----------------------------------\n",
      "Training loss: 0.7339725898121435....\n",
      "Validation loss: 0.7905707974430091....\n",
      "-----------------------------------\n",
      "Training loss: 0.7330006345413478....\n",
      "Validation loss: 0.7896344271230221....\n",
      "-----------------------------------\n",
      "Training loss: 0.7320312270887281....\n",
      "Validation loss: 0.7887006435273379....\n",
      "-----------------------------------\n",
      "Training loss: 0.7310644108072364....\n",
      "Validation loss: 0.7877694626156061....\n",
      "-----------------------------------\n",
      "Training loss: 0.7301001855541432....\n",
      "Validation loss: 0.7868412327887987....\n",
      "-----------------------------------\n",
      "Training loss: 0.729138931742922....\n",
      "Validation loss: 0.7859155900152383....\n",
      "-----------------------------------\n",
      "Training loss: 0.7281803697284691....\n",
      "Validation loss: 0.7849922632134146....\n",
      "-----------------------------------\n",
      "Training loss: 0.7272241902302384....\n",
      "Validation loss: 0.7840714942398513....\n",
      "-----------------------------------\n",
      "Training loss: 0.7262709060947022....\n",
      "Validation loss: 0.7831535138045754....\n",
      "-----------------------------------\n",
      "Training loss: 0.7253200748529792....\n",
      "Validation loss: 0.7822378840359018....\n",
      "-----------------------------------\n",
      "Training loss: 0.7243710950845607....\n",
      "Validation loss: 0.7813248822696174....\n",
      "-----------------------------------\n",
      "Training loss: 0.7234246252739266....\n",
      "Validation loss: 0.7804148554646307....\n",
      "-----------------------------------\n",
      "Training loss: 0.7224807932070529....\n",
      "Validation loss: 0.7795074456656246....\n",
      "-----------------------------------\n",
      "Training loss: 0.7215395279434557....\n",
      "Validation loss: 0.7786029010526565....\n",
      "-----------------------------------\n",
      "Training loss: 0.7206008078772471....\n",
      "Validation loss: 0.7777009044598262....\n",
      "-----------------------------------\n",
      "Training loss: 0.7196644836090847....\n",
      "Validation loss: 0.7768013250855254....\n",
      "-----------------------------------\n",
      "Training loss: 0.7187306712722861....\n",
      "Validation loss: 0.7759045488377069....\n",
      "-----------------------------------\n",
      "Training loss: 0.7177997081475387....\n",
      "Validation loss: 0.7750104494686441....\n",
      "-----------------------------------\n",
      "Training loss: 0.716871226437568....\n",
      "Validation loss: 0.7741185091773555....\n",
      "-----------------------------------\n",
      "Training loss: 0.7159450819518209....\n",
      "Validation loss: 0.7732289492020457....\n",
      "-----------------------------------\n",
      "Training loss: 0.7150217980510902....\n",
      "Validation loss: 0.7723418848286205....\n",
      "-----------------------------------\n",
      "Training loss: 0.7141005649105222....\n",
      "Validation loss: 0.7714572702322637....\n",
      "-----------------------------------\n",
      "Training loss: 0.7131819280024088....\n",
      "Validation loss: 0.7705753041285138....\n",
      "-----------------------------------\n",
      "Training loss: 0.7122660268369629....\n",
      "Validation loss: 0.7696961360492819....\n",
      "-----------------------------------\n",
      "Training loss: 0.7113526743936166....\n",
      "Validation loss: 0.7688199213802516....\n",
      "-----------------------------------\n",
      "Training loss: 0.7104415158936904....\n",
      "Validation loss: 0.7679463113120978....\n",
      "-----------------------------------\n",
      "Training loss: 0.7095328032005247....\n",
      "Validation loss: 0.7670752563383112....\n",
      "-----------------------------------\n",
      "Training loss: 0.7086264484725752....\n",
      "Validation loss: 0.7662066226219244....\n",
      "-----------------------------------\n",
      "Training loss: 0.707722384757054....\n",
      "Validation loss: 0.765340321414433....\n",
      "-----------------------------------\n",
      "Training loss: 0.7068206859572167....\n",
      "Validation loss: 0.7644764456705869....\n",
      "-----------------------------------\n",
      "Training loss: 0.7059213978558794....\n",
      "Validation loss: 0.7636147296038261....\n",
      "-----------------------------------\n",
      "Training loss: 0.7050244007758191....\n",
      "Validation loss: 0.762755694152377....\n",
      "-----------------------------------\n",
      "Training loss: 0.7041301316345794....\n",
      "Validation loss: 0.7618992286320461....\n",
      "-----------------------------------\n",
      "Training loss: 0.7032385674560628....\n",
      "Validation loss: 0.7610446683394883....\n",
      "-----------------------------------\n",
      "Training loss: 0.7023495309684051....\n",
      "Validation loss: 0.7601924657836648....\n",
      "-----------------------------------\n",
      "Training loss: 0.701462972030423....\n",
      "Validation loss: 0.7593424895652923....\n",
      "-----------------------------------\n",
      "Training loss: 0.7005791081114993....\n",
      "Validation loss: 0.7584944880069526....\n",
      "-----------------------------------\n",
      "Training loss: 0.6996978506430745....\n",
      "Validation loss: 0.7576491052784989....\n",
      "-----------------------------------\n",
      "Training loss: 0.6988190581453434....\n",
      "Validation loss: 0.7568058981721033....\n",
      "-----------------------------------\n",
      "Training loss: 0.6979424683969021....\n",
      "Validation loss: 0.7559648900986679....\n",
      "-----------------------------------\n",
      "Training loss: 0.6970684389287742....\n",
      "Validation loss: 0.7551261176070134....\n",
      "-----------------------------------\n",
      "Training loss: 0.6961967081832109....\n",
      "Validation loss: 0.7542895765532356....\n",
      "-----------------------------------\n",
      "Training loss: 0.6953271200096403....\n",
      "Validation loss: 0.7534553765506613....\n",
      "-----------------------------------\n",
      "Training loss: 0.6944598529121339....\n",
      "Validation loss: 0.7526230095786925....\n",
      "-----------------------------------\n",
      "Training loss: 0.6935945277039238....\n",
      "Validation loss: 0.7517928997508516....\n",
      "-----------------------------------\n",
      "Training loss: 0.692731612192759....\n",
      "Validation loss: 0.7509648749413355....\n",
      "-----------------------------------\n",
      "Training loss: 0.6918708681880406....\n",
      "Validation loss: 0.7501389947642566....\n",
      "-----------------------------------\n",
      "Training loss: 0.6910126132707309....\n",
      "Validation loss: 0.7493154231373303....\n",
      "-----------------------------------\n",
      "Training loss: 0.7114797026971944....\n",
      "Validation loss: 0.705887723445934....\n",
      "-----------------------------------\n",
      "Training loss: 0.7105730094483486....\n",
      "Validation loss: 0.7050976280664735....\n",
      "-----------------------------------\n",
      "Training loss: 0.7096697502138793....\n",
      "Validation loss: 0.7043097988572797....\n",
      "-----------------------------------\n",
      "Training loss: 0.7087699909136551....\n",
      "Validation loss: 0.7035242680687698....\n",
      "-----------------------------------\n",
      "Training loss: 0.7078734796887025....\n",
      "Validation loss: 0.7027412881367473....\n",
      "-----------------------------------\n",
      "Training loss: 0.7069803409801042....\n",
      "Validation loss: 0.701960654507041....\n",
      "-----------------------------------\n",
      "Training loss: 0.7060900008144402....\n",
      "Validation loss: 0.7011824379957358....\n",
      "-----------------------------------\n",
      "Training loss: 0.7052029318552819....\n",
      "Validation loss: 0.7004068899851846....\n",
      "-----------------------------------\n",
      "Training loss: 0.7043191979107897....\n",
      "Validation loss: 0.6996332604156824....\n",
      "-----------------------------------\n",
      "Training loss: 0.7034383313714058....\n",
      "Validation loss: 0.6988618579532665....\n",
      "-----------------------------------\n",
      "Training loss: 0.7025605938562837....\n",
      "Validation loss: 0.6980925340341935....\n",
      "-----------------------------------\n",
      "Training loss: 0.7016859974086663....\n",
      "Validation loss: 0.6973253384308868....\n",
      "-----------------------------------\n",
      "Training loss: 0.7008145202432824....\n",
      "Validation loss: 0.6965601518869768....\n",
      "-----------------------------------\n",
      "Training loss: 0.6999460652755413....\n",
      "Validation loss: 0.6957970194165491....\n",
      "-----------------------------------\n",
      "Training loss: 0.699080331113783....\n",
      "Validation loss: 0.6950356049157455....\n",
      "-----------------------------------\n",
      "Training loss: 0.6982169732328415....\n",
      "Validation loss: 0.6942763727353886....\n",
      "-----------------------------------\n",
      "Training loss: 0.6973561168806645....\n",
      "Validation loss: 0.6935191129319461....\n",
      "-----------------------------------\n",
      "Training loss: 0.6964978653975393....\n",
      "Validation loss: 0.6927639226337363....\n",
      "-----------------------------------\n",
      "Training loss: 0.6956424536254407....\n",
      "Validation loss: 0.6920099933314702....\n",
      "-----------------------------------\n",
      "Training loss: 0.6947897524154623....\n",
      "Validation loss: 0.6912580443648453....\n",
      "-----------------------------------\n",
      "Training loss: 0.6939394561083608....\n",
      "Validation loss: 0.6905079941984007....\n",
      "-----------------------------------\n",
      "Training loss: 0.6930914388294558....\n",
      "Validation loss: 0.6897598705006558....\n",
      "-----------------------------------\n",
      "Training loss: 0.6922458825064287....\n",
      "Validation loss: 0.6890136613370695....\n",
      "-----------------------------------\n",
      "Training loss: 0.6914027169884439....\n",
      "Validation loss: 0.6882695104834693....\n",
      "-----------------------------------\n",
      "Training loss: 0.6905621113470964....\n",
      "Validation loss: 0.6875272194777592....\n",
      "-----------------------------------\n",
      "Training loss: 0.689723642171148....\n",
      "Validation loss: 0.6867867114248059....\n",
      "-----------------------------------\n",
      "Training loss: 0.6888872664090376....\n",
      "Validation loss: 0.6860476751155254....\n",
      "-----------------------------------\n",
      "Training loss: 0.6880529991375386....\n",
      "Validation loss: 0.6853108370906716....\n",
      "-----------------------------------\n",
      "Training loss: 0.6872213286895273....\n",
      "Validation loss: 0.6845758054397773....\n",
      "-----------------------------------\n",
      "Training loss: 0.6863921578047607....\n",
      "Validation loss: 0.6838427140141705....\n",
      "-----------------------------------\n",
      "Training loss: 0.685564968575771....\n",
      "Validation loss: 0.683111371098965....\n",
      "-----------------------------------\n",
      "Training loss: 0.6847400867580602....\n",
      "Validation loss: 0.682381836217351....\n",
      "-----------------------------------\n",
      "Training loss: 0.6839177771034043....\n",
      "Validation loss: 0.6816539401405562....\n",
      "-----------------------------------\n",
      "Training loss: 0.6830978109662088....\n",
      "Validation loss: 0.6809279955710242....\n",
      "-----------------------------------\n",
      "Training loss: 0.68228046542352....\n",
      "Validation loss: 0.6802040038669718....\n",
      "-----------------------------------\n",
      "Training loss: 0.6814654922504909....\n",
      "Validation loss: 0.6794817396945919....\n",
      "-----------------------------------\n",
      "Training loss: 0.6806525173930884....\n",
      "Validation loss: 0.6787613993378345....\n",
      "-----------------------------------\n",
      "Training loss: 0.6798418945761542....\n",
      "Validation loss: 0.6780428656806557....\n",
      "-----------------------------------\n",
      "Training loss: 0.6790334144549995....\n",
      "Validation loss: 0.6773262541988745....\n",
      "-----------------------------------\n",
      "Training loss: 0.6782271970990329....\n",
      "Validation loss: 0.6766117443830264....\n",
      "-----------------------------------\n",
      "Training loss: 0.6774231424733321....\n",
      "Validation loss: 0.6758990690650226....\n",
      "-----------------------------------\n",
      "Training loss: 0.6766213498981772....\n",
      "Validation loss: 0.6751880106571431....\n",
      "-----------------------------------\n",
      "Training loss: 0.67582152234939....\n",
      "Validation loss: 0.6744788703939557....\n",
      "-----------------------------------\n",
      "Training loss: 0.6750237808837892....\n",
      "Validation loss: 0.6737713480076565....\n",
      "-----------------------------------\n",
      "Training loss: 0.6742280315723619....\n",
      "Validation loss: 0.6730656976495686....\n",
      "-----------------------------------\n",
      "Training loss: 0.6734344951861346....\n",
      "Validation loss: 0.6723620810513651....\n",
      "-----------------------------------\n",
      "Training loss: 0.6726432053416554....\n",
      "Validation loss: 0.6716601406983821....\n",
      "-----------------------------------\n",
      "Training loss: 0.6718539960419463....\n",
      "Validation loss: 0.6709601963013297....\n",
      "-----------------------------------\n",
      "Training loss: 0.6710669131290216....\n",
      "Validation loss: 0.6702619140738004....\n",
      "-----------------------------------\n",
      "Training loss: 0.6702813864436894....\n",
      "Validation loss: 0.6695655219851323....\n",
      "-----------------------------------\n",
      "Training loss: 0.6694978126535092....\n",
      "Validation loss: 0.6688708207005438....\n",
      "-----------------------------------\n",
      "Training loss: 0.6687162965044321....\n",
      "Validation loss: 0.6681777046766681....\n",
      "-----------------------------------\n",
      "Training loss: 0.6679368521365422....\n",
      "Validation loss: 0.6674864026905746....\n",
      "-----------------------------------\n",
      "Training loss: 0.6671595600217232....\n",
      "Validation loss: 0.6667969394041677....\n",
      "-----------------------------------\n",
      "Training loss: 0.6663842591451798....\n",
      "Validation loss: 0.6661094944903198....\n",
      "-----------------------------------\n",
      "Training loss: 0.6656108576966975....\n",
      "Validation loss: 0.6654237148296658....\n",
      "-----------------------------------\n",
      "Training loss: 0.6648393869916757....\n",
      "Validation loss: 0.6647395476682518....\n",
      "-----------------------------------\n",
      "Training loss: 0.6640697232671626....\n",
      "Validation loss: 0.6640572050508665....\n",
      "-----------------------------------\n",
      "Training loss: 0.6633021742712188....\n",
      "Validation loss: 0.663376451137584....\n",
      "-----------------------------------\n",
      "Training loss: 0.662536106798625....\n",
      "Validation loss: 0.6626974767945176....\n",
      "-----------------------------------\n",
      "Training loss: 0.6617720687319107....\n",
      "Validation loss: 0.6620207936285484....\n",
      "-----------------------------------\n",
      "Training loss: 0.6610103047436993....\n",
      "Validation loss: 0.6613458956239919....\n",
      "-----------------------------------\n",
      "Training loss: 0.6602505329138679....\n",
      "Validation loss: 0.6606729142648533....\n",
      "-----------------------------------\n",
      "Training loss: 0.6594925090280248....\n",
      "Validation loss: 0.6600013780069178....\n",
      "-----------------------------------\n",
      "Training loss: 0.6587363071668538....\n",
      "Validation loss: 0.6593311917317541....\n",
      "-----------------------------------\n",
      "Training loss: 0.6579816909494509....\n",
      "Validation loss: 0.6586627022129423....\n",
      "-----------------------------------\n",
      "Training loss: 0.6572289140951435....\n",
      "Validation loss: 0.6579957600493161....\n",
      "-----------------------------------\n",
      "Training loss: 0.6564780230540388....\n",
      "Validation loss: 0.6573304278659333....\n",
      "-----------------------------------\n",
      "Training loss: 0.6557290405440298....\n",
      "Validation loss: 0.6566666032800375....\n",
      "-----------------------------------\n",
      "Training loss: 0.6549818692744175....\n",
      "Validation loss: 0.6560046440549947....\n",
      "-----------------------------------\n",
      "Training loss: 0.6542366565522373....\n",
      "Validation loss: 0.6553445250854821....\n",
      "-----------------------------------\n",
      "Training loss: 0.6534932347460436....\n",
      "Validation loss: 0.6546859978828814....\n",
      "-----------------------------------\n",
      "Training loss: 0.6527518022379489....\n",
      "Validation loss: 0.6540286967110699....\n",
      "-----------------------------------\n",
      "Training loss: 0.65201242722146....\n",
      "Validation loss: 0.6533729865286455....\n",
      "-----------------------------------\n",
      "Training loss: 0.651274844123689....\n",
      "Validation loss: 0.6527189355956439....\n",
      "-----------------------------------\n",
      "Training loss: 0.6505392056715286....\n",
      "Validation loss: 0.6520662896809711....\n",
      "-----------------------------------\n",
      "Training loss: 0.6498052841158396....\n",
      "Validation loss: 0.6514151579940883....\n",
      "-----------------------------------\n",
      "Training loss: 0.649072967625298....\n",
      "Validation loss: 0.6507657113987535....\n",
      "-----------------------------------\n",
      "Training loss: 0.6483429255292145....\n",
      "Validation loss: 0.6501175527570598....\n",
      "-----------------------------------\n",
      "Training loss: 0.6476146876474002....\n",
      "Validation loss: 0.6494709860980309....\n",
      "-----------------------------------\n",
      "Training loss: 0.6468883127537298....\n",
      "Validation loss: 0.6488261199632559....\n",
      "-----------------------------------\n",
      "Training loss: 0.6461637189698971....\n",
      "Validation loss: 0.6481827978807313....\n",
      "-----------------------------------\n",
      "Training loss: 0.6454407717375851....\n",
      "Validation loss: 0.6475412515509024....\n",
      "-----------------------------------\n",
      "Training loss: 0.6447196369483582....\n",
      "Validation loss: 0.6469013687149547....\n",
      "-----------------------------------\n",
      "Training loss: 0.6440000040951308....\n",
      "Validation loss: 0.6462629495399455....\n",
      "-----------------------------------\n",
      "Training loss: 0.6432819040166283....\n",
      "Validation loss: 0.6456263919616042....\n",
      "-----------------------------------\n",
      "Training loss: 0.642565518268644....\n",
      "Validation loss: 0.6449913749961079....\n",
      "-----------------------------------\n",
      "Training loss: 0.6418509400289781....\n",
      "Validation loss: 0.6443575887782281....\n",
      "-----------------------------------\n",
      "Training loss: 0.6411379323257992....\n",
      "Validation loss: 0.6437253318878595....\n",
      "-----------------------------------\n",
      "Training loss: 0.6404264961417983....\n",
      "Validation loss: 0.6430943940490175....\n",
      "-----------------------------------\n",
      "Training loss: 0.6397167953383761....\n",
      "Validation loss: 0.6424650885249892....\n",
      "-----------------------------------\n",
      "Training loss: 0.6390088163554546....\n",
      "Validation loss: 0.6418374882619093....\n",
      "-----------------------------------\n",
      "Training loss: 0.6383026834081632....\n",
      "Validation loss: 0.6412115511996869....\n",
      "-----------------------------------\n",
      "Training loss: 0.6375985612663048....\n",
      "Validation loss: 0.6405871696586068....\n",
      "-----------------------------------\n",
      "Training loss: 0.6368962943808895....\n",
      "Validation loss: 0.6399642387002118....\n",
      "-----------------------------------\n",
      "Training loss: 0.6361956597014484....\n",
      "Validation loss: 0.6393427869017522....\n",
      "-----------------------------------\n",
      "Training loss: 0.6354966001898031....\n",
      "Validation loss: 0.6387229499506711....\n",
      "-----------------------------------\n",
      "Training loss: 0.6347993419179595....\n",
      "Validation loss: 0.6381046481646485....\n",
      "-----------------------------------\n",
      "Training loss: 0.6341034847950419....\n",
      "Validation loss: 0.6374878901869047....\n",
      "-----------------------------------\n",
      "Training loss: 0.6334091007953219....\n",
      "Validation loss: 0.6368724841000751....\n",
      "-----------------------------------\n",
      "Training loss: 0.6327163453710759....\n",
      "Validation loss: 0.6362586454413319....\n",
      "-----------------------------------\n",
      "Training loss: 0.6320252847322482....\n",
      "Validation loss: 0.6356462500614014....\n",
      "-----------------------------------\n",
      "Training loss: 0.6313359486316666....\n",
      "Validation loss: 0.6350353789987778....\n",
      "-----------------------------------\n",
      "Training loss: 0.6306485295415271....\n",
      "Validation loss: 0.6344261878298705....\n",
      "-----------------------------------\n",
      "Training loss: 0.6299627825345244....\n",
      "Validation loss: 0.6338184929273457....\n",
      "-----------------------------------\n",
      "Training loss: 0.6292784295827847....\n",
      "Validation loss: 0.6332121535913838....\n",
      "-----------------------------------\n",
      "Training loss: 0.6285955054795603....\n",
      "Validation loss: 0.632607548977546....\n",
      "-----------------------------------\n",
      "Training loss: 0.6279142690830268....\n",
      "Validation loss: 0.6320046682190955....\n",
      "-----------------------------------\n",
      "Training loss: 0.6272346153419458....\n",
      "Validation loss: 0.631403417172609....\n",
      "-----------------------------------\n",
      "Training loss: 0.6265565237055598....\n",
      "Validation loss: 0.6308034544669426....\n",
      "-----------------------------------\n",
      "Training loss: 0.6258803710478048....\n",
      "Validation loss: 0.6302050482770544....\n",
      "-----------------------------------\n",
      "Training loss: 0.625205881928703....\n",
      "Validation loss: 0.6296083032132392....\n",
      "-----------------------------------\n",
      "Training loss: 0.6245331699893234....\n",
      "Validation loss: 0.6290129864019602....\n",
      "-----------------------------------\n",
      "Training loss: 0.6238618158116065....\n",
      "Validation loss: 0.628419108502387....\n",
      "-----------------------------------\n",
      "Training loss: 0.6231921637570567....\n",
      "Validation loss: 0.6278267086154361....\n",
      "-----------------------------------\n",
      "Training loss: 0.6225241881653688....\n",
      "Validation loss: 0.6272356891414294....\n",
      "-----------------------------------\n",
      "Training loss: 0.6218578806152588....\n",
      "Validation loss: 0.6266461313627224....\n",
      "-----------------------------------\n",
      "Training loss: 0.6211930346531376....\n",
      "Validation loss: 0.6260582239549458....\n",
      "-----------------------------------\n",
      "Training loss: 0.6205299755538825....\n",
      "Validation loss: 0.6254717172586093....\n",
      "-----------------------------------\n",
      "Training loss: 0.6198685494833426....\n",
      "Validation loss: 0.6248867056388461....\n",
      "-----------------------------------\n",
      "Training loss: 0.6192088448829881....\n",
      "Validation loss: 0.6243029845579554....\n",
      "-----------------------------------\n",
      "Training loss: 0.618550755072458....\n",
      "Validation loss: 0.6237205536247958....\n",
      "-----------------------------------\n",
      "Training loss: 0.617894384717439....\n",
      "Validation loss: 0.6231395116149439....\n",
      "-----------------------------------\n",
      "Training loss: 0.6172396389343987....\n",
      "Validation loss: 0.6225598390734507....\n",
      "-----------------------------------\n",
      "Training loss: 0.6165863301792324....\n",
      "Validation loss: 0.6219817437580568....\n",
      "-----------------------------------\n",
      "Training loss: 0.6159346426509896....\n",
      "Validation loss: 0.621405074374556....\n",
      "-----------------------------------\n",
      "Training loss: 0.6152844375307527....\n",
      "Validation loss: 0.6208297490505218....\n",
      "-----------------------------------\n",
      "Training loss: 0.6146358126436524....\n",
      "Validation loss: 0.6202558246884465....\n",
      "-----------------------------------\n",
      "Training loss: 0.6139886049653207....\n",
      "Validation loss: 0.6196835561116651....\n",
      "-----------------------------------\n",
      "Training loss: 0.6133427119240082....\n",
      "Validation loss: 0.6191127938929574....\n",
      "-----------------------------------\n",
      "Training loss: 0.612698348651886....\n",
      "Validation loss: 0.6185434380134408....\n",
      "-----------------------------------\n",
      "Training loss: 0.6120554888174935....\n",
      "Validation loss: 0.6179754582980311....\n",
      "-----------------------------------\n",
      "Training loss: 0.6114141377425284....\n",
      "Validation loss: 0.617408760824916....\n",
      "-----------------------------------\n",
      "Training loss: 0.6107744594070996....\n",
      "Validation loss: 0.6168435200031961....\n",
      "-----------------------------------\n",
      "Training loss: 0.610136523413904....\n",
      "Validation loss: 0.6162796568136973....\n",
      "-----------------------------------\n",
      "Training loss: 0.6094998140032051....\n",
      "Validation loss: 0.6157170933502799....\n",
      "-----------------------------------\n",
      "Training loss: 0.6088645036389697....\n",
      "Validation loss: 0.6151560919299763....\n",
      "-----------------------------------\n",
      "Training loss: 0.6082307311936184....\n",
      "Validation loss: 0.6145966035880537....\n",
      "-----------------------------------\n",
      "Training loss: 0.6075985722814291....\n",
      "Validation loss: 0.6140386896286553....\n",
      "-----------------------------------\n",
      "Training loss: 0.6069677888197071....\n",
      "Validation loss: 0.6134819639671184....\n",
      "-----------------------------------\n",
      "Training loss: 0.6063385267924617....\n",
      "Validation loss: 0.6129264289165708....\n",
      "-----------------------------------\n",
      "Training loss: 0.6057105178206803....\n",
      "Validation loss: 0.6123721986580689....\n",
      "-----------------------------------\n",
      "Training loss: 0.6050840478971283....\n",
      "Validation loss: 0.6118192481458252....\n",
      "-----------------------------------\n",
      "Training loss: 0.6044591678127904....\n",
      "Validation loss: 0.611267685387726....\n",
      "-----------------------------------\n",
      "Training loss: 0.6038357189465241....\n",
      "Validation loss: 0.6107175368204939....\n",
      "-----------------------------------\n",
      "Training loss: 0.603213707706818....\n",
      "Validation loss: 0.610168608550465....\n",
      "-----------------------------------\n",
      "Training loss: 0.6025932054024988....\n",
      "Validation loss: 0.6096212144258729....\n",
      "-----------------------------------\n",
      "Training loss: 0.601974048196668....\n",
      "Validation loss: 0.6090751464899291....\n",
      "-----------------------------------\n",
      "Training loss: 0.6013565564449911....\n",
      "Validation loss: 0.608530298858154....\n",
      "-----------------------------------\n",
      "Training loss: 0.6007405700026177....\n",
      "Validation loss: 0.6079867289722605....\n",
      "-----------------------------------\n",
      "Training loss: 0.6001260553133699....\n",
      "Validation loss: 0.6074445570168495....\n",
      "-----------------------------------\n",
      "Training loss: 0.5995129972837596....\n",
      "Validation loss: 0.6069035619824984....\n",
      "-----------------------------------\n",
      "Training loss: 0.5989014051886555....\n",
      "Validation loss: 0.6063638450796123....\n",
      "-----------------------------------\n",
      "Training loss: 0.5982911592836183....\n",
      "Validation loss: 0.6058252884662008....\n",
      "-----------------------------------\n",
      "Training loss: 0.5976821903889666....\n",
      "Validation loss: 0.6052883641653766....\n",
      "-----------------------------------\n",
      "Training loss: 0.5970749210678747....\n",
      "Validation loss: 0.6047528280312454....\n",
      "-----------------------------------\n",
      "Training loss: 0.596469082607392....\n",
      "Validation loss: 0.6042184075630458....\n",
      "-----------------------------------\n",
      "Training loss: 0.5958647338507859....\n",
      "Validation loss: 0.6036850205837333....\n",
      "-----------------------------------\n",
      "Training loss: 0.5952616595146104....\n",
      "Validation loss: 0.6031528825402236....\n",
      "-----------------------------------\n",
      "Training loss: 0.5946599762642532....\n",
      "Validation loss: 0.6026220879151882....\n",
      "-----------------------------------\n",
      "Training loss: 0.5940597301334385....\n",
      "Validation loss: 0.602092566454425....\n",
      "-----------------------------------\n",
      "Training loss: 0.5934609917225411....\n",
      "Validation loss: 0.601564560860953....\n",
      "-----------------------------------\n",
      "Training loss: 0.5928636165144908....\n",
      "Validation loss: 0.601037895792032....\n",
      "-----------------------------------\n",
      "Training loss: 0.5922676718035114....\n",
      "Validation loss: 0.6005125719141781....\n",
      "-----------------------------------\n",
      "Training loss: 0.5916731925597387....\n",
      "Validation loss: 0.5999884857010116....\n",
      "-----------------------------------\n",
      "Training loss: 0.591080169100373....\n",
      "Validation loss: 0.5994656648358677....\n",
      "-----------------------------------\n",
      "Training loss: 0.590488626167205....\n",
      "Validation loss: 0.598944025027093....\n",
      "-----------------------------------\n",
      "Training loss: 0.5898983754570688....\n",
      "Validation loss: 0.5984237044712327....\n",
      "-----------------------------------\n",
      "Training loss: 0.5893096252484666....\n",
      "Validation loss: 0.5979044940920549....\n",
      "-----------------------------------\n",
      "Training loss: 0.588722163452601....\n",
      "Validation loss: 0.5973864993627097....\n",
      "-----------------------------------\n",
      "Training loss: 0.5881360644025326....\n",
      "Validation loss: 0.5968698209664026....\n",
      "-----------------------------------\n",
      "Training loss: 0.5875514601728942....\n",
      "Validation loss: 0.5963545448288559....\n",
      "-----------------------------------\n",
      "Training loss: 0.5869682196437204....\n",
      "Validation loss: 0.59584042915747....\n",
      "-----------------------------------\n",
      "Training loss: 0.5863863602538419....\n",
      "Validation loss: 0.5953275596125618....\n",
      "-----------------------------------\n",
      "Training loss: 0.5858060423384388....\n",
      "Validation loss: 0.5948158990817028....\n",
      "-----------------------------------\n",
      "Training loss: 0.585227027182162....\n",
      "Validation loss: 0.5943055098527567....\n",
      "-----------------------------------\n",
      "Training loss: 0.58464929509937....\n",
      "Validation loss: 0.5937963243628969....\n",
      "-----------------------------------\n",
      "Training loss: 0.584072873139897....\n",
      "Validation loss: 0.5932883937434124....\n",
      "-----------------------------------\n",
      "Training loss: 0.5834977732680771....\n",
      "Validation loss: 0.5927816879415798....\n",
      "-----------------------------------\n",
      "Training loss: 0.5829239290164093....\n",
      "Validation loss: 0.5922762412343892....\n",
      "-----------------------------------\n",
      "Training loss: 0.582351140134583....\n",
      "Validation loss: 0.5917718351748115....\n",
      "-----------------------------------\n",
      "Training loss: 0.5817795977352886....\n",
      "Validation loss: 0.5912687421185365....\n",
      "-----------------------------------\n",
      "Training loss: 0.5812092850896469....\n",
      "Validation loss: 0.5907668101253285....\n",
      "-----------------------------------\n",
      "Training loss: 0.580640182533222....\n",
      "Validation loss: 0.5902661065642852....\n",
      "-----------------------------------\n",
      "Training loss: 0.5800723111568541....\n",
      "Validation loss: 0.5897664737733663....\n",
      "-----------------------------------\n",
      "Training loss: 0.5795055674577099....\n",
      "Validation loss: 0.5892677916759163....\n",
      "-----------------------------------\n",
      "Training loss: 0.578940094472665....\n",
      "Validation loss: 0.5887704017087863....\n",
      "-----------------------------------\n",
      "Training loss: 0.5783759770644954....\n",
      "Validation loss: 0.5882742646421589....\n",
      "-----------------------------------\n",
      "Training loss: 0.577813108789757....\n",
      "Validation loss: 0.5877792518560471....\n",
      "-----------------------------------\n",
      "Training loss: 0.577251485009226....\n",
      "Validation loss: 0.5872854041597527....\n",
      "-----------------------------------\n",
      "Training loss: 0.5766910838819365....\n",
      "Validation loss: 0.5867928119813214....\n",
      "-----------------------------------\n",
      "Training loss: 0.5761318538761498....\n",
      "Validation loss: 0.586301411613201....\n",
      "-----------------------------------\n",
      "Training loss: 0.5755739195456908....\n",
      "Validation loss: 0.5858111393887399....\n",
      "-----------------------------------\n",
      "Training loss: 0.5750172950806104....\n",
      "Validation loss: 0.585321944902016....\n",
      "-----------------------------------\n",
      "Training loss: 0.5744618288474936....\n",
      "Validation loss: 0.5848337832137355....\n",
      "-----------------------------------\n",
      "Training loss: 0.5739074136008177....\n",
      "Validation loss: 0.5843465979097217....\n",
      "-----------------------------------\n",
      "Training loss: 0.5733541695522176....\n",
      "Validation loss: 0.5838604641405766....\n",
      "-----------------------------------\n",
      "Training loss: 0.5728020328946376....\n",
      "Validation loss: 0.58337540526964....\n",
      "-----------------------------------\n",
      "Training loss: 0.5722512180282436....\n",
      "Validation loss: 0.5828914627263917....\n",
      "-----------------------------------\n",
      "Training loss: 0.5717018260435198....\n",
      "Validation loss: 0.582408368812816....\n",
      "-----------------------------------\n",
      "Training loss: 0.5711536677388042....\n",
      "Validation loss: 0.5819263768460785....\n",
      "-----------------------------------\n",
      "Training loss: 0.5706066521596497....\n",
      "Validation loss: 0.581445401442588....\n",
      "-----------------------------------\n",
      "Training loss: 0.5700606330329243....\n",
      "Validation loss: 0.5809654715437635....\n",
      "-----------------------------------\n",
      "Training loss: 0.5695157592224205....\n",
      "Validation loss: 0.5804866326231848....\n",
      "-----------------------------------\n",
      "Training loss: 0.5689721821730845....\n",
      "Validation loss: 0.580008960908095....\n",
      "-----------------------------------\n",
      "Training loss: 0.5684296848938512....\n",
      "Validation loss: 0.5795323937942585....\n",
      "-----------------------------------\n",
      "Training loss: 0.5678883358142043....\n",
      "Validation loss: 0.5790568706171434....\n",
      "-----------------------------------\n",
      "Training loss: 0.5673480825319832....\n",
      "Validation loss: 0.5785823302480638....\n",
      "-----------------------------------\n",
      "Training loss: 0.5668089970756746....\n",
      "Validation loss: 0.5781089221117552....\n",
      "-----------------------------------\n",
      "Training loss: 0.5662710505393624....\n",
      "Validation loss: 0.5776365794422242....\n",
      "-----------------------------------\n",
      "Training loss: 0.5657343416611206....\n",
      "Validation loss: 0.577165367821668....\n",
      "-----------------------------------\n",
      "Training loss: 0.5651987557313738....\n",
      "Validation loss: 0.5766954705332503....\n",
      "-----------------------------------\n",
      "Training loss: 0.5646643472301753....\n",
      "Validation loss: 0.5762267133339242....\n",
      "-----------------------------------\n",
      "Training loss: 0.5641310280831706....\n",
      "Validation loss: 0.5757588425014873....\n",
      "-----------------------------------\n",
      "Training loss: 0.5635987605222388....\n",
      "Validation loss: 0.5752918339355249....\n",
      "-----------------------------------\n",
      "Training loss: 0.5630674925418209....\n",
      "Validation loss: 0.5748258639765864....\n",
      "-----------------------------------\n",
      "Training loss: 0.5625374335750613....\n",
      "Validation loss: 0.5743607672285295....\n",
      "-----------------------------------\n",
      "Training loss: 0.5620085011050542....\n",
      "Validation loss: 0.5738967421669121....\n",
      "-----------------------------------\n",
      "Training loss: 0.5614808284983355....\n",
      "Validation loss: 0.5734337711761072....\n",
      "-----------------------------------\n",
      "Training loss: 0.5609542559566786....\n",
      "Validation loss: 0.5729718828927801....\n",
      "-----------------------------------\n",
      "Training loss: 0.5604289212981104....\n",
      "Validation loss: 0.5725111721443359....\n",
      "-----------------------------------\n",
      "Training loss: 0.5599048023760869....\n",
      "Validation loss: 0.5720516211475705....\n",
      "-----------------------------------\n",
      "Training loss: 0.5593818853367405....\n",
      "Validation loss: 0.571593065861625....\n",
      "-----------------------------------\n",
      "Training loss: 0.5588600734770464....\n",
      "Validation loss: 0.5711353891952919....\n",
      "-----------------------------------\n",
      "Training loss: 0.5583394668682008....\n",
      "Validation loss: 0.5706787282968261....\n",
      "-----------------------------------\n",
      "Training loss: 0.5578199869993005....\n",
      "Validation loss: 0.5702228491369483....\n",
      "-----------------------------------\n",
      "Training loss: 0.5573017142305031....\n",
      "Validation loss: 0.5697678588268836....\n",
      "-----------------------------------\n",
      "Training loss: 0.5567844921963562....\n",
      "Validation loss: 0.5693138490251013....\n",
      "-----------------------------------\n",
      "Training loss: 0.5562683563323241....\n",
      "Validation loss: 0.5688609726430359....\n",
      "-----------------------------------\n",
      "Training loss: 0.5557534232231737....\n",
      "Validation loss: 0.5684090843982377....\n",
      "-----------------------------------\n",
      "Training loss: 0.5552396528709183....\n",
      "Validation loss: 0.5679581685688012....\n",
      "-----------------------------------\n",
      "Training loss: 0.554727110489667....\n",
      "Validation loss: 0.5675083208349337....\n",
      "-----------------------------------\n",
      "Training loss: 0.5542157308281891....\n",
      "Validation loss: 0.5670595654610057....\n",
      "-----------------------------------\n",
      "Training loss: 0.5537053542112527....\n",
      "Validation loss: 0.5666117634311133....\n",
      "-----------------------------------\n",
      "Training loss: 0.5531961011331471....\n",
      "Validation loss: 0.5661648807046351....\n",
      "-----------------------------------\n",
      "Training loss: 0.5526879137265474....\n",
      "Validation loss: 0.5657190916684052....\n",
      "-----------------------------------\n",
      "Training loss: 0.5521809403205243....\n",
      "Validation loss: 0.5652741796880648....\n",
      "-----------------------------------\n",
      "Training loss: 0.5516750855680767....\n",
      "Validation loss: 0.5648301933673894....\n",
      "-----------------------------------\n",
      "Training loss: 0.5511702454345263....\n",
      "Validation loss: 0.5643869344730486....\n",
      "-----------------------------------\n",
      "Training loss: 0.5506665247257602....\n",
      "Validation loss: 0.5639447763102935....\n",
      "-----------------------------------\n",
      "Training loss: 0.5501639522447691....\n",
      "Validation loss: 0.5635036765751958....\n",
      "-----------------------------------\n",
      "Training loss: 0.5496625352102487....\n",
      "Validation loss: 0.5630635957446757....\n",
      "-----------------------------------\n",
      "Training loss: 0.5491622892793611....\n",
      "Validation loss: 0.5626245741709134....\n",
      "-----------------------------------\n",
      "Training loss: 0.5486631109505516....\n",
      "Validation loss: 0.5621864308647538....\n",
      "-----------------------------------\n",
      "Training loss: 0.5481648504149436....\n",
      "Validation loss: 0.5617491853319386....\n",
      "-----------------------------------\n",
      "Training loss: 0.5476675796733695....\n",
      "Validation loss: 0.5613129132970706....\n",
      "-----------------------------------\n",
      "Training loss: 0.5471714204734355....\n",
      "Validation loss: 0.5608776694184705....\n",
      "-----------------------------------\n",
      "Training loss: 0.546676220112934....\n",
      "Validation loss: 0.560443327994145....\n",
      "-----------------------------------\n",
      "Training loss: 0.5461817719553438....\n",
      "Validation loss: 0.5600099187099213....\n",
      "-----------------------------------\n",
      "Training loss: 0.545688255653872....\n",
      "Validation loss: 0.5595776114105737....\n",
      "-----------------------------------\n",
      "Training loss: 0.5451957944490756....\n",
      "Validation loss: 0.5591461553537213....\n",
      "-----------------------------------\n",
      "Training loss: 0.5447041404465395....\n",
      "Validation loss: 0.5587156074322905....\n",
      "-----------------------------------\n",
      "Training loss: 0.5442133586587188....\n",
      "Validation loss: 0.5582860239928703....\n",
      "-----------------------------------\n",
      "Training loss: 0.5437236056904479....\n",
      "Validation loss: 0.5578575164792653....\n",
      "-----------------------------------\n",
      "Training loss: 0.5432349659634366....\n",
      "Validation loss: 0.5574299533487757....\n",
      "-----------------------------------\n",
      "Training loss: 0.5427474394653878....\n",
      "Validation loss: 0.5570033649601999....\n",
      "-----------------------------------\n",
      "Training loss: 0.5422609797156127....\n",
      "Validation loss: 0.5565777179430536....\n",
      "-----------------------------------\n",
      "Training loss: 0.5417755437181955....\n",
      "Validation loss: 0.5561530746303539....\n",
      "-----------------------------------\n",
      "Training loss: 0.5412911426906173....\n",
      "Validation loss: 0.5557293331944435....\n",
      "-----------------------------------\n",
      "Training loss: 0.5408078035925297....\n",
      "Validation loss: 0.5553065384211934....\n",
      "-----------------------------------\n",
      "Training loss: 0.5403254345629435....\n",
      "Validation loss: 0.5548847259761118....\n",
      "-----------------------------------\n",
      "Training loss: 0.539844111565015....\n",
      "Validation loss: 0.5544640349349715....\n",
      "-----------------------------------\n",
      "Training loss: 0.5393637187903034....\n",
      "Validation loss: 0.5540442907490076....\n",
      "-----------------------------------\n",
      "Training loss: 0.5388844513518118....\n",
      "Validation loss: 0.5536253666849966....\n",
      "-----------------------------------\n",
      "Training loss: 0.538406156097393....\n",
      "Validation loss: 0.5532071215030764....\n",
      "-----------------------------------\n",
      "Training loss: 0.5379289369642359....\n",
      "Validation loss: 0.5527893861647656....\n",
      "-----------------------------------\n",
      "Training loss: 0.5374527499381535....\n",
      "Validation loss: 0.5523727091672536....\n",
      "-----------------------------------\n",
      "Training loss: 0.5369775582715876....\n",
      "Validation loss: 0.5519570484197848....\n",
      "-----------------------------------\n",
      "Training loss: 0.5365033942108234....\n",
      "Validation loss: 0.551542385424721....\n",
      "-----------------------------------\n",
      "Training loss: 0.5360302999952401....\n",
      "Validation loss: 0.5511285635361773....\n",
      "-----------------------------------\n",
      "Training loss: 0.5355582339421654....\n",
      "Validation loss: 0.5507156398317993....\n",
      "-----------------------------------\n",
      "Training loss: 0.535087088920877....\n",
      "Validation loss: 0.5503035898426595....\n",
      "-----------------------------------\n",
      "Training loss: 0.5346168278528419....\n",
      "Validation loss: 0.5498923785500207....\n",
      "-----------------------------------\n",
      "Training loss: 0.5341475304899906....\n",
      "Validation loss: 0.5494819876173369....\n",
      "-----------------------------------\n",
      "Training loss: 0.5336791253453779....\n",
      "Validation loss: 0.549072432266314....\n",
      "-----------------------------------\n",
      "Training loss: 0.5332115658831816....\n",
      "Validation loss: 0.5486636076086641....\n",
      "-----------------------------------\n",
      "Training loss: 0.5327448087824578....\n",
      "Validation loss: 0.5482557706192436....\n",
      "-----------------------------------\n",
      "Training loss: 0.5322790441801357....\n",
      "Validation loss: 0.5478487319171654....\n",
      "-----------------------------------\n",
      "Training loss: 0.5318143069482182....\n",
      "Validation loss: 0.547442601053743....\n",
      "-----------------------------------\n",
      "Training loss: 0.5313506599308826....\n",
      "Validation loss: 0.5470373642361805....\n",
      "-----------------------------------\n",
      "Training loss: 0.5308879832869173....\n",
      "Validation loss: 0.5466329812174991....\n",
      "-----------------------------------\n",
      "Training loss: 0.5304260033136103....\n",
      "Validation loss: 0.546229470242138....\n",
      "-----------------------------------\n",
      "Training loss: 0.5299650495995433....\n",
      "Validation loss: 0.5458269444980961....\n",
      "-----------------------------------\n",
      "Training loss: 0.5295050621531104....\n",
      "Validation loss: 0.545425324369327....\n",
      "-----------------------------------\n",
      "Training loss: 0.5290460169360531....\n",
      "Validation loss: 0.5450245136627169....\n",
      "-----------------------------------\n",
      "Training loss: 0.5285879393710305....\n",
      "Validation loss: 0.5446245034613921....\n",
      "-----------------------------------\n",
      "Training loss: 0.5281307946166377....\n",
      "Validation loss: 0.5442252450272442....\n",
      "-----------------------------------\n",
      "Training loss: 0.5276744912594624....\n",
      "Validation loss: 0.5438269101990104....\n",
      "-----------------------------------\n",
      "Training loss: 0.5272191525091819....\n",
      "Validation loss: 0.5434294472023514....\n",
      "-----------------------------------\n",
      "Training loss: 0.5267646434283121....\n",
      "Validation loss: 0.5430327721294477....\n",
      "-----------------------------------\n",
      "Training loss: 0.5263109912696562....\n",
      "Validation loss: 0.5426367224008851....\n",
      "-----------------------------------\n",
      "Training loss: 0.5258582136016455....\n",
      "Validation loss: 0.5422412692440732....\n",
      "-----------------------------------\n",
      "Training loss: 0.5254062802989552....\n",
      "Validation loss: 0.541846649429863....\n",
      "-----------------------------------\n",
      "Training loss: 0.5249552893672187....\n",
      "Validation loss: 0.5414528512566784....\n",
      "-----------------------------------\n",
      "Training loss: 0.5245052462860039....\n",
      "Validation loss: 0.54105996244581....\n",
      "-----------------------------------\n",
      "Training loss: 0.5240560446448554....\n",
      "Validation loss: 0.5406677893234101....\n",
      "-----------------------------------\n",
      "Training loss: 0.5236077738041113....\n",
      "Validation loss: 0.5402764741093452....\n",
      "-----------------------------------\n",
      "Training loss: 0.5231605191508285....\n",
      "Validation loss: 0.539886050158425....\n",
      "-----------------------------------\n",
      "Training loss: 0.5227142121218765....\n",
      "Validation loss: 0.5394965305202377....\n",
      "-----------------------------------\n",
      "Training loss: 0.5222688104941134....\n",
      "Validation loss: 0.5391077350549326....\n",
      "-----------------------------------\n",
      "Training loss: 0.5218243409571056....\n",
      "Validation loss: 0.5387198546275062....\n",
      "-----------------------------------\n",
      "Training loss: 0.5213808524621241....\n",
      "Validation loss: 0.5383324713474236....\n",
      "-----------------------------------\n",
      "Training loss: 0.5209383468052051....\n",
      "Validation loss: 0.5379458830735644....\n",
      "-----------------------------------\n",
      "Training loss: 0.5204968448759935....\n",
      "Validation loss: 0.5375602550786053....\n",
      "-----------------------------------\n",
      "Training loss: 0.5200562992428764....\n",
      "Validation loss: 0.5371754709925213....\n",
      "-----------------------------------\n",
      "Training loss: 0.5196166741259046....\n",
      "Validation loss: 0.5367916190636219....\n",
      "-----------------------------------\n",
      "Training loss: 0.5191779261444047....\n",
      "Validation loss: 0.5364084580360693....\n",
      "-----------------------------------\n",
      "Training loss: 0.518740012050444....\n",
      "Validation loss: 0.5360260065277761....\n",
      "-----------------------------------\n",
      "Training loss: 0.518302936094327....\n",
      "Validation loss: 0.5356442903832744....\n",
      "-----------------------------------\n",
      "Training loss: 0.5178667530727473....\n",
      "Validation loss: 0.5352634075509806....\n",
      "-----------------------------------\n",
      "Training loss: 0.5174314289987925....\n",
      "Validation loss: 0.5348833398463884....\n",
      "-----------------------------------\n",
      "Training loss: 0.5169969915586553....\n",
      "Validation loss: 0.5345040137450475....\n",
      "-----------------------------------\n",
      "Training loss: 0.5165633529601372....\n",
      "Validation loss: 0.5341255993137957....\n",
      "-----------------------------------\n",
      "Training loss: 0.5161305092704271....\n",
      "Validation loss: 0.5337479666767005....\n",
      "-----------------------------------\n",
      "Training loss: 0.5156984722243511....\n",
      "Validation loss: 0.5333712833161037....\n",
      "-----------------------------------\n",
      "Training loss: 0.515267241224988....\n",
      "Validation loss: 0.5329955265479739....\n",
      "-----------------------------------\n",
      "Training loss: 0.5148369961505064....\n",
      "Validation loss: 0.5326205664851271....\n",
      "-----------------------------------\n",
      "Training loss: 0.5144076447892666....\n",
      "Validation loss: 0.5322464831956065....\n",
      "-----------------------------------\n",
      "Training loss: 0.5139790958633383....\n",
      "Validation loss: 0.5318732948115613....\n",
      "-----------------------------------\n",
      "Training loss: 0.5135513430045775....\n",
      "Validation loss: 0.5315009557865785....\n",
      "-----------------------------------\n",
      "Training loss: 0.5131244913810238....\n",
      "Validation loss: 0.5311294095801157....\n",
      "-----------------------------------\n",
      "Training loss: 0.5126983479577049....\n",
      "Validation loss: 0.5307585442784266....\n",
      "-----------------------------------\n",
      "Training loss: 0.512273065549022....\n",
      "Validation loss: 0.5303885016089351....\n",
      "-----------------------------------\n",
      "Training loss: 0.5118486611784597....\n",
      "Validation loss: 0.5300192959581066....\n",
      "-----------------------------------\n",
      "Training loss: 0.5114251399472314....\n",
      "Validation loss: 0.5296508605022232....\n",
      "-----------------------------------\n",
      "Training loss: 0.5110025770434681....\n",
      "Validation loss: 0.529283103011066....\n",
      "-----------------------------------\n",
      "Training loss: 0.5105808509683056....\n",
      "Validation loss: 0.5289162481869244....\n",
      "-----------------------------------\n",
      "Training loss: 0.5101599739929639....\n",
      "Validation loss: 0.528550179700122....\n",
      "-----------------------------------\n",
      "Training loss: 0.509739928444412....\n",
      "Validation loss: 0.5281850366496779....\n",
      "-----------------------------------\n",
      "Training loss: 0.5093206853345815....\n",
      "Validation loss: 0.5278208225122075....\n",
      "-----------------------------------\n",
      "Training loss: 0.5089021357870752....\n",
      "Validation loss: 0.5274572367438916....\n",
      "-----------------------------------\n",
      "Training loss: 0.5084841928084669....\n",
      "Validation loss: 0.5270945119000227....\n",
      "-----------------------------------\n",
      "Training loss: 0.50806702293134....\n",
      "Validation loss: 0.5267323874760679....\n",
      "-----------------------------------\n",
      "Training loss: 0.5076505573609087....\n",
      "Validation loss: 0.5263710273920452....\n",
      "-----------------------------------\n",
      "Training loss: 0.5072348815702726....\n",
      "Validation loss: 0.5260105199122075....\n",
      "-----------------------------------\n",
      "Training loss: 0.5068199709248143....\n",
      "Validation loss: 0.5256507123574917....\n",
      "-----------------------------------\n",
      "Training loss: 0.5064059590922054....\n",
      "Validation loss: 0.5252916576770122....\n",
      "-----------------------------------\n",
      "Training loss: 0.5059928466386071....\n",
      "Validation loss: 0.5249329340977175....\n",
      "-----------------------------------\n",
      "Training loss: 0.5055806642303755....\n",
      "Validation loss: 0.5245750033530321....\n",
      "-----------------------------------\n",
      "Training loss: 0.5051693218859409....\n",
      "Validation loss: 0.5242179230667473....\n",
      "-----------------------------------\n",
      "Training loss: 0.5047587879130485....\n",
      "Validation loss: 0.5238616448124432....\n",
      "-----------------------------------\n",
      "Training loss: 0.5043489553939888....\n",
      "Validation loss: 0.5235058445726785....\n",
      "-----------------------------------\n",
      "Training loss: 0.5039398505520621....\n",
      "Validation loss: 0.5231507475605746....\n",
      "-----------------------------------\n",
      "Training loss: 0.5035314949678715....\n",
      "Validation loss: 0.5227964002278834....\n",
      "-----------------------------------\n",
      "Training loss: 0.5031238100792788....\n",
      "Validation loss: 0.5224427370049044....\n",
      "-----------------------------------\n",
      "Training loss: 0.5027168669066338....\n",
      "Validation loss: 0.522089845668176....\n",
      "-----------------------------------\n",
      "Training loss: 0.5023106091026496....\n",
      "Validation loss: 0.5217376454938595....\n",
      "-----------------------------------\n",
      "Training loss: 0.5019051902538807....\n",
      "Validation loss: 0.5213861131824178....\n",
      "-----------------------------------\n",
      "Training loss: 0.5015005863615664....\n",
      "Validation loss: 0.5210352648888142....\n",
      "-----------------------------------\n",
      "Training loss: 0.5010967666214853....\n",
      "Validation loss: 0.5206851560600446....\n",
      "-----------------------------------\n",
      "Training loss: 0.5006937504998916....\n",
      "Validation loss: 0.5203357230335296....\n",
      "-----------------------------------\n",
      "Training loss: 0.500291454902306....\n",
      "Validation loss: 0.5199870320234717....\n",
      "-----------------------------------\n",
      "Training loss: 0.4998899570386855....\n",
      "Validation loss: 0.5196391057779984....\n",
      "-----------------------------------\n",
      "Training loss: 0.49948927141183547....\n",
      "Validation loss: 0.5192917361184874....\n",
      "-----------------------------------\n",
      "Training loss: 0.49908929200094604....\n",
      "Validation loss: 0.518945152278027....\n",
      "-----------------------------------\n",
      "Training loss: 0.4986901901418588....\n",
      "Validation loss: 0.5185992171862147....\n",
      "-----------------------------------\n",
      "Training loss: 0.4982918940859902....\n",
      "Validation loss: 0.5182540353618642....\n",
      "-----------------------------------\n",
      "Training loss: 0.4978944225190012....\n",
      "Validation loss: 0.5179095406527906....\n",
      "-----------------------------------\n",
      "Training loss: 0.49749776043886035....\n",
      "Validation loss: 0.5175655890742087....\n",
      "-----------------------------------\n",
      "Training loss: 0.4971019763497929....\n",
      "Validation loss: 0.5172224713816826....\n",
      "-----------------------------------\n",
      "Training loss: 0.4967069029002501....\n",
      "Validation loss: 0.5168798790497845....\n",
      "-----------------------------------\n",
      "Training loss: 0.49631230772527407....\n",
      "Validation loss: 0.516538050176259....\n",
      "-----------------------------------\n",
      "Training loss: 0.4959184539067963....\n",
      "Validation loss: 0.5161968824045756....\n",
      "-----------------------------------\n",
      "Training loss: 0.4955253708882353....\n",
      "Validation loss: 0.5158564367531548....\n",
      "-----------------------------------\n",
      "Training loss: 0.4951330098246689....\n",
      "Validation loss: 0.5155166684907054....\n",
      "-----------------------------------\n",
      "Training loss: 0.49474140381145754....\n",
      "Validation loss: 0.5151776475588253....\n",
      "-----------------------------------\n",
      "Training loss: 0.49435058732193976....\n",
      "Validation loss: 0.5148393858745555....\n",
      "-----------------------------------\n",
      "Training loss: 0.49396051036100896....\n",
      "Validation loss: 0.51450192213199....\n",
      "-----------------------------------\n",
      "Training loss: 0.4935712446424221....\n",
      "Validation loss: 0.5141651409858723....\n",
      "-----------------------------------\n",
      "Training loss: 0.49318271542700365....\n",
      "Validation loss: 0.5138290921878292....\n",
      "-----------------------------------\n",
      "Training loss: 0.49279486294355856....\n",
      "Validation loss: 0.513493707589795....\n",
      "-----------------------------------\n",
      "Training loss: 0.49240781460947924....\n",
      "Validation loss: 0.5131589750952018....\n",
      "-----------------------------------\n",
      "Training loss: 0.4920215362261989....\n",
      "Validation loss: 0.5128248920334895....\n",
      "-----------------------------------\n",
      "Training loss: 0.4916360247106583....\n",
      "Validation loss: 0.5124914420736245....\n",
      "-----------------------------------\n",
      "Training loss: 0.4912512077876309....\n",
      "Validation loss: 0.5121588208373367....\n",
      "-----------------------------------\n",
      "Training loss: 0.4908670298720419....\n",
      "Validation loss: 0.5118269041825958....\n",
      "-----------------------------------\n",
      "Training loss: 0.4904835996850246....\n",
      "Validation loss: 0.511495682505004....\n",
      "-----------------------------------\n",
      "Training loss: 0.4901009315018235....\n",
      "Validation loss: 0.5111649377486429....\n",
      "-----------------------------------\n",
      "Training loss: 0.4897191088255929....\n",
      "Validation loss: 0.5108346740930207....\n",
      "-----------------------------------\n",
      "Training loss: 0.48933792245042007....\n",
      "Validation loss: 0.5105050944606031....\n",
      "-----------------------------------\n",
      "Training loss: 0.48895748320561117....\n",
      "Validation loss: 0.5101761700164086....\n",
      "-----------------------------------\n",
      "Training loss: 0.48857761578857944....\n",
      "Validation loss: 0.5098479000143111....\n",
      "-----------------------------------\n",
      "Training loss: 0.48819839328055314....\n",
      "Validation loss: 0.509520219459647....\n",
      "-----------------------------------\n",
      "Training loss: 0.48781992685738923....\n",
      "Validation loss: 0.5091932248567875....\n",
      "-----------------------------------\n",
      "Training loss: 0.4874422190217762....\n",
      "Validation loss: 0.5088667690713012....\n",
      "-----------------------------------\n",
      "Training loss: 0.48706513784917327....\n",
      "Validation loss: 0.5085405199456484....\n",
      "-----------------------------------\n",
      "Training loss: 0.48668882517430617....\n",
      "Validation loss: 0.5082149735626377....\n",
      "-----------------------------------\n",
      "Training loss: 0.486313296470588....\n",
      "Validation loss: 0.5078900544385782....\n",
      "-----------------------------------\n",
      "Training loss: 0.4859381571656284....\n",
      "Validation loss: 0.5075656184019672....\n",
      "-----------------------------------\n",
      "Training loss: 0.4855635579635238....\n",
      "Validation loss: 0.5072418155460745....\n",
      "-----------------------------------\n",
      "Training loss: 0.48518963958575945....\n",
      "Validation loss: 0.5069186927947823....\n",
      "-----------------------------------\n",
      "Training loss: 0.4848163683905853....\n",
      "Validation loss: 0.5065961828276466....\n",
      "-----------------------------------\n",
      "Training loss: 0.4844438051536012....\n",
      "Validation loss: 0.5062744186448466....\n",
      "-----------------------------------\n",
      "Training loss: 0.484071964963038....\n",
      "Validation loss: 0.5059533330558074....\n",
      "-----------------------------------\n",
      "Training loss: 0.4837008056387024....\n",
      "Validation loss: 0.5056329708594074....\n",
      "-----------------------------------\n",
      "Training loss: 0.4833305003296998....\n",
      "Validation loss: 0.5053131655687714....\n",
      "-----------------------------------\n",
      "Training loss: 0.48296088102212875....\n",
      "Validation loss: 0.5049938674872041....\n",
      "-----------------------------------\n",
      "Training loss: 0.4825918443178838....\n",
      "Validation loss: 0.5046751410131192....\n",
      "-----------------------------------\n",
      "Training loss: 0.4822235388128236....\n",
      "Validation loss: 0.5043571001262069....\n",
      "-----------------------------------\n",
      "Training loss: 0.481855974821759....\n",
      "Validation loss: 0.5040397030719704....\n",
      "-----------------------------------\n",
      "Training loss: 0.48148911029756764....\n",
      "Validation loss: 0.503723046055074....\n",
      "-----------------------------------\n",
      "Training loss: 0.481122950954493....\n",
      "Validation loss: 0.5034069362817839....\n",
      "-----------------------------------\n",
      "Training loss: 0.48075747700330707....\n",
      "Validation loss: 0.5030915197725758....\n",
      "-----------------------------------\n",
      "Training loss: 0.4803926714022164....\n",
      "Validation loss: 0.5027767571704945....\n",
      "-----------------------------------\n",
      "Training loss: 0.4800286208659784....\n",
      "Validation loss: 0.5024626569086065....\n",
      "-----------------------------------\n",
      "Training loss: 0.4796652340952323....\n",
      "Validation loss: 0.5021490413737526....\n",
      "-----------------------------------\n",
      "Training loss: 0.4793024988774387....\n",
      "Validation loss: 0.5018360136937888....\n",
      "-----------------------------------\n",
      "Training loss: 0.4789404259261089....\n",
      "Validation loss: 0.5015235510930075....\n",
      "-----------------------------------\n",
      "Training loss: 0.4785789303831563....\n",
      "Validation loss: 0.5012118127812755....\n",
      "-----------------------------------\n",
      "Training loss: 0.4782179881412472....\n",
      "Validation loss: 0.5009006460003133....\n",
      "-----------------------------------\n",
      "Training loss: 0.47785774559619937....\n",
      "Validation loss: 0.500590172749467....\n",
      "-----------------------------------\n",
      "Training loss: 0.47749806929616084....\n",
      "Validation loss: 0.5002801780153471....\n",
      "-----------------------------------\n",
      "Training loss: 0.4771389769630093....\n",
      "Validation loss: 0.49997084172872575....\n",
      "-----------------------------------\n",
      "Training loss: 0.4767804854755106....\n",
      "Validation loss: 0.4996620701949882....\n",
      "-----------------------------------\n",
      "Training loss: 0.4764226513318612....\n",
      "Validation loss: 0.4993539861398829....\n",
      "-----------------------------------\n",
      "Training loss: 0.4760655209602605....\n",
      "Validation loss: 0.4990466187346588....\n",
      "-----------------------------------\n",
      "Training loss: 0.47570910284076934....\n",
      "Validation loss: 0.4987397814945628....\n",
      "-----------------------------------\n",
      "Training loss: 0.47535331810813036....\n",
      "Validation loss: 0.4984337519771092....\n",
      "-----------------------------------\n",
      "Training loss: 0.4749980711619216....\n",
      "Validation loss: 0.49812818651555657....\n",
      "-----------------------------------\n",
      "Training loss: 0.4746434350743108....\n",
      "Validation loss: 0.4978232526246717....\n",
      "-----------------------------------\n",
      "Training loss: 0.4742894384662637....\n",
      "Validation loss: 0.49751893916209283....\n",
      "-----------------------------------\n",
      "Training loss: 0.47393601701559124....\n",
      "Validation loss: 0.497215164164134....\n",
      "-----------------------------------\n",
      "Training loss: 0.47358324738243157....\n",
      "Validation loss: 0.4969121732979674....\n",
      "-----------------------------------\n",
      "Training loss: 0.4732312123422721....\n",
      "Validation loss: 0.49660966934216527....\n",
      "-----------------------------------\n",
      "Training loss: 0.4728799167693546....\n",
      "Validation loss: 0.4963077576653018....\n",
      "-----------------------------------\n",
      "Training loss: 0.47252908124603776....\n",
      "Validation loss: 0.4960062370184007....\n",
      "-----------------------------------\n",
      "Training loss: 0.47217884100001845....\n",
      "Validation loss: 0.49570538336266506....\n",
      "-----------------------------------\n",
      "Training loss: 0.47182925426708294....\n",
      "Validation loss: 0.4954050445680511....\n",
      "-----------------------------------\n",
      "Training loss: 0.4714802247474486....\n",
      "Validation loss: 0.49510537108380004....\n",
      "-----------------------------------\n",
      "Training loss: 0.47113183520348934....\n",
      "Validation loss: 0.4948062632541788....\n",
      "-----------------------------------\n",
      "Training loss: 0.47078403180067124....\n",
      "Validation loss: 0.4945078665654496....\n",
      "-----------------------------------\n",
      "Training loss: 0.47043687630235637....\n",
      "Validation loss: 0.49420997091001306....\n",
      "-----------------------------------\n",
      "Training loss: 0.4700903696186011....\n",
      "Validation loss: 0.4939126525137473....\n",
      "-----------------------------------\n",
      "Training loss: 0.46974440438624165....\n",
      "Validation loss: 0.49361584023369437....\n",
      "-----------------------------------\n",
      "Training loss: 0.46939882556457924....\n",
      "Validation loss: 0.4933193013128243....\n",
      "-----------------------------------\n",
      "Training loss: 0.4690537212206528....\n",
      "Validation loss: 0.49302328008431734....\n",
      "-----------------------------------\n",
      "Training loss: 0.4687091569114717....\n",
      "Validation loss: 0.49272784564253735....\n",
      "-----------------------------------\n",
      "Training loss: 0.468365158527271....\n",
      "Validation loss: 0.4924330094003551....\n",
      "-----------------------------------\n",
      "Training loss: 0.46802175627105275....\n",
      "Validation loss: 0.4921386714767119....\n",
      "-----------------------------------\n",
      "Training loss: 0.4676788912554929....\n",
      "Validation loss: 0.4918448183848884....\n",
      "-----------------------------------\n",
      "Training loss: 0.46733667522921485....\n",
      "Validation loss: 0.4915517616074419....\n",
      "-----------------------------------\n",
      "Training loss: 0.46699517747614777....\n",
      "Validation loss: 0.49125931448306076....\n",
      "-----------------------------------\n",
      "Training loss: 0.46665440565039207....\n",
      "Validation loss: 0.490967341810833....\n",
      "-----------------------------------\n",
      "Training loss: 0.46631428525121044....\n",
      "Validation loss: 0.4906758218506686....\n",
      "-----------------------------------\n",
      "Training loss: 0.46597478865165615....\n",
      "Validation loss: 0.4903848671723869....\n",
      "-----------------------------------\n",
      "Training loss: 0.46563592289386035....\n",
      "Validation loss: 0.49009450578288183....\n",
      "-----------------------------------\n",
      "Training loss: 0.4652975920175688....\n",
      "Validation loss: 0.48980469302128055....\n",
      "-----------------------------------\n",
      "Training loss: 0.46495986859733873....\n",
      "Validation loss: 0.48951526423082664....\n",
      "-----------------------------------\n",
      "Training loss: 0.4646227551328856....\n",
      "Validation loss: 0.4892263565885599....\n",
      "-----------------------------------\n",
      "Training loss: 0.4642862469858517....\n",
      "Validation loss: 0.4889380121981876....\n",
      "-----------------------------------\n",
      "Training loss: 0.4639504158866969....\n",
      "Validation loss: 0.4886501581951194....\n",
      "-----------------------------------\n",
      "Training loss: 0.46361524691901....\n",
      "Validation loss: 0.48836284523356766....\n",
      "-----------------------------------\n",
      "Training loss: 0.4632807471399195....\n",
      "Validation loss: 0.4880760560146612....\n",
      "-----------------------------------\n",
      "Training loss: 0.4629469238125708....\n",
      "Validation loss: 0.48778973421776645....\n",
      "-----------------------------------\n",
      "Training loss: 0.4626136689992431....\n",
      "Validation loss: 0.48750400719752807....\n",
      "-----------------------------------\n",
      "Training loss: 0.4622808225112936....\n",
      "Validation loss: 0.4872187489929991....\n",
      "-----------------------------------\n",
      "Training loss: 0.46194849248168485....\n",
      "Validation loss: 0.4869340591669246....\n",
      "-----------------------------------\n",
      "Training loss: 0.4616167562814591....\n",
      "Validation loss: 0.48665001197271984....\n",
      "-----------------------------------\n",
      "Training loss: 0.4612855938826521....\n",
      "Validation loss: 0.48636645349625485....\n",
      "-----------------------------------\n",
      "Training loss: 0.4609549470904889....\n",
      "Validation loss: 0.4860833436449431....\n",
      "-----------------------------------\n",
      "Training loss: 0.46062485256438285....\n",
      "Validation loss: 0.48580073205527613....\n",
      "-----------------------------------\n",
      "Training loss: 0.46029528772700284....\n",
      "Validation loss: 0.4855186598465206....\n",
      "-----------------------------------\n",
      "Training loss: 0.45996617418117897....\n",
      "Validation loss: 0.4852370347415516....\n",
      "-----------------------------------\n",
      "Training loss: 0.45963765179230437....\n",
      "Validation loss: 0.48495610526764665....\n",
      "-----------------------------------\n",
      "Training loss: 0.45930978551259194....\n",
      "Validation loss: 0.4846756755560783....\n",
      "-----------------------------------\n",
      "Training loss: 0.45898252992377303....\n",
      "Validation loss: 0.48439581945328847....\n",
      "-----------------------------------\n",
      "Training loss: 0.45865584427387335....\n",
      "Validation loss: 0.4841163731195211....\n",
      "-----------------------------------\n",
      "Training loss: 0.4583297240805171....\n",
      "Validation loss: 0.48383751109320033....\n",
      "-----------------------------------\n",
      "Training loss: 0.4580042082680181....\n",
      "Validation loss: 0.4835591468977469....\n",
      "-----------------------------------\n",
      "Training loss: 0.4576792508646976....\n",
      "Validation loss: 0.48328127295290907....\n",
      "-----------------------------------\n",
      "Training loss: 0.4573548403789342....\n",
      "Validation loss: 0.48300396830859393....\n",
      "-----------------------------------\n",
      "Training loss: 0.4570309433865227....\n",
      "Validation loss: 0.4827271684227962....\n",
      "-----------------------------------\n",
      "Training loss: 0.45670764511825357....\n",
      "Validation loss: 0.4824510308277818....\n",
      "-----------------------------------\n",
      "Training loss: 0.4563849572853385....\n",
      "Validation loss: 0.48217530421756494....\n",
      "-----------------------------------\n",
      "Training loss: 0.45606291123734033....\n",
      "Validation loss: 0.4819001582215285....\n",
      "-----------------------------------\n",
      "Training loss: 0.45574141585264366....\n",
      "Validation loss: 0.48162538199871197....\n",
      "-----------------------------------\n",
      "Training loss: 0.4554204014317214....\n",
      "Validation loss: 0.48135114990691763....\n",
      "-----------------------------------\n",
      "Training loss: 0.45509990090736047....\n",
      "Validation loss: 0.48107737733609807....\n",
      "-----------------------------------\n",
      "Training loss: 0.4547799070092798....\n",
      "Validation loss: 0.4808041392363439....\n",
      "-----------------------------------\n",
      "Training loss: 0.4544604773175443....\n",
      "Validation loss: 0.48053127502090365....\n",
      "-----------------------------------\n",
      "Training loss: 0.45414148005991867....\n",
      "Validation loss: 0.4802589243216162....\n",
      "-----------------------------------\n",
      "Training loss: 0.45382301870322383....\n",
      "Validation loss: 0.4799870705528482....\n",
      "-----------------------------------\n",
      "Training loss: 0.4535050919474108....\n",
      "Validation loss: 0.4797157855890555....\n",
      "-----------------------------------\n",
      "Training loss: 0.45318767722862935....\n",
      "Validation loss: 0.47944504246679637....\n",
      "-----------------------------------\n",
      "Training loss: 0.45287082376525856....\n",
      "Validation loss: 0.47917487470312764....\n",
      "-----------------------------------\n",
      "Training loss: 0.4525545609005054....\n",
      "Validation loss: 0.47890518315439295....\n",
      "-----------------------------------\n",
      "Training loss: 0.4522388510862279....\n",
      "Validation loss: 0.47863582307072566....\n",
      "-----------------------------------\n",
      "Training loss: 0.4519236823119327....\n",
      "Validation loss: 0.4783669117442528....\n",
      "-----------------------------------\n",
      "Training loss: 0.4516090417091215....\n",
      "Validation loss: 0.4780985007787504....\n",
      "-----------------------------------\n",
      "Training loss: 0.4512949870651198....\n",
      "Validation loss: 0.4778305996287834....\n",
      "-----------------------------------\n",
      "Training loss: 0.4509815117345716....\n",
      "Validation loss: 0.47756316980332125....\n",
      "-----------------------------------\n",
      "Training loss: 0.4506685103679375....\n",
      "Validation loss: 0.47729620828611086....\n",
      "-----------------------------------\n",
      "Training loss: 0.45035604253662986....\n",
      "Validation loss: 0.47702980172735016....\n",
      "-----------------------------------\n",
      "Training loss: 0.4500441219369279....\n",
      "Validation loss: 0.47676385985542374....\n",
      "-----------------------------------\n",
      "Training loss: 0.4497327122795073....\n",
      "Validation loss: 0.47649840625596007....\n",
      "-----------------------------------\n",
      "Training loss: 0.449421867876792....\n",
      "Validation loss: 0.4762334604793622....\n",
      "-----------------------------------\n",
      "Training loss: 0.44911157371437044....\n",
      "Validation loss: 0.47596893486321984....\n",
      "-----------------------------------\n",
      "Training loss: 0.44880178065553084....\n",
      "Validation loss: 0.47570497654443955....\n",
      "-----------------------------------\n",
      "Training loss: 0.4484925084263954....\n",
      "Validation loss: 0.47544158370334616....\n",
      "-----------------------------------\n",
      "Training loss: 0.44818379243511997....\n",
      "Validation loss: 0.4751787121806832....\n",
      "-----------------------------------\n",
      "Training loss: 0.44787558362729096....\n",
      "Validation loss: 0.4749164209347952....\n",
      "-----------------------------------\n",
      "Training loss: 0.4475678587815465....\n",
      "Validation loss: 0.4746545548893965....\n",
      "-----------------------------------\n",
      "Training loss: 0.44726059794172723....\n",
      "Validation loss: 0.47439317538360376....\n",
      "-----------------------------------\n",
      "Training loss: 0.4469538904736627....\n",
      "Validation loss: 0.47413218003059265....\n",
      "-----------------------------------\n",
      "Training loss: 0.4466477665970535....\n",
      "Validation loss: 0.473871651585127....\n",
      "-----------------------------------\n",
      "Training loss: 0.4463422090782993....\n",
      "Validation loss: 0.473611506350645....\n",
      "-----------------------------------\n",
      "Training loss: 0.4460371391027051....\n",
      "Validation loss: 0.47335182264508674....\n",
      "-----------------------------------\n",
      "Training loss: 0.44573265487540314....\n",
      "Validation loss: 0.47309262854812295....\n",
      "-----------------------------------\n",
      "Training loss: 0.4454287589710472....\n",
      "Validation loss: 0.4728338898462258....\n",
      "-----------------------------------\n",
      "Training loss: 0.4451254151582179....\n",
      "Validation loss: 0.4725757093359631....\n",
      "-----------------------------------\n",
      "Training loss: 0.44482256236837187....\n",
      "Validation loss: 0.47231806677728855....\n",
      "-----------------------------------\n",
      "Training loss: 0.4445202262977512....\n",
      "Validation loss: 0.4720608309669342....\n",
      "-----------------------------------\n",
      "Training loss: 0.44421835661419723....\n",
      "Validation loss: 0.47180396786098583....\n",
      "-----------------------------------\n",
      "Training loss: 0.44391696074304077....\n",
      "Validation loss: 0.47154758055893053....\n",
      "-----------------------------------\n",
      "Training loss: 0.4436161289571187....\n",
      "Validation loss: 0.471291681527179....\n",
      "-----------------------------------\n",
      "Training loss: 0.44331581052169694....\n",
      "Validation loss: 0.471036302299148....\n",
      "-----------------------------------\n",
      "Training loss: 0.4430160015218297....\n",
      "Validation loss: 0.47078127497714767....\n",
      "-----------------------------------\n",
      "Training loss: 0.442716685294538....\n",
      "Validation loss: 0.470526825747754....\n",
      "-----------------------------------\n",
      "Training loss: 0.4424179236981348....\n",
      "Validation loss: 0.47027291621725215....\n",
      "-----------------------------------\n",
      "Training loss: 0.4421197176829784....\n",
      "Validation loss: 0.4700195022344255....\n",
      "-----------------------------------\n",
      "Training loss: 0.4418220119428157....\n",
      "Validation loss: 0.4697665678847993....\n",
      "-----------------------------------\n",
      "Training loss: 0.4415247967802535....\n",
      "Validation loss: 0.4695140825159961....\n",
      "-----------------------------------\n",
      "Training loss: 0.4412280874964315....\n",
      "Validation loss: 0.4692621492933636....\n",
      "-----------------------------------\n",
      "Training loss: 0.44093187274819023....\n",
      "Validation loss: 0.46901061756803303....\n",
      "-----------------------------------\n",
      "Training loss: 0.4406361643123491....\n",
      "Validation loss: 0.4687595556663482....\n",
      "-----------------------------------\n",
      "Training loss: 0.44034101455802804....\n",
      "Validation loss: 0.4685090278801699....\n",
      "-----------------------------------\n",
      "Training loss: 0.4400463844464882....\n",
      "Validation loss: 0.46825901011683585....\n",
      "-----------------------------------\n",
      "Training loss: 0.43975229702827473....\n",
      "Validation loss: 0.4680093773344551....\n",
      "-----------------------------------\n",
      "Training loss: 0.4394587376665588....\n",
      "Validation loss: 0.4677601784995449....\n",
      "-----------------------------------\n",
      "Training loss: 0.4391656919541915....\n",
      "Validation loss: 0.46751142878400737....\n",
      "-----------------------------------\n",
      "Training loss: 0.43887308889050186....\n",
      "Validation loss: 0.4672630376999431....\n",
      "-----------------------------------\n",
      "Training loss: 0.43858089941044875....\n",
      "Validation loss: 0.4670150565026372....\n",
      "-----------------------------------\n",
      "Training loss: 0.43828903855058293....\n",
      "Validation loss: 0.4667675368545882....\n",
      "-----------------------------------\n",
      "Training loss: 0.4379976202453421....\n",
      "Validation loss: 0.46652057759462967....\n",
      "-----------------------------------\n",
      "Training loss: 0.43770669098064124....\n",
      "Validation loss: 0.4662740357357764....\n",
      "-----------------------------------\n",
      "Training loss: 0.4374162433118892....\n",
      "Validation loss: 0.46602794264911634....\n",
      "-----------------------------------\n",
      "Training loss: 0.4371262219000175....\n",
      "Validation loss: 0.46578238869429567....\n",
      "-----------------------------------\n",
      "Training loss: 0.4368366402822487....\n",
      "Validation loss: 0.46553728012756856....\n",
      "-----------------------------------\n",
      "Training loss: 0.4365475666711091....\n",
      "Validation loss: 0.46529258419121167....\n",
      "-----------------------------------\n",
      "Training loss: 0.43625897334999947....\n",
      "Validation loss: 0.46504842756742043....\n",
      "-----------------------------------\n",
      "Training loss: 0.43597089685136853....\n",
      "Validation loss: 0.4648046876385949....\n",
      "-----------------------------------\n",
      "Training loss: 0.43568331900248414....\n",
      "Validation loss: 0.46456145700248125....\n",
      "-----------------------------------\n",
      "Training loss: 0.4353962247797359....\n",
      "Validation loss: 0.46431861548107756....\n",
      "-----------------------------------\n",
      "Training loss: 0.43510963594116786....\n",
      "Validation loss: 0.46407622485058253....\n",
      "-----------------------------------\n",
      "Training loss: 0.4348235429209442....\n",
      "Validation loss: 0.46383422448629225....\n",
      "-----------------------------------\n",
      "Training loss: 0.4345379306805419....\n",
      "Validation loss: 0.4635927562053418....\n",
      "-----------------------------------\n",
      "Training loss: 0.4342527710180543....\n",
      "Validation loss: 0.4633517392297109....\n",
      "-----------------------------------\n",
      "Training loss: 0.4339680486264097....\n",
      "Validation loss: 0.4631110679687579....\n",
      "-----------------------------------\n",
      "Training loss: 0.43368374808287347....\n",
      "Validation loss: 0.4628708307918255....\n",
      "-----------------------------------\n",
      "Training loss: 0.4333998334891255....\n",
      "Validation loss: 0.462630997779708....\n",
      "-----------------------------------\n",
      "Training loss: 0.4331163684594925....\n",
      "Validation loss: 0.4623914946638487....\n",
      "-----------------------------------\n",
      "Training loss: 0.43283332941516894....\n",
      "Validation loss: 0.4621525431461877....\n",
      "-----------------------------------\n",
      "Training loss: 0.432550851449349....\n",
      "Validation loss: 0.4619139859193894....\n",
      "-----------------------------------\n",
      "Training loss: 0.43226876843804196....\n",
      "Validation loss: 0.4616757658348416....\n",
      "-----------------------------------\n",
      "Training loss: 0.4319871468952867....\n",
      "Validation loss: 0.4614379772580777....\n",
      "-----------------------------------\n",
      "Training loss: 0.4317059378705548....\n",
      "Validation loss: 0.46120080110372796....\n",
      "-----------------------------------\n",
      "Training loss: 0.43142511760080093....\n",
      "Validation loss: 0.4609640456366971....\n",
      "-----------------------------------\n",
      "Training loss: 0.43114461966162293....\n",
      "Validation loss: 0.4607277373227763....\n",
      "-----------------------------------\n",
      "Training loss: 0.43086460750442807....\n",
      "Validation loss: 0.4604918131921844....\n",
      "-----------------------------------\n",
      "Training loss: 0.4305850519075184....\n",
      "Validation loss: 0.4602562010123516....\n",
      "-----------------------------------\n",
      "Training loss: 0.4303059361388003....\n",
      "Validation loss: 0.4600209642409129....\n",
      "-----------------------------------\n",
      "Training loss: 0.43002724319452473....\n",
      "Validation loss: 0.4597861111995754....\n",
      "-----------------------------------\n",
      "Training loss: 0.4297490150155903....\n",
      "Validation loss: 0.4595515838342318....\n",
      "-----------------------------------\n",
      "Training loss: 0.4294712094530147....\n",
      "Validation loss: 0.45931743918894247....\n",
      "-----------------------------------\n",
      "Training loss: 0.4291938701982835....\n",
      "Validation loss: 0.4590837239537548....\n",
      "-----------------------------------\n",
      "Training loss: 0.42891695678763253....\n",
      "Validation loss: 0.4588506122785612....\n",
      "-----------------------------------\n",
      "Training loss: 0.4286404480594302....\n",
      "Validation loss: 0.458617892257342....\n",
      "-----------------------------------\n",
      "Training loss: 0.4283644002197971....\n",
      "Validation loss: 0.4583855315094476....\n",
      "-----------------------------------\n",
      "Training loss: 0.4280887688713409....\n",
      "Validation loss: 0.45815357959082714....\n",
      "-----------------------------------\n",
      "Training loss: 0.4278135685751653....\n",
      "Validation loss: 0.45792200651718173....\n",
      "-----------------------------------\n",
      "Training loss: 0.42753885349078913....\n",
      "Validation loss: 0.45769078061621615....\n",
      "-----------------------------------\n",
      "Training loss: 0.4272645915658077....\n",
      "Validation loss: 0.4574599972735....\n",
      "-----------------------------------\n",
      "Training loss: 0.4269907677712204....\n",
      "Validation loss: 0.45722960906517757....\n",
      "-----------------------------------\n",
      "Training loss: 0.42671738846821433....\n",
      "Validation loss: 0.45699963788836534....\n",
      "-----------------------------------\n",
      "Training loss: 0.42644448155209547....\n",
      "Validation loss: 0.4567700381567906....\n",
      "-----------------------------------\n",
      "Training loss: 0.42617203087810607....\n",
      "Validation loss: 0.4565408985331893....\n",
      "-----------------------------------\n",
      "Training loss: 0.4259000077786836....\n",
      "Validation loss: 0.4563121868318252....\n",
      "-----------------------------------\n",
      "Training loss: 0.42562839091732296....\n",
      "Validation loss: 0.45608387252640764....\n",
      "-----------------------------------\n",
      "Training loss: 0.42535724273529457....\n",
      "Validation loss: 0.45585592533938....\n",
      "-----------------------------------\n",
      "Training loss: 0.42508654008306723....\n",
      "Validation loss: 0.45562830284023076....\n",
      "-----------------------------------\n",
      "Training loss: 0.4248162386223481....\n",
      "Validation loss: 0.4554011090097286....\n",
      "-----------------------------------\n",
      "Training loss: 0.4245463924641612....\n",
      "Validation loss: 0.45517416148220596....\n",
      "-----------------------------------\n",
      "Training loss: 0.4242769914437725....\n",
      "Validation loss: 0.45494765039569346....\n",
      "-----------------------------------\n",
      "Training loss: 0.4240080895192685....\n",
      "Validation loss: 0.45472153478639393....\n",
      "-----------------------------------\n",
      "Training loss: 0.42373961988509024....\n",
      "Validation loss: 0.4544957822263417....\n",
      "-----------------------------------\n",
      "Training loss: 0.4234715070777015....\n",
      "Validation loss: 0.45427041153752407....\n",
      "-----------------------------------\n",
      "Training loss: 0.42320369162590726....\n",
      "Validation loss: 0.45404540461396564....\n",
      "-----------------------------------\n",
      "Training loss: 0.422936265072733....\n",
      "Validation loss: 0.4538207141779385....\n",
      "-----------------------------------\n",
      "Training loss: 0.4226691575952336....\n",
      "Validation loss: 0.4535963675201821....\n",
      "-----------------------------------\n",
      "Training loss: 0.42240244161745....\n",
      "Validation loss: 0.4533725245054141....\n",
      "-----------------------------------\n",
      "Training loss: 0.4221361583487223....\n",
      "Validation loss: 0.4531489588174192....\n",
      "-----------------------------------\n",
      "Training loss: 0.42187019938547315....\n",
      "Validation loss: 0.4529257323960476....\n",
      "-----------------------------------\n",
      "Training loss: 0.4216046450228885....\n",
      "Validation loss: 0.4527028962527931....\n",
      "-----------------------------------\n",
      "Training loss: 0.42133948816512495....\n",
      "Validation loss: 0.4524804688729538....\n",
      "-----------------------------------\n",
      "Training loss: 0.42107472764394116....\n",
      "Validation loss: 0.45225841716525256....\n",
      "-----------------------------------\n",
      "Training loss: 0.4208103448333364....\n",
      "Validation loss: 0.45203676532526926....\n",
      "-----------------------------------\n",
      "Training loss: 0.4205463882784074....\n",
      "Validation loss: 0.45181548405669836....\n",
      "-----------------------------------\n",
      "Training loss: 0.42028288323416885....\n",
      "Validation loss: 0.45159457011885945....\n",
      "-----------------------------------\n",
      "Training loss: 0.4200198101875782....\n",
      "Validation loss: 0.45137408572362475....\n",
      "-----------------------------------\n",
      "Training loss: 0.4197571789879014....\n",
      "Validation loss: 0.451153986719049....\n",
      "-----------------------------------\n",
      "Training loss: 0.4194949740107034....\n",
      "Validation loss: 0.45093426487554183....\n",
      "-----------------------------------\n",
      "Training loss: 0.41923321578448375....\n",
      "Validation loss: 0.4507150054302996....\n",
      "-----------------------------------\n",
      "Training loss: 0.4189718743386974....\n",
      "Validation loss: 0.4504963147456032....\n",
      "-----------------------------------\n",
      "Training loss: 0.4187108781678687....\n",
      "Validation loss: 0.4502780360109703....\n",
      "-----------------------------------\n",
      "Training loss: 0.41845030842813974....\n",
      "Validation loss: 0.45006015402940486....\n",
      "-----------------------------------\n",
      "Training loss: 0.4181900577365512....\n",
      "Validation loss: 0.44984264124418955....\n",
      "-----------------------------------\n",
      "Training loss: 0.41793023103571614....\n",
      "Validation loss: 0.44962549878491215....\n",
      "-----------------------------------\n",
      "Training loss: 0.4176708837622256....\n",
      "Validation loss: 0.44940873151145494....\n",
      "-----------------------------------\n",
      "Training loss: 0.4174119638477018....\n",
      "Validation loss: 0.44919241618867856....\n",
      "-----------------------------------\n",
      "Training loss: 0.4171534582773764....\n",
      "Validation loss: 0.4489764408271721....\n",
      "-----------------------------------\n",
      "Training loss: 0.4168954199591481....\n",
      "Validation loss: 0.44876064841413515....\n",
      "-----------------------------------\n",
      "Training loss: 0.4166377252912242....\n",
      "Validation loss: 0.44854525678451046....\n",
      "-----------------------------------\n",
      "Training loss: 0.41638043363793026....\n",
      "Validation loss: 0.4483302445047842....\n",
      "-----------------------------------\n",
      "Training loss: 0.41612354062450585....\n",
      "Validation loss: 0.4481155578193115....\n",
      "-----------------------------------\n",
      "Training loss: 0.4158669636573432....\n",
      "Validation loss: 0.44790122509792585....\n",
      "-----------------------------------\n",
      "Training loss: 0.41561077710012384....\n",
      "Validation loss: 0.4476872121989146....\n",
      "-----------------------------------\n",
      "Training loss: 0.4153549644691159....\n",
      "Validation loss: 0.4474736001678481....\n",
      "-----------------------------------\n",
      "Training loss: 0.41509954702829654....\n",
      "Validation loss: 0.44726013710387236....\n",
      "-----------------------------------\n",
      "Training loss: 0.41484444564758355....\n",
      "Validation loss: 0.4470470997599543....\n",
      "-----------------------------------\n",
      "Training loss: 0.4145897214111782....\n",
      "Validation loss: 0.44683432477194845....\n",
      "-----------------------------------\n",
      "Training loss: 0.4143353688347699....\n",
      "Validation loss: 0.44662192597884304....\n",
      "-----------------------------------\n",
      "Training loss: 0.41408139567846136....\n",
      "Validation loss: 0.4464098126421902....\n",
      "-----------------------------------\n",
      "Training loss: 0.4138277936963146....\n",
      "Validation loss: 0.446198136487343....\n",
      "-----------------------------------\n",
      "Training loss: 0.41357460517000694....\n",
      "Validation loss: 0.4459866803492989....\n",
      "-----------------------------------\n",
      "Training loss: 0.41332178752676646....\n",
      "Validation loss: 0.44577572627756534....\n",
      "-----------------------------------\n",
      "Training loss: 0.41306935439772113....\n",
      "Validation loss: 0.445564988151006....\n",
      "-----------------------------------\n",
      "Training loss: 0.41281731372140135....\n",
      "Validation loss: 0.44535470747363537....\n",
      "-----------------------------------\n",
      "Training loss: 0.41256564249769173....\n",
      "Validation loss: 0.445144753198991....\n",
      "-----------------------------------\n",
      "Training loss: 0.4123143544342104....\n",
      "Validation loss: 0.4449351913885953....\n",
      "-----------------------------------\n",
      "Training loss: 0.41206344642404685....\n",
      "Validation loss: 0.44472593046540715....\n",
      "-----------------------------------\n",
      "Training loss: 0.41181284954713115....\n",
      "Validation loss: 0.44451699045540544....\n",
      "-----------------------------------\n",
      "Training loss: 0.41156256603677416....\n",
      "Validation loss: 0.4443083576292733....\n",
      "-----------------------------------\n",
      "Training loss: 0.4113125781547071....\n",
      "Validation loss: 0.44410014636236816....\n",
      "-----------------------------------\n",
      "Training loss: 0.4110629417084989....\n",
      "Validation loss: 0.44389229003439473....\n",
      "-----------------------------------\n",
      "Training loss: 0.4108135444064097....\n",
      "Validation loss: 0.4436848244566769....\n",
      "-----------------------------------\n",
      "Training loss: 0.41056453020257294....\n",
      "Validation loss: 0.44347763659080874....\n",
      "-----------------------------------\n",
      "Training loss: 0.41031590379307936....\n",
      "Validation loss: 0.4432708038190851....\n",
      "-----------------------------------\n",
      "Training loss: 0.41006767214216155....\n",
      "Validation loss: 0.4430643066382716....\n",
      "-----------------------------------\n",
      "Training loss: 0.4098197912468482....\n",
      "Validation loss: 0.4428580182218689....\n",
      "-----------------------------------\n",
      "Training loss: 0.40957228303391646....\n",
      "Validation loss: 0.4426520538147406....\n",
      "-----------------------------------\n",
      "Training loss: 0.4093251418663876....\n",
      "Validation loss: 0.44244636253283576....\n",
      "-----------------------------------\n",
      "Training loss: 0.4090783444969924....\n",
      "Validation loss: 0.4422410252487183....\n",
      "-----------------------------------\n",
      "Training loss: 0.4088318678961601....\n",
      "Validation loss: 0.4420359553973382....\n",
      "-----------------------------------\n",
      "Training loss: 0.40858575170876865....\n",
      "Validation loss: 0.44183129848411995....\n",
      "-----------------------------------\n",
      "Training loss: 0.4083401042261891....\n",
      "Validation loss: 0.44162702844308965....\n",
      "-----------------------------------\n",
      "Training loss: 0.40809488779613656....\n",
      "Validation loss: 0.44142321109617083....\n",
      "-----------------------------------\n",
      "Training loss: 0.4078500325182365....\n",
      "Validation loss: 0.4412195272296794....\n",
      "-----------------------------------\n",
      "Training loss: 0.40760553020500345....\n",
      "Validation loss: 0.44101626660591475....\n",
      "-----------------------------------\n",
      "Training loss: 0.4073614131970088....\n",
      "Validation loss: 0.44081339398807207....\n",
      "-----------------------------------\n",
      "Training loss: 0.4071176690019616....\n",
      "Validation loss: 0.44061076533591875....\n",
      "-----------------------------------\n",
      "Training loss: 0.4068742643017484....\n",
      "Validation loss: 0.44040833056627243....\n",
      "-----------------------------------\n",
      "Training loss: 0.40663118819107924....\n",
      "Validation loss: 0.44020627553555997....\n",
      "-----------------------------------\n",
      "Training loss: 0.406388460167549....\n",
      "Validation loss: 0.44000456973168856....\n",
      "-----------------------------------\n",
      "Training loss: 0.4061461008465984....\n",
      "Validation loss: 0.4398032023671353....\n",
      "-----------------------------------\n",
      "Training loss: 0.4059041227548381....\n",
      "Validation loss: 0.4396021439905212....\n",
      "-----------------------------------\n",
      "Training loss: 0.4056625223793905....\n",
      "Validation loss: 0.4394014128354832....\n",
      "-----------------------------------\n",
      "Training loss: 0.40542129743283034....\n",
      "Validation loss: 0.43920106694552374....\n",
      "-----------------------------------\n",
      "Training loss: 0.4051804236248012....\n",
      "Validation loss: 0.4390009199793169....\n",
      "-----------------------------------\n",
      "Training loss: 0.4049398756392634....\n",
      "Validation loss: 0.4388011017364876....\n",
      "-----------------------------------\n",
      "Training loss: 0.40469964991174706....\n",
      "Validation loss: 0.43860165357168895....\n",
      "-----------------------------------\n",
      "Training loss: 0.4044598102802497....\n",
      "Validation loss: 0.4384025948366246....\n",
      "-----------------------------------\n",
      "Training loss: 0.40422036799758065....\n",
      "Validation loss: 0.4382038414992782....\n",
      "-----------------------------------\n",
      "Training loss: 0.4039812501645417....\n",
      "Validation loss: 0.4380053265004334....\n",
      "-----------------------------------\n",
      "Training loss: 0.4037424025654267....\n",
      "Validation loss: 0.43780710684732743....\n",
      "-----------------------------------\n",
      "Training loss: 0.40350393356945635....\n",
      "Validation loss: 0.43760920295378847....\n",
      "-----------------------------------\n",
      "Training loss: 0.4032658287054005....\n",
      "Validation loss: 0.4374116328621601....\n",
      "-----------------------------------\n",
      "Training loss: 0.40302812864053117....\n",
      "Validation loss: 0.43721439607658064....\n",
      "-----------------------------------\n",
      "Training loss: 0.4027907445446688....\n",
      "Validation loss: 0.4370173814733418....\n",
      "-----------------------------------\n",
      "Training loss: 0.40255369374195826....\n",
      "Validation loss: 0.4368205501548366....\n",
      "-----------------------------------\n",
      "Training loss: 0.4023169648827226....\n",
      "Validation loss: 0.43662408514494605....\n",
      "-----------------------------------\n",
      "Training loss: 0.4020805714714639....\n",
      "Validation loss: 0.43642794714131855....\n",
      "-----------------------------------\n",
      "Training loss: 0.40184453165288864....\n",
      "Validation loss: 0.4362320426970215....\n",
      "-----------------------------------\n",
      "Training loss: 0.4016088469121673....\n",
      "Validation loss: 0.43603633144354936....\n",
      "-----------------------------------\n",
      "Training loss: 0.4013734682217818....\n",
      "Validation loss: 0.43584096979208875....\n",
      "-----------------------------------\n",
      "Training loss: 0.4011383551421823....\n",
      "Validation loss: 0.43564588459161824....\n",
      "-----------------------------------\n",
      "Training loss: 0.4009035005716839....\n",
      "Validation loss: 0.435451190607991....\n",
      "-----------------------------------\n",
      "Training loss: 0.40066900010192175....\n",
      "Validation loss: 0.435256805289618....\n",
      "-----------------------------------\n",
      "Training loss: 0.4004348154468825....\n",
      "Validation loss: 0.43506268350121746....\n",
      "-----------------------------------\n",
      "Training loss: 0.40020094226281355....\n",
      "Validation loss: 0.4348688862741491....\n",
      "-----------------------------------\n",
      "Training loss: 0.39996740977524675....\n",
      "Validation loss: 0.434675377585333....\n",
      "-----------------------------------\n",
      "Training loss: 0.3997342176953875....\n",
      "Validation loss: 0.4344820566948238....\n",
      "-----------------------------------\n",
      "Training loss: 0.39950132637128244....\n",
      "Validation loss: 0.4342890947266243....\n",
      "-----------------------------------\n",
      "Training loss: 0.3992687718002398....\n",
      "Validation loss: 0.43409644516721874....\n",
      "-----------------------------------\n",
      "Training loss: 0.39903656459173537....\n",
      "Validation loss: 0.43390404005252536....\n",
      "-----------------------------------\n",
      "Training loss: 0.39880468254507523....\n",
      "Validation loss: 0.4337119964697554....\n",
      "-----------------------------------\n",
      "Training loss: 0.398573120624996....\n",
      "Validation loss: 0.433520255383818....\n",
      "-----------------------------------\n",
      "Training loss: 0.3983418891735221....\n",
      "Validation loss: 0.4333288161091639....\n",
      "-----------------------------------\n",
      "Training loss: 0.3981109081084688....\n",
      "Validation loss: 0.4331376169474494....\n",
      "-----------------------------------\n",
      "Training loss: 0.39788027571776446....\n",
      "Validation loss: 0.4329467873925896....\n",
      "-----------------------------------\n",
      "Training loss: 0.39764996501428373....\n",
      "Validation loss: 0.432756252927687....\n",
      "-----------------------------------\n",
      "Training loss: 0.3974199672129601....\n",
      "Validation loss: 0.43256604795461606....\n",
      "-----------------------------------\n",
      "Training loss: 0.39719030137544414....\n",
      "Validation loss: 0.43237609082139705....\n",
      "-----------------------------------\n",
      "Training loss: 0.3969609606110454....\n",
      "Validation loss: 0.43218642778388927....\n",
      "-----------------------------------\n",
      "Training loss: 0.3967319844310776....\n",
      "Validation loss: 0.43199713396345146....\n",
      "-----------------------------------\n",
      "Training loss: 0.39650335616478277....\n",
      "Validation loss: 0.43180808931303205....\n",
      "-----------------------------------\n",
      "Training loss: 0.3962750225985568....\n",
      "Validation loss: 0.4316192467852461....\n",
      "-----------------------------------\n",
      "Training loss: 0.3960469556790172....\n",
      "Validation loss: 0.4314308236987248....\n",
      "-----------------------------------\n",
      "Training loss: 0.3958192416632284....\n",
      "Validation loss: 0.43124272200015334....\n",
      "-----------------------------------\n",
      "Training loss: 0.3955919178168542....\n",
      "Validation loss: 0.43105491960519565....\n",
      "-----------------------------------\n",
      "Training loss: 0.395364920515503....\n",
      "Validation loss: 0.4308673988976393....\n",
      "-----------------------------------\n",
      "Training loss: 0.39513821221369566....\n",
      "Validation loss: 0.4306800497861147....\n",
      "-----------------------------------\n",
      "Training loss: 0.39491173814239744....\n",
      "Validation loss: 0.4304931038018027....\n",
      "-----------------------------------\n",
      "Training loss: 0.39468560766756616....\n",
      "Validation loss: 0.4303064160990256....\n",
      "-----------------------------------\n",
      "Training loss: 0.39445977941526533....\n",
      "Validation loss: 0.4301200162624033....\n",
      "-----------------------------------\n",
      "Training loss: 0.39423424997239476....\n",
      "Validation loss: 0.429933925932639....\n",
      "-----------------------------------\n",
      "Training loss: 0.39400904625024585....\n",
      "Validation loss: 0.42974805362919327....\n",
      "-----------------------------------\n",
      "Training loss: 0.3937841461939905....\n",
      "Validation loss: 0.42956248806735026....\n",
      "-----------------------------------\n",
      "Training loss: 0.3935595361626717....\n",
      "Validation loss: 0.4293772633838528....\n",
      "-----------------------------------\n",
      "Training loss: 0.39333523945905735....\n",
      "Validation loss: 0.42919227208168853....\n",
      "-----------------------------------\n",
      "Training loss: 0.39311125662666935....\n",
      "Validation loss: 0.42900765869214286....\n",
      "-----------------------------------\n",
      "Training loss: 0.39288758244536315....\n",
      "Validation loss: 0.42882338050643326....\n",
      "-----------------------------------\n",
      "Training loss: 0.39266426984635205....\n",
      "Validation loss: 0.4286393015291108....\n",
      "-----------------------------------\n",
      "Training loss: 0.3924413153221505....\n",
      "Validation loss: 0.4284556721564885....\n",
      "-----------------------------------\n",
      "Training loss: 0.3922187252836493....\n",
      "Validation loss: 0.42827229803834893....\n",
      "-----------------------------------\n",
      "Training loss: 0.39199657973477453....\n",
      "Validation loss: 0.4280892245183633....\n",
      "-----------------------------------\n",
      "Training loss: 0.3917747320013289....\n",
      "Validation loss: 0.4279063789130494....\n",
      "-----------------------------------\n",
      "Training loss: 0.3915531540646695....\n",
      "Validation loss: 0.4277238233712225....\n",
      "-----------------------------------\n",
      "Training loss: 0.3913319208013101....\n",
      "Validation loss: 0.42754138024377936....\n",
      "-----------------------------------\n",
      "Training loss: 0.3911109201166154....\n",
      "Validation loss: 0.4273592599686515....\n",
      "-----------------------------------\n",
      "Training loss: 0.3908902235418109....\n",
      "Validation loss: 0.427177406312214....\n",
      "-----------------------------------\n",
      "Training loss: 0.39066987175186196....\n",
      "Validation loss: 0.4269958738969995....\n",
      "-----------------------------------\n",
      "Training loss: 0.39044985928329207....\n",
      "Validation loss: 0.42681452238349016....\n",
      "-----------------------------------\n",
      "Training loss: 0.3902301457360391....\n",
      "Validation loss: 0.42663352035660146....\n",
      "-----------------------------------\n",
      "Training loss: 0.39001073468716496....\n",
      "Validation loss: 0.4264528039691461....\n",
      "-----------------------------------\n",
      "Training loss: 0.3897916391709677....\n",
      "Validation loss: 0.42627241494774504....\n",
      "-----------------------------------\n",
      "Training loss: 0.38957287510177974....\n",
      "Validation loss: 0.42609239799046456....\n",
      "-----------------------------------\n",
      "Training loss: 0.38935445227492094....\n",
      "Validation loss: 0.4259126492073831....\n",
      "-----------------------------------\n",
      "Training loss: 0.389136307816007....\n",
      "Validation loss: 0.4257332443222587....\n",
      "-----------------------------------\n",
      "Training loss: 0.38891842300241763....\n",
      "Validation loss: 0.4255541202444196....\n",
      "-----------------------------------\n",
      "Training loss: 0.38870087424736....\n",
      "Validation loss: 0.4253753486195377....\n",
      "-----------------------------------\n",
      "Training loss: 0.38848369555321277....\n",
      "Validation loss: 0.4251968175812962....\n",
      "-----------------------------------\n",
      "Training loss: 0.3882668322979878....\n",
      "Validation loss: 0.4250185640109497....\n",
      "-----------------------------------\n",
      "Training loss: 0.3880503037923839....\n",
      "Validation loss: 0.42484054929282744....\n",
      "-----------------------------------\n",
      "Training loss: 0.3878340599044237....\n",
      "Validation loss: 0.4246628215097194....\n",
      "-----------------------------------\n",
      "Training loss: 0.3876181266393767....\n",
      "Validation loss: 0.42448534484858386....\n",
      "-----------------------------------\n",
      "Training loss: 0.3874025152600883....\n",
      "Validation loss: 0.42430811295122245....\n",
      "-----------------------------------\n",
      "Training loss: 0.38718720416530694....\n",
      "Validation loss: 0.4241311151681437....\n",
      "-----------------------------------\n",
      "Training loss: 0.3869721744942935....\n",
      "Validation loss: 0.42395445705590773....\n",
      "-----------------------------------\n",
      "Training loss: 0.38675746474671746....\n",
      "Validation loss: 0.4237780947650323....\n",
      "-----------------------------------\n",
      "Training loss: 0.386543094671578....\n",
      "Validation loss: 0.4236019663149839....\n",
      "-----------------------------------\n",
      "Training loss: 0.38632897465570587....\n",
      "Validation loss: 0.4234261959808785....\n",
      "-----------------------------------\n",
      "Training loss: 0.38611512791529085....\n",
      "Validation loss: 0.42325071128809605....\n",
      "-----------------------------------\n",
      "Training loss: 0.3859014726773838....\n",
      "Validation loss: 0.4230754656296668....\n",
      "-----------------------------------\n",
      "Training loss: 0.3856880814696985....\n",
      "Validation loss: 0.42290058896599886....\n",
      "-----------------------------------\n",
      "Training loss: 0.38547499316375744....\n",
      "Validation loss: 0.4227259522593049....\n",
      "-----------------------------------\n",
      "Training loss: 0.38526221012659223....\n",
      "Validation loss: 0.4225515260187427....\n",
      "-----------------------------------\n",
      "Training loss: 0.3850497479940207....\n",
      "Validation loss: 0.4223774670098034....\n",
      "-----------------------------------\n",
      "Training loss: 0.3848375898888019....\n",
      "Validation loss: 0.4222035820561477....\n",
      "-----------------------------------\n",
      "Training loss: 0.38462574869924565....\n",
      "Validation loss: 0.422029862071396....\n",
      "-----------------------------------\n",
      "Training loss: 0.3844141710640398....\n",
      "Validation loss: 0.42185645152878964....\n",
      "-----------------------------------\n",
      "Training loss: 0.41813779644776417....\n",
      "Validation loss: 0.3538089244360851....\n",
      "-----------------------------------\n",
      "Training loss: 0.41789370128781966....\n",
      "Validation loss: 0.35364684862016355....\n",
      "-----------------------------------\n",
      "Training loss: 0.41765092697479345....\n",
      "Validation loss: 0.3534852175892603....\n",
      "-----------------------------------\n",
      "Training loss: 0.4174094708943899....\n",
      "Validation loss: 0.3533239569964247....\n",
      "-----------------------------------\n",
      "Training loss: 0.41716931892040243....\n",
      "Validation loss: 0.353163085416943....\n",
      "-----------------------------------\n",
      "Training loss: 0.4169304257658088....\n",
      "Validation loss: 0.35300263441712326....\n",
      "-----------------------------------\n",
      "Training loss: 0.4166926906069931....\n",
      "Validation loss: 0.35284271365188247....\n",
      "-----------------------------------\n",
      "Training loss: 0.416456095396102....\n",
      "Validation loss: 0.35268306349047984....\n",
      "-----------------------------------\n",
      "Training loss: 0.4162205322311925....\n",
      "Validation loss: 0.3525236739925522....\n",
      "-----------------------------------\n",
      "Training loss: 0.41598583110862897....\n",
      "Validation loss: 0.3523644879685049....\n",
      "-----------------------------------\n",
      "Training loss: 0.4157519644899846....\n",
      "Validation loss: 0.35220551884157913....\n",
      "-----------------------------------\n",
      "Training loss: 0.41551895986090426....\n",
      "Validation loss: 0.3520467767898767....\n",
      "-----------------------------------\n",
      "Training loss: 0.4152868550135886....\n",
      "Validation loss: 0.3518882484530736....\n",
      "-----------------------------------\n",
      "Training loss: 0.41505558380760865....\n",
      "Validation loss: 0.35172980909008106....\n",
      "-----------------------------------\n",
      "Training loss: 0.41482505731278013....\n",
      "Validation loss: 0.35157156635963577....\n",
      "-----------------------------------\n",
      "Training loss: 0.4145953624433026....\n",
      "Validation loss: 0.35141358069839845....\n",
      "-----------------------------------\n",
      "Training loss: 0.414366429715133....\n",
      "Validation loss: 0.35125577157917....\n",
      "-----------------------------------\n",
      "Training loss: 0.4141381763023621....\n",
      "Validation loss: 0.35109809931900243....\n",
      "-----------------------------------\n",
      "Training loss: 0.41391058070347725....\n",
      "Validation loss: 0.35094064850203843....\n",
      "-----------------------------------\n",
      "Training loss: 0.41368365610111824....\n",
      "Validation loss: 0.35078341604082003....\n",
      "-----------------------------------\n",
      "Training loss: 0.4134573586236527....\n",
      "Validation loss: 0.35062638192055623....\n",
      "-----------------------------------\n",
      "Training loss: 0.41323168577379793....\n",
      "Validation loss: 0.35046948361619595....\n",
      "-----------------------------------\n",
      "Training loss: 0.4130066260567188....\n",
      "Validation loss: 0.3503126906400922....\n",
      "-----------------------------------\n",
      "Training loss: 0.4127821667153046....\n",
      "Validation loss: 0.35015591990886247....\n",
      "-----------------------------------\n",
      "Training loss: 0.4125583192885774....\n",
      "Validation loss: 0.3499992352654573....\n",
      "-----------------------------------\n",
      "Training loss: 0.41233501351050506....\n",
      "Validation loss: 0.3498426527943349....\n",
      "-----------------------------------\n",
      "Training loss: 0.41211223042761147....\n",
      "Validation loss: 0.3496862301107505....\n",
      "-----------------------------------\n",
      "Training loss: 0.4118899648413203....\n",
      "Validation loss: 0.3495299024642108....\n",
      "-----------------------------------\n",
      "Training loss: 0.4116681362353052....\n",
      "Validation loss: 0.349373697155909....\n",
      "-----------------------------------\n",
      "Training loss: 0.41144686117649787....\n",
      "Validation loss: 0.34921766155678047....\n",
      "-----------------------------------\n",
      "Training loss: 0.4112261355242881....\n",
      "Validation loss: 0.3490617768546737....\n",
      "-----------------------------------\n",
      "Training loss: 0.41100589636294615....\n",
      "Validation loss: 0.3489060573106797....\n",
      "-----------------------------------\n",
      "Training loss: 0.41078616360779585....\n",
      "Validation loss: 0.3487505121990679....\n",
      "-----------------------------------\n",
      "Training loss: 0.41056692596824645....\n",
      "Validation loss: 0.34859511094481005....\n",
      "-----------------------------------\n",
      "Training loss: 0.4103481505627915....\n",
      "Validation loss: 0.34843987915044616....\n",
      "-----------------------------------\n",
      "Training loss: 0.4101298269544748....\n",
      "Validation loss: 0.348284795221895....\n",
      "-----------------------------------\n",
      "Training loss: 0.40991192568215984....\n",
      "Validation loss: 0.34812989897484453....\n",
      "-----------------------------------\n",
      "Training loss: 0.4096944808523649....\n",
      "Validation loss: 0.3479751703345742....\n",
      "-----------------------------------\n",
      "Training loss: 0.4094774977299552....\n",
      "Validation loss: 0.3478207171875549....\n",
      "-----------------------------------\n",
      "Training loss: 0.4092609808496513....\n",
      "Validation loss: 0.3476665052290214....\n",
      "-----------------------------------\n",
      "Training loss: 0.4090448936129368....\n",
      "Validation loss: 0.3475124861659453....\n",
      "-----------------------------------\n",
      "Training loss: 0.4088292254203457....\n",
      "Validation loss: 0.34735861862241174....\n",
      "-----------------------------------\n",
      "Training loss: 0.40861397727663656....\n",
      "Validation loss: 0.3472049175346593....\n",
      "-----------------------------------\n",
      "Training loss: 0.4083991637019729....\n",
      "Validation loss: 0.34705139490949943....\n",
      "-----------------------------------\n",
      "Training loss: 0.4081847383638429....\n",
      "Validation loss: 0.3468979891510162....\n",
      "-----------------------------------\n",
      "Training loss: 0.407970635129388....\n",
      "Validation loss: 0.3467447384535585....\n",
      "-----------------------------------\n",
      "Training loss: 0.4077568305308044....\n",
      "Validation loss: 0.3465916442012494....\n",
      "-----------------------------------\n",
      "Training loss: 0.40754342144326106....\n",
      "Validation loss: 0.3464386669184018....\n",
      "-----------------------------------\n",
      "Training loss: 0.40733039775768615....\n",
      "Validation loss: 0.34628578774995394....\n",
      "-----------------------------------\n",
      "Training loss: 0.4071177475337263....\n",
      "Validation loss: 0.34613309496908495....\n",
      "-----------------------------------\n",
      "Training loss: 0.40690541886636983....\n",
      "Validation loss: 0.34598048356161715....\n",
      "-----------------------------------\n",
      "Training loss: 0.40669341364686296....\n",
      "Validation loss: 0.3458280425190453....\n",
      "-----------------------------------\n",
      "Training loss: 0.4064818495620832....\n",
      "Validation loss: 0.3456757591747971....\n",
      "-----------------------------------\n",
      "Training loss: 0.4062707878053094....\n",
      "Validation loss: 0.3455234572408779....\n",
      "-----------------------------------\n",
      "Training loss: 0.4060601558195407....\n",
      "Validation loss: 0.34537134354213317....\n",
      "-----------------------------------\n",
      "Training loss: 0.4058499049637021....\n",
      "Validation loss: 0.34521941788381777....\n",
      "-----------------------------------\n",
      "Training loss: 0.405639965002315....\n",
      "Validation loss: 0.34506766972922553....\n",
      "-----------------------------------\n",
      "Training loss: 0.40543037550785144....\n",
      "Validation loss: 0.34491608250697464....\n",
      "-----------------------------------\n",
      "Training loss: 0.4052212100022904....\n",
      "Validation loss: 0.3447646594167107....\n",
      "-----------------------------------\n",
      "Training loss: 0.4050123736429375....\n",
      "Validation loss: 0.344613379112971....\n",
      "-----------------------------------\n",
      "Training loss: 0.40480388801910316....\n",
      "Validation loss: 0.3444622824744711....\n",
      "-----------------------------------\n",
      "Training loss: 0.4045957333380497....\n",
      "Validation loss: 0.34431130712400254....\n",
      "-----------------------------------\n",
      "Training loss: 0.40438785969586855....\n",
      "Validation loss: 0.3441605142970236....\n",
      "-----------------------------------\n",
      "Training loss: 0.40418033924504226....\n",
      "Validation loss: 0.3440099303993915....\n",
      "-----------------------------------\n",
      "Training loss: 0.4039731773786769....\n",
      "Validation loss: 0.34385953968125044....\n",
      "-----------------------------------\n",
      "Training loss: 0.40376634145855494....\n",
      "Validation loss: 0.34370940175261194....\n",
      "-----------------------------------\n",
      "Training loss: 0.4035599142985289....\n",
      "Validation loss: 0.3435594595968055....\n",
      "-----------------------------------\n",
      "Training loss: 0.40335385961255227....\n",
      "Validation loss: 0.3434097105362878....\n",
      "-----------------------------------\n",
      "Training loss: 0.4031481727674868....\n",
      "Validation loss: 0.3432601599695744....\n",
      "-----------------------------------\n",
      "Training loss: 0.4029427969108252....\n",
      "Validation loss: 0.34311077702464177....\n",
      "-----------------------------------\n",
      "Training loss: 0.4027376985770779....\n",
      "Validation loss: 0.34296152524785906....\n",
      "-----------------------------------\n",
      "Training loss: 0.4025328450598507....\n",
      "Validation loss: 0.3428125122851547....\n",
      "-----------------------------------\n",
      "Training loss: 0.4023283022356338....\n",
      "Validation loss: 0.3426636902380697....\n",
      "-----------------------------------\n",
      "Training loss: 0.4021241326206328....\n",
      "Validation loss: 0.34251506988586267....\n",
      "-----------------------------------\n",
      "Training loss: 0.40192028291000853....\n",
      "Validation loss: 0.34236666846349906....\n",
      "-----------------------------------\n",
      "Training loss: 0.4017167330764586....\n",
      "Validation loss: 0.3422184751413665....\n",
      "-----------------------------------\n",
      "Training loss: 0.40151351755263387....\n",
      "Validation loss: 0.3420704684434824....\n",
      "-----------------------------------\n",
      "Training loss: 0.4013106044719978....\n",
      "Validation loss: 0.34192259652812274....\n",
      "-----------------------------------\n",
      "Training loss: 0.40110801512497357....\n",
      "Validation loss: 0.3417748922363097....\n",
      "-----------------------------------\n",
      "Training loss: 0.40090573322098666....\n",
      "Validation loss: 0.34162738750090715....\n",
      "-----------------------------------\n",
      "Training loss: 0.40070376794947005....\n",
      "Validation loss: 0.3414801374213576....\n",
      "-----------------------------------\n",
      "Training loss: 0.4005021371239709....\n",
      "Validation loss: 0.34133310601251293....\n",
      "-----------------------------------\n",
      "Training loss: 0.4003008180869818....\n",
      "Validation loss: 0.3411862282914938....\n",
      "-----------------------------------\n",
      "Training loss: 0.4000999384947621....\n",
      "Validation loss: 0.34103961802899696....\n",
      "-----------------------------------\n",
      "Training loss: 0.3998992959480436....\n",
      "Validation loss: 0.34089320094837805....\n",
      "-----------------------------------\n",
      "Training loss: 0.3996989476953844....\n",
      "Validation loss: 0.34074697803133763....\n",
      "-----------------------------------\n",
      "Training loss: 0.39949881612962973....\n",
      "Validation loss: 0.3406009330440925....\n",
      "-----------------------------------\n",
      "Training loss: 0.3992988632359983....\n",
      "Validation loss: 0.34045511947379486....\n",
      "-----------------------------------\n",
      "Training loss: 0.39909926393007694....\n",
      "Validation loss: 0.3403095039036297....\n",
      "-----------------------------------\n",
      "Training loss: 0.3988999679273109....\n",
      "Validation loss: 0.34016407437975743....\n",
      "-----------------------------------\n",
      "Training loss: 0.3987009100815375....\n",
      "Validation loss: 0.34001880556775016....\n",
      "-----------------------------------\n",
      "Training loss: 0.398502162718106....\n",
      "Validation loss: 0.3398737738617338....\n",
      "-----------------------------------\n",
      "Training loss: 0.3983036659679541....\n",
      "Validation loss: 0.339728961061742....\n",
      "-----------------------------------\n",
      "Training loss: 0.3981054597622714....\n",
      "Validation loss: 0.33958425527543906....\n",
      "-----------------------------------\n",
      "Training loss: 0.39790760187050356....\n",
      "Validation loss: 0.3394397276783671....\n",
      "-----------------------------------\n",
      "Training loss: 0.39771008293680565....\n",
      "Validation loss: 0.3392954438287367....\n",
      "-----------------------------------\n",
      "Training loss: 0.39751289585580185....\n",
      "Validation loss: 0.339151412344097....\n",
      "-----------------------------------\n",
      "Training loss: 0.3973160122830043....\n",
      "Validation loss: 0.3390075631572379....\n",
      "-----------------------------------\n",
      "Training loss: 0.39711940906946064....\n",
      "Validation loss: 0.33886391051089043....\n",
      "-----------------------------------\n",
      "Training loss: 0.39692312813448705....\n",
      "Validation loss: 0.338720439385008....\n",
      "-----------------------------------\n",
      "Training loss: 0.39672714079634114....\n",
      "Validation loss: 0.3385772204972823....\n",
      "-----------------------------------\n",
      "Training loss: 0.3965314515678545....\n",
      "Validation loss: 0.33843425445244724....\n",
      "-----------------------------------\n",
      "Training loss: 0.3963360969595525....\n",
      "Validation loss: 0.3382914961190897....\n",
      "-----------------------------------\n",
      "Training loss: 0.39614101177682676....\n",
      "Validation loss: 0.33814889646770396....\n",
      "-----------------------------------\n",
      "Training loss: 0.3959462453425525....\n",
      "Validation loss: 0.3380064949413339....\n",
      "-----------------------------------\n",
      "Training loss: 0.3957518195142435....\n",
      "Validation loss: 0.3378642972116176....\n",
      "-----------------------------------\n",
      "Training loss: 0.39555768682371684....\n",
      "Validation loss: 0.3377222342817885....\n",
      "-----------------------------------\n",
      "Training loss: 0.3953638878409593....\n",
      "Validation loss: 0.33758035461326386....\n",
      "-----------------------------------\n",
      "Training loss: 0.3951703829496158....\n",
      "Validation loss: 0.33743868845510194....\n",
      "-----------------------------------\n",
      "Training loss: 0.39497718844116414....\n",
      "Validation loss: 0.3372972510268233....\n",
      "-----------------------------------\n",
      "Training loss: 0.3947842220552514....\n",
      "Validation loss: 0.33715599147707764....\n",
      "-----------------------------------\n",
      "Training loss: 0.39459150737068527....\n",
      "Validation loss: 0.3370149086383895....\n",
      "-----------------------------------\n",
      "Training loss: 0.3943990055400403....\n",
      "Validation loss: 0.336874026292899....\n",
      "-----------------------------------\n",
      "Training loss: 0.39420679572063333....\n",
      "Validation loss: 0.33673335311943814....\n",
      "-----------------------------------\n",
      "Training loss: 0.3940148362941596....\n",
      "Validation loss: 0.33659290756671906....\n",
      "-----------------------------------\n",
      "Training loss: 0.3938231414988326....\n",
      "Validation loss: 0.3364526558726576....\n",
      "-----------------------------------\n",
      "Training loss: 0.3936317504908991....\n",
      "Validation loss: 0.33631259377396466....\n",
      "-----------------------------------\n",
      "Training loss: 0.3934406324626501....\n",
      "Validation loss: 0.33617273704822426....\n",
      "-----------------------------------\n",
      "Training loss: 0.39324970946527676....\n",
      "Validation loss: 0.3360330682921907....\n",
      "-----------------------------------\n",
      "Training loss: 0.39305905873136526....\n",
      "Validation loss: 0.33589355212081784....\n",
      "-----------------------------------\n",
      "Training loss: 0.3928686287467213....\n",
      "Validation loss: 0.3357542197789089....\n",
      "-----------------------------------\n",
      "Training loss: 0.39267842087450866....\n",
      "Validation loss: 0.33561505970443684....\n",
      "-----------------------------------\n",
      "Training loss: 0.39248845589809983....\n",
      "Validation loss: 0.33547604524818025....\n",
      "-----------------------------------\n",
      "Training loss: 0.39229876465719055....\n",
      "Validation loss: 0.3353372821260559....\n",
      "-----------------------------------\n",
      "Training loss: 0.3921093634266882....\n",
      "Validation loss: 0.3351986910467412....\n",
      "-----------------------------------\n",
      "Training loss: 0.3919202021333275....\n",
      "Validation loss: 0.33506032169995353....\n",
      "-----------------------------------\n",
      "Training loss: 0.39173130675205003....\n",
      "Validation loss: 0.3349221213225468....\n",
      "-----------------------------------\n",
      "Training loss: 0.39154267245540497....\n",
      "Validation loss: 0.3347840520925805....\n",
      "-----------------------------------\n",
      "Training loss: 0.39135430183584496....\n",
      "Validation loss: 0.3346461727469472....\n",
      "-----------------------------------\n",
      "Training loss: 0.3911661893119167....\n",
      "Validation loss: 0.33450849661523946....\n",
      "-----------------------------------\n",
      "Training loss: 0.3909783032386606....\n",
      "Validation loss: 0.3343710302203502....\n",
      "-----------------------------------\n",
      "Training loss: 0.39079072418891064....\n",
      "Validation loss: 0.3342337869793964....\n",
      "-----------------------------------\n",
      "Training loss: 0.3906033092034768....\n",
      "Validation loss: 0.3340967443035115....\n",
      "-----------------------------------\n",
      "Training loss: 0.39041618670478867....\n",
      "Validation loss: 0.33395985288371305....\n",
      "-----------------------------------\n",
      "Training loss: 0.39022945349660254....\n",
      "Validation loss: 0.33382312295225575....\n",
      "-----------------------------------\n",
      "Training loss: 0.39004293744791....\n",
      "Validation loss: 0.33368654197298087....\n",
      "-----------------------------------\n",
      "Training loss: 0.3898566745968109....\n",
      "Validation loss: 0.333550155263315....\n",
      "-----------------------------------\n",
      "Training loss: 0.3896707069493421....\n",
      "Validation loss: 0.3334138998448528....\n",
      "-----------------------------------\n",
      "Training loss: 0.38948506101965524....\n",
      "Validation loss: 0.3332778288580939....\n",
      "-----------------------------------\n",
      "Training loss: 0.38929968984062796....\n",
      "Validation loss: 0.3331419178515157....\n",
      "-----------------------------------\n",
      "Training loss: 0.3891145606550732....\n",
      "Validation loss: 0.3330061885684686....\n",
      "-----------------------------------\n",
      "Training loss: 0.388929640314143....\n",
      "Validation loss: 0.332870630636922....\n",
      "-----------------------------------\n",
      "Training loss: 0.38874496416987375....\n",
      "Validation loss: 0.3327352586937057....\n",
      "-----------------------------------\n",
      "Training loss: 0.38856056318309....\n",
      "Validation loss: 0.33260004841741786....\n",
      "-----------------------------------\n",
      "Training loss: 0.38837642698590286....\n",
      "Validation loss: 0.33246500857605144....\n",
      "-----------------------------------\n",
      "Training loss: 0.38819255592863805....\n",
      "Validation loss: 0.33233014758388846....\n",
      "-----------------------------------\n",
      "Training loss: 0.3880089579851531....\n",
      "Validation loss: 0.332195469361661....\n",
      "-----------------------------------\n",
      "Training loss: 0.38782563133230946....\n",
      "Validation loss: 0.33206095702799027....\n",
      "-----------------------------------\n",
      "Training loss: 0.3876425600387985....\n",
      "Validation loss: 0.33192664854841025....\n",
      "-----------------------------------\n",
      "Training loss: 0.38745977520968644....\n",
      "Validation loss: 0.33179243138540016....\n",
      "-----------------------------------\n",
      "Training loss: 0.3872772489366435....\n",
      "Validation loss: 0.3316583581447784....\n",
      "-----------------------------------\n",
      "Training loss: 0.387094965747727....\n",
      "Validation loss: 0.33152444382386137....\n",
      "-----------------------------------\n",
      "Training loss: 0.3869128723418669....\n",
      "Validation loss: 0.3313907300822571....\n",
      "-----------------------------------\n",
      "Training loss: 0.38673101160169976....\n",
      "Validation loss: 0.33125721620492216....\n",
      "-----------------------------------\n",
      "Training loss: 0.3865494035078444....\n",
      "Validation loss: 0.33112389839848044....\n",
      "-----------------------------------\n",
      "Training loss: 0.38636804850979445....\n",
      "Validation loss: 0.3309907627544243....\n",
      "-----------------------------------\n",
      "Training loss: 0.3861869628722633....\n",
      "Validation loss: 0.3308577826875101....\n",
      "-----------------------------------\n",
      "Training loss: 0.38600607158021405....\n",
      "Validation loss: 0.33072496998533296....\n",
      "-----------------------------------\n",
      "Training loss: 0.3858254082661416....\n",
      "Validation loss: 0.33059237865253965....\n",
      "-----------------------------------\n",
      "Training loss: 0.38564490809385166....\n",
      "Validation loss: 0.3304599666900117....\n",
      "-----------------------------------\n",
      "Training loss: 0.38546467235291115....\n",
      "Validation loss: 0.3303277306777105....\n",
      "-----------------------------------\n",
      "Training loss: 0.3852846761604617....\n",
      "Validation loss: 0.33019568683145956....\n",
      "-----------------------------------\n",
      "Training loss: 0.3851048761125254....\n",
      "Validation loss: 0.33006379480730264....\n",
      "-----------------------------------\n",
      "Training loss: 0.3849253118543886....\n",
      "Validation loss: 0.32993204595325776....\n",
      "-----------------------------------\n",
      "Training loss: 0.3847460007665291....\n",
      "Validation loss: 0.32980045358531807....\n",
      "-----------------------------------\n",
      "Training loss: 0.38456696016796116....\n",
      "Validation loss: 0.3296690318471407....\n",
      "-----------------------------------\n",
      "Training loss: 0.38438816376793306....\n",
      "Validation loss: 0.3295377226182824....\n",
      "-----------------------------------\n",
      "Training loss: 0.38420961922402685....\n",
      "Validation loss: 0.3294065863228851....\n",
      "-----------------------------------\n",
      "Training loss: 0.384031314854971....\n",
      "Validation loss: 0.32927562676661165....\n",
      "-----------------------------------\n",
      "Training loss: 0.3838532358825905....\n",
      "Validation loss: 0.32914487779545865....\n",
      "-----------------------------------\n",
      "Training loss: 0.383675488549217....\n",
      "Validation loss: 0.3290142914683703....\n",
      "-----------------------------------\n",
      "Training loss: 0.38349800194938666....\n",
      "Validation loss: 0.32888385904148076....\n",
      "-----------------------------------\n",
      "Training loss: 0.383320727553412....\n",
      "Validation loss: 0.32875350249250446....\n",
      "-----------------------------------\n",
      "Training loss: 0.38314359419417....\n",
      "Validation loss: 0.3286233243970379....\n",
      "-----------------------------------\n",
      "Training loss: 0.3829666746419361....\n",
      "Validation loss: 0.3284933038667456....\n",
      "-----------------------------------\n",
      "Training loss: 0.38278989272257186....\n",
      "Validation loss: 0.32836345696576197....\n",
      "-----------------------------------\n",
      "Training loss: 0.3826133517410004....\n",
      "Validation loss: 0.32823378812888043....\n",
      "-----------------------------------\n",
      "Training loss: 0.38243705610082496....\n",
      "Validation loss: 0.32810428385951845....\n",
      "-----------------------------------\n",
      "Training loss: 0.38226091757322134....\n",
      "Validation loss: 0.32797485670383164....\n",
      "-----------------------------------\n",
      "Training loss: 0.38208487290116744....\n",
      "Validation loss: 0.3278455123958212....\n",
      "-----------------------------------\n",
      "Training loss: 0.38190905788198726....\n",
      "Validation loss: 0.327716351183781....\n",
      "-----------------------------------\n",
      "Training loss: 0.38173349862626843....\n",
      "Validation loss: 0.32758720178571....\n",
      "-----------------------------------\n",
      "Training loss: 0.3815581008197057....\n",
      "Validation loss: 0.3274582469463771....\n",
      "-----------------------------------\n",
      "Training loss: 0.381382917668837....\n",
      "Validation loss: 0.32732944859893465....\n",
      "-----------------------------------\n",
      "Training loss: 0.3812079600225896....\n",
      "Validation loss: 0.3272008135713383....\n",
      "-----------------------------------\n",
      "Training loss: 0.3810331893544774....\n",
      "Validation loss: 0.3270723743666484....\n",
      "-----------------------------------\n",
      "Training loss: 0.3808586177576208....\n",
      "Validation loss: 0.32694417734348374....\n",
      "-----------------------------------\n",
      "Training loss: 0.3806842669770499....\n",
      "Validation loss: 0.32681597358748277....\n",
      "-----------------------------------\n",
      "Training loss: 0.38051001832859943....\n",
      "Validation loss: 0.3266879372778166....\n",
      "-----------------------------------\n",
      "Training loss: 0.38033600410361806....\n",
      "Validation loss: 0.32656008380460294....\n",
      "-----------------------------------\n",
      "Training loss: 0.38016223343357436....\n",
      "Validation loss: 0.3264324020484867....\n",
      "-----------------------------------\n",
      "Training loss: 0.3799886616033476....\n",
      "Validation loss: 0.32630488995357193....\n",
      "-----------------------------------\n",
      "Training loss: 0.3798153308330317....\n",
      "Validation loss: 0.32617756849708246....\n",
      "-----------------------------------\n",
      "Training loss: 0.3796422044691558....\n",
      "Validation loss: 0.32605037868154063....\n",
      "-----------------------------------\n",
      "Training loss: 0.3794691985706807....\n",
      "Validation loss: 0.32592338092979684....\n",
      "-----------------------------------\n",
      "Training loss: 0.3792964107091218....\n",
      "Validation loss: 0.3257965327992464....\n",
      "-----------------------------------\n",
      "Training loss: 0.37912380337949597....\n",
      "Validation loss: 0.32566985903029....\n",
      "-----------------------------------\n",
      "Training loss: 0.37895138395523115....\n",
      "Validation loss: 0.3255433648024122....\n",
      "-----------------------------------\n",
      "Training loss: 0.37877918402167826....\n",
      "Validation loss: 0.32541709399914104....\n",
      "-----------------------------------\n",
      "Training loss: 0.37860722326084534....\n",
      "Validation loss: 0.3252910605503963....\n",
      "-----------------------------------\n",
      "Training loss: 0.3784355136948633....\n",
      "Validation loss: 0.3251651942270012....\n",
      "-----------------------------------\n",
      "Training loss: 0.37826402964228983....\n",
      "Validation loss: 0.3250394701957239....\n",
      "-----------------------------------\n",
      "Training loss: 0.3780927065409531....\n",
      "Validation loss: 0.32491392386401485....\n",
      "-----------------------------------\n",
      "Training loss: 0.3779215979288324....\n",
      "Validation loss: 0.3247884893027967....\n",
      "-----------------------------------\n",
      "Training loss: 0.37775071448750913....\n",
      "Validation loss: 0.32466329562491436....\n",
      "-----------------------------------\n",
      "Training loss: 0.3775800640497271....\n",
      "Validation loss: 0.3245381934581566....\n",
      "-----------------------------------\n",
      "Training loss: 0.37740964423919765....\n",
      "Validation loss: 0.32441336420431377....\n",
      "-----------------------------------\n",
      "Training loss: 0.3772394142998333....\n",
      "Validation loss: 0.32428857226131524....\n",
      "-----------------------------------\n",
      "Training loss: 0.37706936984282796....\n",
      "Validation loss: 0.3241639952382407....\n",
      "-----------------------------------\n",
      "Training loss: 0.3768995249795193....\n",
      "Validation loss: 0.32403953752992304....\n",
      "-----------------------------------\n",
      "Training loss: 0.37672983967398427....\n",
      "Validation loss: 0.3239152130167455....\n",
      "-----------------------------------\n",
      "Training loss: 0.37656036951624733....\n",
      "Validation loss: 0.32379110761877483....\n",
      "-----------------------------------\n",
      "Training loss: 0.37639112900738114....\n",
      "Validation loss: 0.32366701819244104....\n",
      "-----------------------------------\n",
      "Training loss: 0.37622210510729603....\n",
      "Validation loss: 0.3235431737729548....\n",
      "-----------------------------------\n",
      "Training loss: 0.3760532923147059....\n",
      "Validation loss: 0.3234194132759774....\n",
      "-----------------------------------\n",
      "Training loss: 0.3758847103027835....\n",
      "Validation loss: 0.32329591380082284....\n",
      "-----------------------------------\n",
      "Training loss: 0.37571636599802116....\n",
      "Validation loss: 0.32317249289856653....\n",
      "-----------------------------------\n",
      "Training loss: 0.3755481636024508....\n",
      "Validation loss: 0.3230491283178021....\n",
      "-----------------------------------\n",
      "Training loss: 0.3753801271148454....\n",
      "Validation loss: 0.32292604656382645....\n",
      "-----------------------------------\n",
      "Training loss: 0.37521228172911003....\n",
      "Validation loss: 0.32280296309898737....\n",
      "-----------------------------------\n",
      "Training loss: 0.3750445775334347....\n",
      "Validation loss: 0.322680128293626....\n",
      "-----------------------------------\n",
      "Training loss: 0.3748771175181839....\n",
      "Validation loss: 0.32255742264909193....\n",
      "-----------------------------------\n",
      "Training loss: 0.3747098634183126....\n",
      "Validation loss: 0.32243501847620337....\n",
      "-----------------------------------\n",
      "Training loss: 0.3745428071270802....\n",
      "Validation loss: 0.3223126640900857....\n",
      "-----------------------------------\n",
      "Training loss: 0.3743759651611619....\n",
      "Validation loss: 0.3221905871907328....\n",
      "-----------------------------------\n",
      "Training loss: 0.3742093418563033....\n",
      "Validation loss: 0.32206862567021366....\n",
      "-----------------------------------\n",
      "Training loss: 0.3740429470737305....\n",
      "Validation loss: 0.32194675267763706....\n",
      "-----------------------------------\n",
      "Training loss: 0.37387678138205604....\n",
      "Validation loss: 0.3218251724000203....\n",
      "-----------------------------------\n",
      "Training loss: 0.37371083084721396....\n",
      "Validation loss: 0.3217036614457539....\n",
      "-----------------------------------\n",
      "Training loss: 0.3735451274989898....\n",
      "Validation loss: 0.32158241792996217....\n",
      "-----------------------------------\n",
      "Training loss: 0.3733796380147421....\n",
      "Validation loss: 0.3214612365199098....\n",
      "-----------------------------------\n",
      "Training loss: 0.37321434978876816....\n",
      "Validation loss: 0.32134026880920935....\n",
      "-----------------------------------\n",
      "Training loss: 0.37304924646137194....\n",
      "Validation loss: 0.3212194900175946....\n",
      "-----------------------------------\n",
      "Training loss: 0.3728843475648791....\n",
      "Validation loss: 0.32109881229493437....\n",
      "-----------------------------------\n",
      "Training loss: 0.3727196641742819....\n",
      "Validation loss: 0.3209783955836674....\n",
      "-----------------------------------\n",
      "Training loss: 0.37255519097342893....\n",
      "Validation loss: 0.3208580316390299....\n",
      "-----------------------------------\n",
      "Training loss: 0.37239089182425744....\n",
      "Validation loss: 0.3207378837103069....\n",
      "-----------------------------------\n",
      "Training loss: 0.37222679429764594....\n",
      "Validation loss: 0.32061778712002703....\n",
      "-----------------------------------\n",
      "Training loss: 0.37206291984442963....\n",
      "Validation loss: 0.3204979700383941....\n",
      "-----------------------------------\n",
      "Training loss: 0.37189921597694675....\n",
      "Validation loss: 0.3203781834197181....\n",
      "-----------------------------------\n",
      "Training loss: 0.37173572275232314....\n",
      "Validation loss: 0.32025857623385917....\n",
      "-----------------------------------\n",
      "Training loss: 0.3715724361335613....\n",
      "Validation loss: 0.320139055389489....\n",
      "-----------------------------------\n",
      "Training loss: 0.3714093539441194....\n",
      "Validation loss: 0.3200196746685068....\n",
      "-----------------------------------\n",
      "Training loss: 0.3712464672817614....\n",
      "Validation loss: 0.319900500489077....\n",
      "-----------------------------------\n",
      "Training loss: 0.371083796873253....\n",
      "Validation loss: 0.31978138318055455....\n",
      "-----------------------------------\n",
      "Training loss: 0.3709213273432552....\n",
      "Validation loss: 0.3196625362719958....\n",
      "-----------------------------------\n",
      "Training loss: 0.370759069343573....\n",
      "Validation loss: 0.3195437756380729....\n",
      "-----------------------------------\n",
      "Training loss: 0.3705969935633418....\n",
      "Validation loss: 0.31942524757289814....\n",
      "-----------------------------------\n",
      "Training loss: 0.370435129138828....\n",
      "Validation loss: 0.31930678406874485....\n",
      "-----------------------------------\n",
      "Training loss: 0.3702734427004723....\n",
      "Validation loss: 0.31918858229244207....\n",
      "-----------------------------------\n",
      "Training loss: 0.37011194924825874....\n",
      "Validation loss: 0.3190705237535965....\n",
      "-----------------------------------\n",
      "Training loss: 0.3699506726192293....\n",
      "Validation loss: 0.31895265036868375....\n",
      "-----------------------------------\n",
      "Training loss: 0.3697895935123254....\n",
      "Validation loss: 0.3188349826472797....\n",
      "-----------------------------------\n",
      "Training loss: 0.36962872990414747....\n",
      "Validation loss: 0.3187173474766941....\n",
      "-----------------------------------\n",
      "Training loss: 0.36946803461112765....\n",
      "Validation loss: 0.31859986434846715....\n",
      "-----------------------------------\n",
      "Training loss: 0.3693074885587804....\n",
      "Validation loss: 0.31848242026858875....\n",
      "-----------------------------------\n",
      "Training loss: 0.369147147247584....\n",
      "Validation loss: 0.31836519765468746....\n",
      "-----------------------------------\n",
      "Training loss: 0.3689869757840492....\n",
      "Validation loss: 0.3182481145374112....\n",
      "-----------------------------------\n",
      "Training loss: 0.3688270110043324....\n",
      "Validation loss: 0.31813110765936425....\n",
      "-----------------------------------\n",
      "Training loss: 0.3686672498014489....\n",
      "Validation loss: 0.31801432593839785....\n",
      "-----------------------------------\n",
      "Training loss: 0.36850769869425837....\n",
      "Validation loss: 0.31789757122785656....\n",
      "-----------------------------------\n",
      "Training loss: 0.36834835432475865....\n",
      "Validation loss: 0.3177810214937103....\n",
      "-----------------------------------\n",
      "Training loss: 0.36818924286302984....\n",
      "Validation loss: 0.317664577280867....\n",
      "-----------------------------------\n",
      "Training loss: 0.3680303244474756....\n",
      "Validation loss: 0.31754833209673494....\n",
      "-----------------------------------\n",
      "Training loss: 0.3678715857912054....\n",
      "Validation loss: 0.3174321982950835....\n",
      "-----------------------------------\n",
      "Training loss: 0.36771304421670287....\n",
      "Validation loss: 0.31731627081253067....\n",
      "-----------------------------------\n",
      "Training loss: 0.3675547119870662....\n",
      "Validation loss: 0.31720046019042947....\n",
      "-----------------------------------\n",
      "Training loss: 0.3673966028180491....\n",
      "Validation loss: 0.3170847366356626....\n",
      "-----------------------------------\n",
      "Training loss: 0.36723868903526724....\n",
      "Validation loss: 0.3169691924226205....\n",
      "-----------------------------------\n",
      "Training loss: 0.367080947366685....\n",
      "Validation loss: 0.31685383005265905....\n",
      "-----------------------------------\n",
      "Training loss: 0.3669234091161509....\n",
      "Validation loss: 0.31673850273894866....\n",
      "-----------------------------------\n",
      "Training loss: 0.3667660758507293....\n",
      "Validation loss: 0.3166234196521825....\n",
      "-----------------------------------\n",
      "Training loss: 0.36660888037708117....\n",
      "Validation loss: 0.31650845800921756....\n",
      "-----------------------------------\n",
      "Training loss: 0.36645173881190773....\n",
      "Validation loss: 0.3163936498023855....\n",
      "-----------------------------------\n",
      "Training loss: 0.36629477478586137....\n",
      "Validation loss: 0.31627895077458373....\n",
      "-----------------------------------\n",
      "Training loss: 0.3661379761509926....\n",
      "Validation loss: 0.31616429664119017....\n",
      "-----------------------------------\n",
      "Training loss: 0.365981390453161....\n",
      "Validation loss: 0.31604989485467705....\n",
      "-----------------------------------\n",
      "Training loss: 0.36582510346277897....\n",
      "Validation loss: 0.3159355014144111....\n",
      "-----------------------------------\n",
      "Training loss: 0.36566905665868754....\n",
      "Validation loss: 0.31582135817434526....\n",
      "-----------------------------------\n",
      "Training loss: 0.36551323798837554....\n",
      "Validation loss: 0.31570732759977305....\n",
      "-----------------------------------\n",
      "Training loss: 0.36535760739895046....\n",
      "Validation loss: 0.3155934247353405....\n",
      "-----------------------------------\n",
      "Training loss: 0.36520217986360654....\n",
      "Validation loss: 0.3154797027399059....\n",
      "-----------------------------------\n",
      "Training loss: 0.36504698571410504....\n",
      "Validation loss: 0.3153661495712184....\n",
      "-----------------------------------\n",
      "Training loss: 0.3648920153448022....\n",
      "Validation loss: 0.3152527121400274....\n",
      "-----------------------------------\n",
      "Training loss: 0.36473729606557415....\n",
      "Validation loss: 0.315139482889496....\n",
      "-----------------------------------\n",
      "Training loss: 0.3645827454211145....\n",
      "Validation loss: 0.31502642732153635....\n",
      "-----------------------------------\n",
      "Training loss: 0.3644283979097207....\n",
      "Validation loss: 0.3149134069861521....\n",
      "-----------------------------------\n",
      "Training loss: 0.36427423354395383....\n",
      "Validation loss: 0.3148006399789757....\n",
      "-----------------------------------\n",
      "Training loss: 0.3641202630328511....\n",
      "Validation loss: 0.31468792557173314....\n",
      "-----------------------------------\n",
      "Training loss: 0.36396645239369574....\n",
      "Validation loss: 0.3145754138835965....\n",
      "-----------------------------------\n",
      "Training loss: 0.36381282557120176....\n",
      "Validation loss: 0.31446300251471276....\n",
      "-----------------------------------\n",
      "Training loss: 0.36365937787178126....\n",
      "Validation loss: 0.31435066672377593....\n",
      "-----------------------------------\n",
      "Training loss: 0.3635061162650233....\n",
      "Validation loss: 0.31423855339241524....\n",
      "-----------------------------------\n",
      "Training loss: 0.3633530229655824....\n",
      "Validation loss: 0.3141265044875934....\n",
      "-----------------------------------\n",
      "Training loss: 0.36320004242556525....\n",
      "Validation loss: 0.3140148202651843....\n",
      "-----------------------------------\n",
      "Training loss: 0.3630471551475332....\n",
      "Validation loss: 0.31390324315165785....\n",
      "-----------------------------------\n",
      "Training loss: 0.36289446729740155....\n",
      "Validation loss: 0.31379179405762886....\n",
      "-----------------------------------\n",
      "Training loss: 0.36274197201680225....\n",
      "Validation loss: 0.3136804936981674....\n",
      "-----------------------------------\n",
      "Training loss: 0.3625896535863727....\n",
      "Validation loss: 0.3135692705632538....\n",
      "-----------------------------------\n",
      "Training loss: 0.36243746963337103....\n",
      "Validation loss: 0.3134582852350996....\n",
      "-----------------------------------\n",
      "Training loss: 0.3622854393860562....\n",
      "Validation loss: 0.3133473861353985....\n",
      "-----------------------------------\n",
      "Training loss: 0.3621335712437117....\n",
      "Validation loss: 0.3132365408589125....\n",
      "-----------------------------------\n",
      "Training loss: 0.3619818708088319....\n",
      "Validation loss: 0.3131259284457627....\n",
      "-----------------------------------\n",
      "Training loss: 0.3618303429357006....\n",
      "Validation loss: 0.313015337526844....\n",
      "-----------------------------------\n",
      "Training loss: 0.3616790384977725....\n",
      "Validation loss: 0.3129049128745171....\n",
      "-----------------------------------\n",
      "Training loss: 0.36152791431740516....\n",
      "Validation loss: 0.31279465342280116....\n",
      "-----------------------------------\n",
      "Training loss: 0.3613769543335535....\n",
      "Validation loss: 0.31268445730687233....\n",
      "-----------------------------------\n",
      "Training loss: 0.36122617060440854....\n",
      "Validation loss: 0.31257443749984726....\n",
      "-----------------------------------\n",
      "Training loss: 0.36107556777257827....\n",
      "Validation loss: 0.31246448729404547....\n",
      "-----------------------------------\n",
      "Training loss: 0.3609251721677093....\n",
      "Validation loss: 0.31235469313141806....\n",
      "-----------------------------------\n",
      "Training loss: 0.3607749519563239....\n",
      "Validation loss: 0.3122450315300353....\n",
      "-----------------------------------\n",
      "Training loss: 0.3606248880036219....\n",
      "Validation loss: 0.31213539422131886....\n",
      "-----------------------------------\n",
      "Training loss: 0.3604749759316763....\n",
      "Validation loss: 0.31202592160851567....\n",
      "-----------------------------------\n",
      "Training loss: 0.3603252239062565....\n",
      "Validation loss: 0.31191660004996....\n",
      "-----------------------------------\n",
      "Training loss: 0.36017559159757706....\n",
      "Validation loss: 0.311807463857423....\n",
      "-----------------------------------\n",
      "Training loss: 0.3600261312419693....\n",
      "Validation loss: 0.31169842944151044....\n",
      "-----------------------------------\n",
      "Training loss: 0.3598768770612407....\n",
      "Validation loss: 0.31158950711954064....\n",
      "-----------------------------------\n",
      "Training loss: 0.359727837533571....\n",
      "Validation loss: 0.31148072466645654....\n",
      "-----------------------------------\n",
      "Training loss: 0.35957895922020316....\n",
      "Validation loss: 0.3113721344890715....\n",
      "-----------------------------------\n",
      "Training loss: 0.3594302484027795....\n",
      "Validation loss: 0.31126352109499983....\n",
      "-----------------------------------\n",
      "Training loss: 0.3592817177852348....\n",
      "Validation loss: 0.3111551216077299....\n",
      "-----------------------------------\n",
      "Training loss: 0.3591333696837114....\n",
      "Validation loss: 0.3110468135626215....\n",
      "-----------------------------------\n",
      "Training loss: 0.3589851948099492....\n",
      "Validation loss: 0.31093871125735933....\n",
      "-----------------------------------\n",
      "Training loss: 0.3588371835222982....\n",
      "Validation loss: 0.3108307194874737....\n",
      "-----------------------------------\n",
      "Training loss: 0.35868931931892994....\n",
      "Validation loss: 0.31072283300035414....\n",
      "-----------------------------------\n",
      "Training loss: 0.3585416380539256....\n",
      "Validation loss: 0.31061511416308557....\n",
      "-----------------------------------\n",
      "Training loss: 0.35839413537884895....\n",
      "Validation loss: 0.31050754590919566....\n",
      "-----------------------------------\n",
      "Training loss: 0.3582467764628698....\n",
      "Validation loss: 0.31040003416968964....\n",
      "-----------------------------------\n",
      "Training loss: 0.35809958647855084....\n",
      "Validation loss: 0.310292792331478....\n",
      "-----------------------------------\n",
      "Training loss: 0.3579525856027988....\n",
      "Validation loss: 0.3101856752483592....\n",
      "-----------------------------------\n",
      "Training loss: 0.357805769560991....\n",
      "Validation loss: 0.3100786429922756....\n",
      "-----------------------------------\n",
      "Training loss: 0.35765910932855666....\n",
      "Validation loss: 0.30997181383053257....\n",
      "-----------------------------------\n",
      "Training loss: 0.35751262613622703....\n",
      "Validation loss: 0.30986509542631574....\n",
      "-----------------------------------\n",
      "Training loss: 0.3573663038431034....\n",
      "Validation loss: 0.30975842293225386....\n",
      "-----------------------------------\n",
      "Training loss: 0.3572201536457004....\n",
      "Validation loss: 0.30965199332411947....\n",
      "-----------------------------------\n",
      "Training loss: 0.35707416618789306....\n",
      "Validation loss: 0.309545636320516....\n",
      "-----------------------------------\n",
      "Training loss: 0.35692834757334674....\n",
      "Validation loss: 0.3094393481673603....\n",
      "-----------------------------------\n",
      "Training loss: 0.35678266397811537....\n",
      "Validation loss: 0.30933318098420354....\n",
      "-----------------------------------\n",
      "Training loss: 0.35663711791162467....\n",
      "Validation loss: 0.30922710534319015....\n",
      "-----------------------------------\n",
      "Training loss: 0.35649175253197407....\n",
      "Validation loss: 0.30912115693208786....\n",
      "-----------------------------------\n",
      "Training loss: 0.3563465599531155....\n",
      "Validation loss: 0.30901533355177396....\n",
      "-----------------------------------\n",
      "Training loss: 0.35620153872521254....\n",
      "Validation loss: 0.3089096871468155....\n",
      "-----------------------------------\n",
      "Training loss: 0.3560566917260228....\n",
      "Validation loss: 0.30880410801974084....\n",
      "-----------------------------------\n",
      "Training loss: 0.3559120085495516....\n",
      "Validation loss: 0.3086987047237439....\n",
      "-----------------------------------\n",
      "Training loss: 0.35576751695213044....\n",
      "Validation loss: 0.30859353137011236....\n",
      "-----------------------------------\n",
      "Training loss: 0.3556232289817161....\n",
      "Validation loss: 0.30848836012759967....\n",
      "-----------------------------------\n",
      "Training loss: 0.3554790832256605....\n",
      "Validation loss: 0.3083833957980658....\n",
      "-----------------------------------\n",
      "Training loss: 0.35533508742650943....\n",
      "Validation loss: 0.30827854659914916....\n",
      "-----------------------------------\n",
      "Training loss: 0.35519123772206934....\n",
      "Validation loss: 0.30817378123671685....\n",
      "-----------------------------------\n",
      "Training loss: 0.3550475812957734....\n",
      "Validation loss: 0.3080692304793217....\n",
      "-----------------------------------\n",
      "Training loss: 0.35490408256398387....\n",
      "Validation loss: 0.30796471975840295....\n",
      "-----------------------------------\n",
      "Training loss: 0.3547607124497508....\n",
      "Validation loss: 0.30786026533649546....\n",
      "-----------------------------------\n",
      "Training loss: 0.35461746379238795....\n",
      "Validation loss: 0.3077560925234427....\n",
      "-----------------------------------\n",
      "Training loss: 0.35447436109205166....\n",
      "Validation loss: 0.3076520738490357....\n",
      "-----------------------------------\n",
      "Training loss: 0.35433145504611097....\n",
      "Validation loss: 0.3075481000594143....\n",
      "-----------------------------------\n",
      "Training loss: 0.35418871114618455....\n",
      "Validation loss: 0.3074443263435224....\n",
      "-----------------------------------\n",
      "Training loss: 0.3540461018749802....\n",
      "Validation loss: 0.30734064882303147....\n",
      "-----------------------------------\n",
      "Training loss: 0.3539036645526411....\n",
      "Validation loss: 0.307237044546529....\n",
      "-----------------------------------\n",
      "Training loss: 0.3537613912122431....\n",
      "Validation loss: 0.3071336118263776....\n",
      "-----------------------------------\n",
      "Training loss: 0.353619305221869....\n",
      "Validation loss: 0.3070302516221657....\n",
      "-----------------------------------\n",
      "Training loss: 0.35347738296378955....\n",
      "Validation loss: 0.30692706388036006....\n",
      "-----------------------------------\n",
      "Training loss: 0.35333561992939644....\n",
      "Validation loss: 0.3068240094455771....\n",
      "-----------------------------------\n",
      "Training loss: 0.3531939577122279....\n",
      "Validation loss: 0.3067211049247158....\n",
      "-----------------------------------\n",
      "Training loss: 0.35305239960014434....\n",
      "Validation loss: 0.3066183750495274....\n",
      "-----------------------------------\n",
      "Training loss: 0.35291097555242845....\n",
      "Validation loss: 0.30651565381495144....\n",
      "-----------------------------------\n",
      "Training loss: 0.3527697109924568....\n",
      "Validation loss: 0.30641314761102467....\n",
      "-----------------------------------\n",
      "Training loss: 0.3526286254730726....\n",
      "Validation loss: 0.3063107341646784....\n",
      "-----------------------------------\n",
      "Training loss: 0.35248769435080557....\n",
      "Validation loss: 0.30620830935767124....\n",
      "-----------------------------------\n",
      "Training loss: 0.35234689591606566....\n",
      "Validation loss: 0.30610597632293035....\n",
      "-----------------------------------\n",
      "Training loss: 0.3522063736225841....\n",
      "Validation loss: 0.30600373986205487....\n",
      "-----------------------------------\n",
      "Training loss: 0.35206607952326924....\n",
      "Validation loss: 0.30590170186544136....\n",
      "-----------------------------------\n",
      "Training loss: 0.35192592921894084....\n",
      "Validation loss: 0.3057996468366232....\n",
      "-----------------------------------\n",
      "Training loss: 0.35178594173042305....\n",
      "Validation loss: 0.30569777176945967....\n",
      "-----------------------------------\n",
      "Training loss: 0.3516461227601624....\n",
      "Validation loss: 0.3055960762665264....\n",
      "-----------------------------------\n",
      "Training loss: 0.3515064661174081....\n",
      "Validation loss: 0.3054945234542742....\n",
      "-----------------------------------\n",
      "Training loss: 0.3513669587846422....\n",
      "Validation loss: 0.30539306497707075....\n",
      "-----------------------------------\n",
      "Training loss: 0.3512275996123119....\n",
      "Validation loss: 0.30529173242118457....\n",
      "-----------------------------------\n",
      "Training loss: 0.3510883716963609....\n",
      "Validation loss: 0.3051905587658172....\n",
      "-----------------------------------\n",
      "Training loss: 0.3509492908135908....\n",
      "Validation loss: 0.3050894086093937....\n",
      "-----------------------------------\n",
      "Training loss: 0.3508103638509855....\n",
      "Validation loss: 0.30498846346702374....\n",
      "-----------------------------------\n",
      "Training loss: 0.35067159163067513....\n",
      "Validation loss: 0.3048875068979246....\n",
      "-----------------------------------\n",
      "Training loss: 0.3505329644050202....\n",
      "Validation loss: 0.30478670784072787....\n",
      "-----------------------------------\n",
      "Training loss: 0.3503945056405899....\n",
      "Validation loss: 0.30468609558542653....\n",
      "-----------------------------------\n",
      "Training loss: 0.35025621067217344....\n",
      "Validation loss: 0.30458554936102333....\n",
      "-----------------------------------\n",
      "Training loss: 0.35011807172152176....\n",
      "Validation loss: 0.3044850463229388....\n",
      "-----------------------------------\n",
      "Training loss: 0.34998008768339195....\n",
      "Validation loss: 0.30438470593665623....\n",
      "-----------------------------------\n",
      "Training loss: 0.34984226806059177....\n",
      "Validation loss: 0.3042844833246457....\n",
      "-----------------------------------\n",
      "Training loss: 0.34970460886919896....\n",
      "Validation loss: 0.30418432928474537....\n",
      "-----------------------------------\n",
      "Training loss: 0.3495670981599016....\n",
      "Validation loss: 0.3040843729011188....\n",
      "-----------------------------------\n",
      "Training loss: 0.3494297389834783....\n",
      "Validation loss: 0.3039844428923639....\n",
      "-----------------------------------\n",
      "Training loss: 0.34929254106208585....\n",
      "Validation loss: 0.30388470835423814....\n",
      "-----------------------------------\n",
      "Training loss: 0.34915550592428335....\n",
      "Validation loss: 0.30378501125962665....\n",
      "-----------------------------------\n",
      "Training loss: 0.34901861627083647....\n",
      "Validation loss: 0.30368544239661505....\n",
      "-----------------------------------\n",
      "Training loss: 0.34888186888660805....\n",
      "Validation loss: 0.30358598119840824....\n",
      "-----------------------------------\n",
      "Training loss: 0.34874526229519553....\n",
      "Validation loss: 0.3034865953404613....\n",
      "-----------------------------------\n",
      "Training loss: 0.3486088048720657....\n",
      "Validation loss: 0.3033874062947659....\n",
      "-----------------------------------\n",
      "Training loss: 0.34847248329299885....\n",
      "Validation loss: 0.30328827088198185....\n",
      "-----------------------------------\n",
      "Training loss: 0.3483363032203232....\n",
      "Validation loss: 0.3031892545424195....\n",
      "-----------------------------------\n",
      "Training loss: 0.34820021518082855....\n",
      "Validation loss: 0.3030904698806011....\n",
      "-----------------------------------\n",
      "Training loss: 0.3480642942461157....\n",
      "Validation loss: 0.3029916955592065....\n",
      "-----------------------------------\n",
      "Training loss: 0.3479285451574602....\n",
      "Validation loss: 0.30289309391120545....\n",
      "-----------------------------------\n",
      "Training loss: 0.3477930133301287....\n",
      "Validation loss: 0.3027946287810366....\n",
      "-----------------------------------\n",
      "Training loss: 0.3476576277430336....\n",
      "Validation loss: 0.3026962804046541....\n",
      "-----------------------------------\n",
      "Training loss: 0.347522379737041....\n",
      "Validation loss: 0.3025980083190831....\n",
      "-----------------------------------\n",
      "Training loss: 0.34738725995607067....\n",
      "Validation loss: 0.30249979491318646....\n",
      "-----------------------------------\n",
      "Training loss: 0.347252294969227....\n",
      "Validation loss: 0.30240172080855937....\n",
      "-----------------------------------\n",
      "Training loss: 0.3471174807113704....\n",
      "Validation loss: 0.30230382183513776....\n",
      "-----------------------------------\n",
      "Training loss: 0.34698280798713316....\n",
      "Validation loss: 0.30220599097431633....\n",
      "-----------------------------------\n",
      "Training loss: 0.3468482826868046....\n",
      "Validation loss: 0.30210824571805506....\n",
      "-----------------------------------\n",
      "Training loss: 0.3467139147812226....\n",
      "Validation loss: 0.3020106176667155....\n",
      "-----------------------------------\n",
      "Training loss: 0.3465797105107007....\n",
      "Validation loss: 0.3019131311816976....\n",
      "-----------------------------------\n",
      "Training loss: 0.3464456639155289....\n",
      "Validation loss: 0.3018157793512451....\n",
      "-----------------------------------\n",
      "Training loss: 0.346311755410087....\n",
      "Validation loss: 0.30171862270983263....\n",
      "-----------------------------------\n",
      "Training loss: 0.3461779746121033....\n",
      "Validation loss: 0.3016214542944269....\n",
      "-----------------------------------\n",
      "Training loss: 0.346044289866553....\n",
      "Validation loss: 0.30152442868584745....\n",
      "-----------------------------------\n",
      "Training loss: 0.34591065320019515....\n",
      "Validation loss: 0.3014275077112376....\n",
      "-----------------------------------\n",
      "Training loss: 0.34577714928825176....\n",
      "Validation loss: 0.30133065439085316....\n",
      "-----------------------------------\n",
      "Training loss: 0.3456437927561509....\n",
      "Validation loss: 0.3012338910770314....\n",
      "-----------------------------------\n",
      "Training loss: 0.3455105968744441....\n",
      "Validation loss: 0.30113732892008216....\n",
      "-----------------------------------\n",
      "Training loss: 0.34537757309721406....\n",
      "Validation loss: 0.3010405400933996....\n",
      "-----------------------------------\n",
      "Training loss: 0.345244701782489....\n",
      "Validation loss: 0.30094416483131514....\n",
      "-----------------------------------\n",
      "Training loss: 0.34511197130663723....\n",
      "Validation loss: 0.30084758781395066....\n",
      "-----------------------------------\n",
      "Training loss: 0.3449793899338113....\n",
      "Validation loss: 0.30075143456964476....\n",
      "-----------------------------------\n",
      "Training loss: 0.34484694868765575....\n",
      "Validation loss: 0.30065510241682103....\n",
      "-----------------------------------\n",
      "Training loss: 0.34471463161244265....\n",
      "Validation loss: 0.30055910567533173....\n",
      "-----------------------------------\n",
      "Training loss: 0.3445824283453527....\n",
      "Validation loss: 0.30046291755907995....\n",
      "-----------------------------------\n",
      "Training loss: 0.3444503718034516....\n",
      "Validation loss: 0.3003670809263846....\n",
      "-----------------------------------\n",
      "Training loss: 0.34431841518992495....\n",
      "Validation loss: 0.3002710780671859....\n",
      "-----------------------------------\n",
      "Training loss: 0.344186603980806....\n",
      "Validation loss: 0.3001755134860405....\n",
      "-----------------------------------\n",
      "Training loss: 0.3440549176148268....\n",
      "Validation loss: 0.3000800425895654....\n",
      "-----------------------------------\n",
      "Training loss: 0.3439234370151362....\n",
      "Validation loss: 0.2999843005121144....\n",
      "-----------------------------------\n",
      "Training loss: 0.343792074925517....\n",
      "Validation loss: 0.2998889638782989....\n",
      "-----------------------------------\n",
      "Training loss: 0.34366086611765845....\n",
      "Validation loss: 0.29979343013054877....\n",
      "-----------------------------------\n",
      "Training loss: 0.3435297953736332....\n",
      "Validation loss: 0.2996983374123324....\n",
      "-----------------------------------\n",
      "Training loss: 0.3433989000974121....\n",
      "Validation loss: 0.29960302890071516....\n",
      "-----------------------------------\n",
      "Training loss: 0.34326814129055305....\n",
      "Validation loss: 0.29950811318515946....\n",
      "-----------------------------------\n",
      "Training loss: 0.34313751207804855....\n",
      "Validation loss: 0.2994129870084137....\n",
      "-----------------------------------\n",
      "Training loss: 0.34300701253240284....\n",
      "Validation loss: 0.29931834237492255....\n",
      "-----------------------------------\n",
      "Training loss: 0.34287665766215547....\n",
      "Validation loss: 0.2992235738665234....\n",
      "-----------------------------------\n",
      "Training loss: 0.34274645512258145....\n",
      "Validation loss: 0.2991291235660093....\n",
      "-----------------------------------\n",
      "Training loss: 0.34261638322189764....\n",
      "Validation loss: 0.2990345110936031....\n",
      "-----------------------------------\n",
      "Training loss: 0.34248642150487185....\n",
      "Validation loss: 0.2989402749624166....\n",
      "-----------------------------------\n",
      "Training loss: 0.34235657363498834....\n",
      "Validation loss: 0.29884592399798565....\n",
      "-----------------------------------\n",
      "Training loss: 0.3422268621233407....\n",
      "Validation loss: 0.29875197140946547....\n",
      "-----------------------------------\n",
      "Training loss: 0.3420972786420799....\n",
      "Validation loss: 0.2986578117089543....\n",
      "-----------------------------------\n",
      "Training loss: 0.34196784922185997....\n",
      "Validation loss: 0.298564047208987....\n",
      "-----------------------------------\n",
      "Training loss: 0.3418385534786332....\n",
      "Validation loss: 0.298470106678235....\n",
      "-----------------------------------\n",
      "Training loss: 0.3417094393481754....\n",
      "Validation loss: 0.2983765636255593....\n",
      "-----------------------------------\n",
      "Training loss: 0.3415804727437666....\n",
      "Validation loss: 0.2982827690811415....\n",
      "-----------------------------------\n",
      "Training loss: 0.3414517076432048....\n",
      "Validation loss: 0.29818938658268096....\n",
      "-----------------------------------\n",
      "Training loss: 0.3413230657116704....\n",
      "Validation loss: 0.29809587131303383....\n",
      "-----------------------------------\n",
      "Training loss: 0.3411945472039623....\n",
      "Validation loss: 0.2980027096343102....\n",
      "-----------------------------------\n",
      "Training loss: 0.34106618783459747....\n",
      "Validation loss: 0.2979093657527541....\n",
      "-----------------------------------\n",
      "Training loss: 0.3409379325144547....\n",
      "Validation loss: 0.29781648203986383....\n",
      "-----------------------------------\n",
      "Training loss: 0.3408097903891627....\n",
      "Validation loss: 0.29772335533381167....\n",
      "-----------------------------------\n",
      "Training loss: 0.3406817444188392....\n",
      "Validation loss: 0.297630622363046....\n",
      "-----------------------------------\n",
      "Training loss: 0.34055382547930907....\n",
      "Validation loss: 0.29753777032783124....\n",
      "-----------------------------------\n",
      "Training loss: 0.3404260392909306....\n",
      "Validation loss: 0.29744521805337726....\n",
      "-----------------------------------\n",
      "Training loss: 0.34029835447304074....\n",
      "Validation loss: 0.29735248265693687....\n",
      "-----------------------------------\n",
      "Training loss: 0.340170801748618....\n",
      "Validation loss: 0.29725986827681605....\n",
      "-----------------------------------\n",
      "Training loss: 0.340043411107338....\n",
      "Validation loss: 0.2971676046561811....\n",
      "-----------------------------------\n",
      "Training loss: 0.33991614514017754....\n",
      "Validation loss: 0.29707520462211945....\n",
      "-----------------------------------\n",
      "Training loss: 0.33978901453560734....\n",
      "Validation loss: 0.296983177292806....\n",
      "-----------------------------------\n",
      "Training loss: 0.33966203013247503....\n",
      "Validation loss: 0.296890977021086....\n",
      "-----------------------------------\n",
      "Training loss: 0.33953510771519374....\n",
      "Validation loss: 0.2967989164040074....\n",
      "-----------------------------------\n",
      "Training loss: 0.3394082843662134....\n",
      "Validation loss: 0.29670716270941905....\n",
      "-----------------------------------\n",
      "Training loss: 0.33928155072839616....\n",
      "Validation loss: 0.2966152612066069....\n",
      "-----------------------------------\n",
      "Training loss: 0.33915494678429614....\n",
      "Validation loss: 0.2965237044060242....\n",
      "-----------------------------------\n",
      "Training loss: 0.33902848176076167....\n",
      "Validation loss: 0.2964320516126355....\n",
      "-----------------------------------\n",
      "Training loss: 0.3389020800829002....\n",
      "Validation loss: 0.29634074160999757....\n",
      "-----------------------------------\n",
      "Training loss: 0.33877580073727215....\n",
      "Validation loss: 0.2962491980797895....\n",
      "-----------------------------------\n",
      "Training loss: 0.3386496606603173....\n",
      "Validation loss: 0.29615772442426525....\n",
      "-----------------------------------\n",
      "Training loss: 0.33852362553566856....\n",
      "Validation loss: 0.29606669144062264....\n",
      "-----------------------------------\n",
      "Training loss: 0.33839769195902525....\n",
      "Validation loss: 0.295975419459422....\n",
      "-----------------------------------\n",
      "Training loss: 0.3382718504825677....\n",
      "Validation loss: 0.2958845660738048....\n",
      "-----------------------------------\n",
      "Training loss: 0.3381461057221375....\n",
      "Validation loss: 0.2957934781848965....\n",
      "-----------------------------------\n",
      "Training loss: 0.33802048588581857....\n",
      "Validation loss: 0.2957027726701481....\n",
      "-----------------------------------\n",
      "Training loss: 0.33789497387604606....\n",
      "Validation loss: 0.2956118774229907....\n",
      "-----------------------------------\n",
      "Training loss: 0.33776954956089306....\n",
      "Validation loss: 0.2955213771648147....\n",
      "-----------------------------------\n",
      "Training loss: 0.33764426025207367....\n",
      "Validation loss: 0.2954306509992597....\n",
      "-----------------------------------\n",
      "Training loss: 0.3375190791038681....\n",
      "Validation loss: 0.2953403195397731....\n",
      "-----------------------------------\n",
      "Training loss: 0.33739403131522333....\n",
      "Validation loss: 0.295249822198662....\n",
      "-----------------------------------\n",
      "Training loss: 0.3372690971002071....\n",
      "Validation loss: 0.2951596788120062....\n",
      "-----------------------------------\n",
      "Training loss: 0.3371443172453676....\n",
      "Validation loss: 0.29506933871802743....\n",
      "-----------------------------------\n",
      "Training loss: 0.337019649834692....\n",
      "Validation loss: 0.2949791080514428....\n",
      "-----------------------------------\n",
      "Training loss: 0.3368951458349485....\n",
      "Validation loss: 0.29488924896572355....\n",
      "-----------------------------------\n",
      "Training loss: 0.33677077960326846....\n",
      "Validation loss: 0.2947991669339723....\n",
      "-----------------------------------\n",
      "Training loss: 0.3366465764776458....\n",
      "Validation loss: 0.29470949014671166....\n",
      "-----------------------------------\n",
      "Training loss: 0.33652247242054534....\n",
      "Validation loss: 0.29461960383742347....\n",
      "-----------------------------------\n",
      "Training loss: 0.33639843885494036....\n",
      "Validation loss: 0.29453010337913343....\n",
      "-----------------------------------\n",
      "Training loss: 0.33627451018915727....\n",
      "Validation loss: 0.294440444219347....\n",
      "-----------------------------------\n",
      "Training loss: 0.33615065601602806....\n",
      "Validation loss: 0.2943512090151122....\n",
      "-----------------------------------\n",
      "Training loss: 0.33602691500674087....\n",
      "Validation loss: 0.2942617865717077....\n",
      "-----------------------------------\n",
      "Training loss: 0.33590329711358935....\n",
      "Validation loss: 0.29417277661022445....\n",
      "-----------------------------------\n",
      "Training loss: 0.3357798122709168....\n",
      "Validation loss: 0.2940835942981382....\n",
      "-----------------------------------\n",
      "Training loss: 0.33565644091918023....\n",
      "Validation loss: 0.2939948039621975....\n",
      "-----------------------------------\n",
      "Training loss: 0.33553320984653956....\n",
      "Validation loss: 0.29390582866267173....\n",
      "-----------------------------------\n",
      "Training loss: 0.335410089833219....\n",
      "Validation loss: 0.29381691412610195....\n",
      "-----------------------------------\n",
      "Training loss: 0.33528707586068135....\n",
      "Validation loss: 0.2937283932202726....\n",
      "-----------------------------------\n",
      "Training loss: 0.3351641672100429....\n",
      "Validation loss: 0.29363967586956546....\n",
      "-----------------------------------\n",
      "Training loss: 0.33504140057644466....\n",
      "Validation loss: 0.29355131328244044....\n",
      "-----------------------------------\n",
      "Training loss: 0.3349187942607671....\n",
      "Validation loss: 0.29346274705002273....\n",
      "-----------------------------------\n",
      "Training loss: 0.334796354325278....\n",
      "Validation loss: 0.29337456897228675....\n",
      "-----------------------------------\n",
      "Training loss: 0.3346740493627955....\n",
      "Validation loss: 0.29328628069171064....\n",
      "-----------------------------------\n",
      "Training loss: 0.3345518918961694....\n",
      "Validation loss: 0.29319834813788864....\n",
      "-----------------------------------\n",
      "Training loss: 0.3344298520274815....\n",
      "Validation loss: 0.2931102206234988....\n",
      "-----------------------------------\n",
      "Training loss: 0.3343079469835309....\n",
      "Validation loss: 0.2930224974760008....\n",
      "-----------------------------------\n",
      "Training loss: 0.33418619799011506....\n",
      "Validation loss: 0.29293456251993627....\n",
      "-----------------------------------\n",
      "Training loss: 0.3340645765518957....\n",
      "Validation loss: 0.29284673195456296....\n",
      "-----------------------------------\n",
      "Training loss: 0.33394309643955095....\n",
      "Validation loss: 0.2927593222660313....\n",
      "-----------------------------------\n",
      "Training loss: 0.3338217054225326....\n",
      "Validation loss: 0.2926716854394387....\n",
      "-----------------------------------\n",
      "Training loss: 0.33370044655441916....\n",
      "Validation loss: 0.2925845008361669....\n",
      "-----------------------------------\n",
      "Training loss: 0.33357931402647967....\n",
      "Validation loss: 0.29249711349839713....\n",
      "-----------------------------------\n",
      "Training loss: 0.33345832341563575....\n",
      "Validation loss: 0.29241015131092024....\n",
      "-----------------------------------\n",
      "Training loss: 0.33333745499610257....\n",
      "Validation loss: 0.292322988147....\n",
      "-----------------------------------\n",
      "Training loss: 0.33321671098219374....\n",
      "Validation loss: 0.2922361811813416....\n",
      "-----------------------------------\n",
      "Training loss: 0.33309610573431686....\n",
      "Validation loss: 0.29214918238999643....\n",
      "-----------------------------------\n",
      "Training loss: 0.33297559198724275....\n",
      "Validation loss: 0.29206257727364215....\n",
      "-----------------------------------\n",
      "Training loss: 0.3328552395680439....\n",
      "Validation loss: 0.29197570670584283....\n",
      "-----------------------------------\n",
      "Training loss: 0.33273502242374386....\n",
      "Validation loss: 0.2918889539635042....\n",
      "-----------------------------------\n",
      "Training loss: 0.33261495873643254....\n",
      "Validation loss: 0.29180257291658335....\n",
      "-----------------------------------\n",
      "Training loss: 0.33249503313701506....\n",
      "Validation loss: 0.2917160575603265....\n",
      "-----------------------------------\n",
      "Training loss: 0.33237524925286716....\n",
      "Validation loss: 0.2916298604325383....\n",
      "-----------------------------------\n",
      "Training loss: 0.33225549595118076....\n",
      "Validation loss: 0.2915434272334855....\n",
      "-----------------------------------\n",
      "Training loss: 0.33213585254740186....\n",
      "Validation loss: 0.29145744047087074....\n",
      "-----------------------------------\n",
      "Training loss: 0.33201631713376734....\n",
      "Validation loss: 0.2913712645581992....\n",
      "-----------------------------------\n",
      "Training loss: 0.33189690274949313....\n",
      "Validation loss: 0.2912854441401145....\n",
      "-----------------------------------\n",
      "Training loss: 0.3317776716581378....\n",
      "Validation loss: 0.29119948848315363....\n",
      "-----------------------------------\n",
      "Training loss: 0.3316585712220561....\n",
      "Validation loss: 0.29111360576581524....\n",
      "-----------------------------------\n",
      "Training loss: 0.33153959298840474....\n",
      "Validation loss: 0.2910281518310371....\n",
      "-----------------------------------\n",
      "Training loss: 0.3314207002072031....\n",
      "Validation loss: 0.29094250293640483....\n",
      "-----------------------------------\n",
      "Training loss: 0.3313019436129566....\n",
      "Validation loss: 0.29085720853917396....\n",
      "-----------------------------------\n",
      "Training loss: 0.33118329763628457....\n",
      "Validation loss: 0.29077175699638264....\n",
      "-----------------------------------\n",
      "Training loss: 0.3310647557991064....\n",
      "Validation loss: 0.29068670471435953....\n",
      "-----------------------------------\n",
      "Training loss: 0.33094634621928437....\n",
      "Validation loss: 0.2906014469824145....\n",
      "-----------------------------------\n",
      "Training loss: 0.33082805380869995....\n",
      "Validation loss: 0.2905165926241718....\n",
      "-----------------------------------\n",
      "Training loss: 0.33070989691392705....\n",
      "Validation loss: 0.29043153954349993....\n",
      "-----------------------------------\n",
      "Training loss: 0.3305918369157848....\n",
      "Validation loss: 0.2903466323335036....\n",
      "-----------------------------------\n",
      "Training loss: 0.3304738986805187....\n",
      "Validation loss: 0.29026205576347996....\n",
      "-----------------------------------\n",
      "Training loss: 0.3303560707886519....\n",
      "Validation loss: 0.2901772757294313....\n",
      "-----------------------------------\n",
      "Training loss: 0.33023834613381853....\n",
      "Validation loss: 0.29009289909348696....\n",
      "-----------------------------------\n",
      "Training loss: 0.33012075372909316....\n",
      "Validation loss: 0.2900082833246961....\n",
      "-----------------------------------\n",
      "Training loss: 0.3300032730604238....\n",
      "Validation loss: 0.28992381562301134....\n",
      "-----------------------------------\n",
      "Training loss: 0.3298859122867535....\n",
      "Validation loss: 0.2898397347933013....\n",
      "-----------------------------------\n",
      "Training loss: 0.32976868218727656....\n",
      "Validation loss: 0.28975539365947006....\n",
      "-----------------------------------\n",
      "Training loss: 0.32965160085722434....\n",
      "Validation loss: 0.28967148331374054....\n",
      "-----------------------------------\n",
      "Training loss: 0.3295346479536157....\n",
      "Validation loss: 0.28958735162839383....\n",
      "-----------------------------------\n",
      "Training loss: 0.3294177909392772....\n",
      "Validation loss: 0.2895033897867694....\n",
      "-----------------------------------\n",
      "Training loss: 0.3293010579379428....\n",
      "Validation loss: 0.2894198002836284....\n",
      "-----------------------------------\n",
      "Training loss: 0.32918444936748414....\n",
      "Validation loss: 0.28933595752463886....\n",
      "-----------------------------------\n",
      "Training loss: 0.32906794150978647....\n",
      "Validation loss: 0.2892525324803442....\n",
      "-----------------------------------\n",
      "Training loss: 0.32895152057015054....\n",
      "Validation loss: 0.28916887691082094....\n",
      "-----------------------------------\n",
      "Training loss: 0.3288352032055116....\n",
      "Validation loss: 0.28908532876555737....\n",
      "-----------------------------------\n",
      "Training loss: 0.32871900954924616....\n",
      "Validation loss: 0.28900218820341234....\n",
      "-----------------------------------\n",
      "Training loss: 0.32860294229509174....\n",
      "Validation loss: 0.28891881804625896....\n",
      "-----------------------------------\n",
      "Training loss: 0.3284869885364558....\n",
      "Validation loss: 0.2888355502994202....\n",
      "-----------------------------------\n",
      "Training loss: 0.32837116871023897....\n",
      "Validation loss: 0.2887526290940664....\n",
      "-----------------------------------\n",
      "Training loss: 0.32825545089681807....\n",
      "Validation loss: 0.28866950784797013....\n",
      "-----------------------------------\n",
      "Training loss: 0.32813984845354033....\n",
      "Validation loss: 0.288586786054366....\n",
      "-----------------------------------\n",
      "Training loss: 0.3280243571760004....\n",
      "Validation loss: 0.28850383884302017....\n",
      "-----------------------------------\n",
      "Training loss: 0.32790896677104947....\n",
      "Validation loss: 0.288421003609416....\n",
      "-----------------------------------\n",
      "Training loss: 0.3277937035747589....\n",
      "Validation loss: 0.2883385207921719....\n",
      "-----------------------------------\n",
      "Training loss: 0.327678538194243....\n",
      "Validation loss: 0.28825586086150545....\n",
      "-----------------------------------\n",
      "Training loss: 0.32756349266477697....\n",
      "Validation loss: 0.2881735932217785....\n",
      "-----------------------------------\n",
      "Training loss: 0.32744857525720483....\n",
      "Validation loss: 0.28809111148791045....\n",
      "-----------------------------------\n",
      "Training loss: 0.3273337658893365....\n",
      "Validation loss: 0.2880087340125506....\n",
      "-----------------------------------\n",
      "Training loss: 0.3272190630556618....\n",
      "Validation loss: 0.28792675012530294....\n",
      "-----------------------------------\n",
      "Training loss: 0.32710446910780105....\n",
      "Validation loss: 0.2878445138579872....\n",
      "-----------------------------------\n",
      "Training loss: 0.3269899728694157....\n",
      "Validation loss: 0.2877627305189316....\n",
      "-----------------------------------\n",
      "Training loss: 0.3268755046449261....\n",
      "Validation loss: 0.28768069803554985....\n",
      "-----------------------------------\n",
      "Training loss: 0.3267611473709544....\n",
      "Validation loss: 0.2875987704517352....\n",
      "-----------------------------------\n",
      "Training loss: 0.326646901402524....\n",
      "Validation loss: 0.2875171777852844....\n",
      "-----------------------------------\n",
      "Training loss: 0.3265327745799303....\n",
      "Validation loss: 0.2874353930322122....\n",
      "-----------------------------------\n",
      "Training loss: 0.32641875121164526....\n",
      "Validation loss: 0.2873537188515008....\n",
      "-----------------------------------\n",
      "Training loss: 0.32630484792784675....\n",
      "Validation loss: 0.287272341187942....\n",
      "-----------------------------------\n",
      "Training loss: 0.3261910538374427....\n",
      "Validation loss: 0.2871908030746186....\n",
      "-----------------------------------\n",
      "Training loss: 0.32607740360515813....\n",
      "Validation loss: 0.28710964264912103....\n",
      "-----------------------------------\n",
      "Training loss: 0.32596386322625476....\n",
      "Validation loss: 0.2870283243859058....\n",
      "-----------------------------------\n",
      "Training loss: 0.3258503965667126....\n",
      "Validation loss: 0.286947032542562....\n",
      "-----------------------------------\n",
      "Training loss: 0.3257370358745014....\n",
      "Validation loss: 0.2868661592639805....\n",
      "-----------------------------------\n",
      "Training loss: 0.3256237836546533....\n",
      "Validation loss: 0.28678502368253905....\n",
      "-----------------------------------\n",
      "Training loss: 0.3255106189099189....\n",
      "Validation loss: 0.286704046160784....\n",
      "-----------------------------------\n",
      "Training loss: 0.325397603772113....\n",
      "Validation loss: 0.2866234297955734....\n",
      "-----------------------------------\n",
      "Training loss: 0.32528469452766823....\n",
      "Validation loss: 0.2865425627650839....\n",
      "-----------------------------------\n",
      "Training loss: 0.3251718887221331....\n",
      "Validation loss: 0.2864618358989717....\n",
      "-----------------------------------\n",
      "Training loss: 0.32505923289557875....\n",
      "Validation loss: 0.2863814129067522....\n",
      "-----------------------------------\n",
      "Training loss: 0.3249466774052976....\n",
      "Validation loss: 0.2863008518191945....\n",
      "-----------------------------------\n",
      "Training loss: 0.32483422305778903....\n",
      "Validation loss: 0.2862206244008622....\n",
      "-----------------------------------\n",
      "Training loss: 0.3247218887459314....\n",
      "Validation loss: 0.2861401897267689....\n",
      "-----------------------------------\n",
      "Training loss: 0.3246096597705747....\n",
      "Validation loss: 0.286059789477347....\n",
      "-----------------------------------\n",
      "Training loss: 0.32449751273661687....\n",
      "Validation loss: 0.28597977823384274....\n",
      "-----------------------------------\n",
      "Training loss: 0.324385445665176....\n",
      "Validation loss: 0.28589953370544713....\n",
      "-----------------------------------\n",
      "Training loss: 0.32427349178464043....\n",
      "Validation loss: 0.2858196943569391....\n",
      "-----------------------------------\n",
      "Training loss: 0.3241616311961161....\n",
      "Validation loss: 0.2857396486582297....\n",
      "-----------------------------------\n",
      "Training loss: 0.324049861698495....\n",
      "Validation loss: 0.285660025073192....\n",
      "-----------------------------------\n",
      "Training loss: 0.32393817327848007....\n",
      "Validation loss: 0.285580200584746....\n",
      "-----------------------------------\n",
      "Training loss: 0.32382657854907737....\n",
      "Validation loss: 0.2855007470364946....\n",
      "-----------------------------------\n",
      "Training loss: 0.32371509693321115....\n",
      "Validation loss: 0.2854209832396038....\n",
      "-----------------------------------\n",
      "Training loss: 0.32360367551932495....\n",
      "Validation loss: 0.2853413509564104....\n",
      "-----------------------------------\n",
      "Training loss: 0.3234923769985437....\n",
      "Validation loss: 0.285262024035457....\n",
      "-----------------------------------\n",
      "Training loss: 0.3233812055245118....\n",
      "Validation loss: 0.2851825592529237....\n",
      "-----------------------------------\n",
      "Training loss: 0.32327016968052863....\n",
      "Validation loss: 0.28510344891030437....\n",
      "-----------------------------------\n",
      "Training loss: 0.32315923676053615....\n",
      "Validation loss: 0.28502408414672586....\n",
      "-----------------------------------\n",
      "Training loss: 0.32304842023492303....\n",
      "Validation loss: 0.2849451184178585....\n",
      "-----------------------------------\n",
      "Training loss: 0.3229377210522823....\n",
      "Validation loss: 0.28486593443204905....\n",
      "-----------------------------------\n",
      "Training loss: 0.322827109261103....\n",
      "Validation loss: 0.28478710674561836....\n",
      "-----------------------------------\n",
      "Training loss: 0.32271659530930596....\n",
      "Validation loss: 0.2847080474880554....\n",
      "-----------------------------------\n",
      "Training loss: 0.32260615681677257....\n",
      "Validation loss: 0.28462913301431997....\n",
      "-----------------------------------\n",
      "Training loss: 0.3224958373589759....\n",
      "Validation loss: 0.28455051463305625....\n",
      "-----------------------------------\n",
      "Training loss: 0.3223856044306034....\n",
      "Validation loss: 0.284471763230784....\n",
      "-----------------------------------\n",
      "Training loss: 0.3222754922379741....\n",
      "Validation loss: 0.28439330427305876....\n",
      "-----------------------------------\n",
      "Training loss: 0.32216548410271756....\n",
      "Validation loss: 0.2843146923930394....\n",
      "-----------------------------------\n",
      "Training loss: 0.3220555714898537....\n",
      "Validation loss: 0.2842363878254964....\n",
      "-----------------------------------\n",
      "Training loss: 0.3219457538285011....\n",
      "Validation loss: 0.2841579546416126....\n",
      "-----------------------------------\n",
      "Training loss: 0.32183602693921437....\n",
      "Validation loss: 0.2840798589605199....\n",
      "-----------------------------------\n",
      "Training loss: 0.3217264096824272....\n",
      "Validation loss: 0.28400154994268295....\n",
      "-----------------------------------\n",
      "Training loss: 0.32161686197563166....\n",
      "Validation loss: 0.28392337533437595....\n",
      "-----------------------------------\n",
      "Training loss: 0.32150741467259303....\n",
      "Validation loss: 0.28384552373025057....\n",
      "-----------------------------------\n",
      "Training loss: 0.32139810606160657....\n",
      "Validation loss: 0.28376751696192754....\n",
      "-----------------------------------\n",
      "Training loss: 0.3212889105213576....\n",
      "Validation loss: 0.2836898625373169....\n",
      "-----------------------------------\n",
      "Training loss: 0.32117980972117904....\n",
      "Validation loss: 0.28361200703379846....\n",
      "-----------------------------------\n",
      "Training loss: 0.3210708141508741....\n",
      "Validation loss: 0.2835344701528983....\n",
      "-----------------------------------\n",
      "Training loss: 0.32096197085394246....\n",
      "Validation loss: 0.2834567663984652....\n",
      "-----------------------------------\n",
      "Training loss: 0.3208532251611823....\n",
      "Validation loss: 0.28337916576470407....\n",
      "-----------------------------------\n",
      "Training loss: 0.32074459853201254....\n",
      "Validation loss: 0.283301921769795....\n",
      "-----------------------------------\n",
      "Training loss: 0.3206360682349704....\n",
      "Validation loss: 0.2832244704252711....\n",
      "-----------------------------------\n",
      "Training loss: 0.3205276458637763....\n",
      "Validation loss: 0.2831473104125953....\n",
      "-----------------------------------\n",
      "Training loss: 0.3204193230073405....\n",
      "Validation loss: 0.28306998425782437....\n",
      "-----------------------------------\n",
      "Training loss: 0.32031110191614504....\n",
      "Validation loss: 0.28299305540037867....\n",
      "-----------------------------------\n",
      "Training loss: 0.3202029909526271....\n",
      "Validation loss: 0.28291589921945637....\n",
      "-----------------------------------\n",
      "Training loss: 0.32009497078109805....\n",
      "Validation loss: 0.28283881265442046....\n",
      "-----------------------------------\n",
      "Training loss: 0.31998706812685146....\n",
      "Validation loss: 0.2827620719541391....\n",
      "-----------------------------------\n",
      "Training loss: 0.3198792361803298....\n",
      "Validation loss: 0.28268514679119405....\n",
      "-----------------------------------\n",
      "Training loss: 0.3197715182774073....\n",
      "Validation loss: 0.2826085847449715....\n",
      "-----------------------------------\n",
      "Training loss: 0.3196638910944522....\n",
      "Validation loss: 0.2825318428491421....\n",
      "-----------------------------------\n",
      "Training loss: 0.31955636892506584....\n",
      "Validation loss: 0.2824553648123081....\n",
      "-----------------------------------\n",
      "Training loss: 0.3194489440654675....\n",
      "Validation loss: 0.28237875437974136....\n",
      "-----------------------------------\n",
      "Training loss: 0.319341612914664....\n",
      "Validation loss: 0.2823024955203668....\n",
      "-----------------------------------\n",
      "Training loss: 0.3192343855948404....\n",
      "Validation loss: 0.2822260218231222....\n",
      "-----------------------------------\n",
      "Training loss: 0.31912723849911095....\n",
      "Validation loss: 0.2821496555364132....\n",
      "-----------------------------------\n",
      "Training loss: 0.3190202025120831....\n",
      "Validation loss: 0.2820735984370083....\n",
      "-----------------------------------\n",
      "Training loss: 0.3189132375269316....\n",
      "Validation loss: 0.28199740978323784....\n",
      "-----------------------------------\n",
      "Training loss: 0.318806359867817....\n",
      "Validation loss: 0.28192153338073006....\n",
      "-----------------------------------\n",
      "Training loss: 0.3186995724668916....\n",
      "Validation loss: 0.28184543334129203....\n",
      "-----------------------------------\n",
      "Training loss: 0.31859288706347216....\n",
      "Validation loss: 0.2817697029219835....\n",
      "-----------------------------------\n",
      "Training loss: 0.3184863138279053....\n",
      "Validation loss: 0.281693660346689....\n",
      "-----------------------------------\n",
      "Training loss: 0.31837984214948084....\n",
      "Validation loss: 0.28161804920980443....\n",
      "-----------------------------------\n",
      "Training loss: 0.31827348348296136....\n",
      "Validation loss: 0.2815422182502483....\n",
      "-----------------------------------\n",
      "Training loss: 0.318167218531219....\n",
      "Validation loss: 0.28146647479224207....\n",
      "-----------------------------------\n",
      "Training loss: 0.3180610702285964....\n",
      "Validation loss: 0.28139101246262743....\n",
      "-----------------------------------\n",
      "Training loss: 0.3179550080250297....\n",
      "Validation loss: 0.2813154230757714....\n",
      "-----------------------------------\n",
      "Training loss: 0.3178490337321341....\n",
      "Validation loss: 0.2812401306083102....\n",
      "-----------------------------------\n",
      "Training loss: 0.31774313183454583....\n",
      "Validation loss: 0.28116465534960206....\n",
      "-----------------------------------\n",
      "Training loss: 0.3176373245292804....\n",
      "Validation loss: 0.28108951856717157....\n",
      "-----------------------------------\n",
      "Training loss: 0.31753163597795425....\n",
      "Validation loss: 0.28101418544731754....\n",
      "-----------------------------------\n",
      "Training loss: 0.3174260220143199....\n",
      "Validation loss: 0.28093888770010195....\n",
      "-----------------------------------\n",
      "Training loss: 0.31732052166649266....\n",
      "Validation loss: 0.2808640286228367....\n",
      "-----------------------------------\n",
      "Training loss: 0.3172151065502013....\n",
      "Validation loss: 0.2807889461118285....\n",
      "-----------------------------------\n",
      "Training loss: 0.3171097833560127....\n",
      "Validation loss: 0.2807141789916561....\n",
      "-----------------------------------\n",
      "Training loss: 0.3170045668760033....\n",
      "Validation loss: 0.28063923105003624....\n",
      "-----------------------------------\n",
      "Training loss: 0.3168994320668529....\n",
      "Validation loss: 0.2805643449986065....\n",
      "-----------------------------------\n",
      "Training loss: 0.31679437328647175....\n",
      "Validation loss: 0.2804895098004992....\n",
      "-----------------------------------\n",
      "Training loss: 0.31668941071549683....\n",
      "Validation loss: 0.28041506407145367....\n",
      "-----------------------------------\n",
      "Training loss: 0.3165845090165963....\n",
      "Validation loss: 0.2803404396618433....\n",
      "-----------------------------------\n",
      "Training loss: 0.31647970960600436....\n",
      "Validation loss: 0.28026590663864137....\n",
      "-----------------------------------\n",
      "Training loss: 0.316375034707385....\n",
      "Validation loss: 0.28019172438598827....\n",
      "-----------------------------------\n",
      "Training loss: 0.3162704399383422....\n",
      "Validation loss: 0.2801173753156216....\n",
      "-----------------------------------\n",
      "Training loss: 0.31616593620838296....\n",
      "Validation loss: 0.2800430400296793....\n",
      "-----------------------------------\n",
      "Training loss: 0.31606156093624904....\n",
      "Validation loss: 0.27996911399713087....\n",
      "-----------------------------------\n",
      "Training loss: 0.3159572838333314....\n",
      "Validation loss: 0.2798950006799139....\n",
      "-----------------------------------\n",
      "Training loss: 0.3158530993103609....\n",
      "Validation loss: 0.2798212103573421....\n",
      "-----------------------------------\n",
      "Training loss: 0.31574901055104354....\n",
      "Validation loss: 0.27974721840530953....\n",
      "-----------------------------------\n",
      "Training loss: 0.31564498432289845....\n",
      "Validation loss: 0.27967333787553006....\n",
      "-----------------------------------\n",
      "Training loss: 0.3155410555736989....\n",
      "Validation loss: 0.2795998086875497....\n",
      "-----------------------------------\n",
      "Training loss: 0.31543723825590697....\n",
      "Validation loss: 0.2795260299891151....\n",
      "-----------------------------------\n",
      "Training loss: 0.31533349469962946....\n",
      "Validation loss: 0.27945238520000604....\n",
      "-----------------------------------\n",
      "Training loss: 0.315229847210751....\n",
      "Validation loss: 0.27937903503465755....\n",
      "-----------------------------------\n",
      "Training loss: 0.31512630749807286....\n",
      "Validation loss: 0.27930555873158425....\n",
      "-----------------------------------\n",
      "Training loss: 0.3150228651932245....\n",
      "Validation loss: 0.2792321866272533....\n",
      "-----------------------------------\n",
      "Training loss: 0.3149195381905564....\n",
      "Validation loss: 0.27915911035567453....\n",
      "-----------------------------------\n",
      "Training loss: 0.314816315715844....\n",
      "Validation loss: 0.2790858612170723....\n",
      "-----------------------------------\n",
      "Training loss: 0.3147131687442924....\n",
      "Validation loss: 0.2790125995417701....\n",
      "-----------------------------------\n",
      "Training loss: 0.3146101027763462....\n",
      "Validation loss: 0.27893975285649125....\n",
      "-----------------------------------\n",
      "Training loss: 0.31450709507386765....\n",
      "Validation loss: 0.2788667024275883....\n",
      "-----------------------------------\n",
      "Training loss: 0.31440418098438166....\n",
      "Validation loss: 0.27879370506535145....\n",
      "-----------------------------------\n",
      "Training loss: 0.3143013647226244....\n",
      "Validation loss: 0.27872105851564816....\n",
      "-----------------------------------\n",
      "Training loss: 0.31419862448868663....\n",
      "Validation loss: 0.27864817224692373....\n",
      "-----------------------------------\n",
      "Training loss: 0.314095940461124....\n",
      "Validation loss: 0.2785753405819838....\n",
      "-----------------------------------\n",
      "Training loss: 0.3139933414222434....\n",
      "Validation loss: 0.27850288869940015....\n",
      "-----------------------------------\n",
      "Training loss: 0.31389085512769244....\n",
      "Validation loss: 0.27843024609225175....\n",
      "-----------------------------------\n",
      "Training loss: 0.31378845640172986....\n",
      "Validation loss: 0.27835765824337716....\n",
      "-----------------------------------\n",
      "Training loss: 0.3136861476413947....\n",
      "Validation loss: 0.2782851265966844....\n",
      "-----------------------------------\n",
      "Training loss: 0.31358394808903794....\n",
      "Validation loss: 0.2782129529356135....\n",
      "-----------------------------------\n",
      "Training loss: 0.31348183392807666....\n",
      "Validation loss: 0.2781405118256774....\n",
      "-----------------------------------\n",
      "Training loss: 0.3133798079633942....\n",
      "Validation loss: 0.27806818854620785....\n",
      "-----------------------------------\n",
      "Training loss: 0.31327787869283286....\n",
      "Validation loss: 0.2779962503274019....\n",
      "-----------------------------------\n",
      "Training loss: 0.31317605422918965....\n",
      "Validation loss: 0.27792402657959087....\n",
      "-----------------------------------\n",
      "Training loss: 0.31307429516719265....\n",
      "Validation loss: 0.2778519264867596....\n",
      "-----------------------------------\n",
      "Training loss: 0.3129726142910848....\n",
      "Validation loss: 0.27777987992814196....\n",
      "-----------------------------------\n",
      "Training loss: 0.31287100900852377....\n",
      "Validation loss: 0.27770822198111....\n",
      "-----------------------------------\n",
      "Training loss: 0.3127694927807555....\n",
      "Validation loss: 0.27763634502694073....\n",
      "-----------------------------------\n",
      "Training loss: 0.31266805822400195....\n",
      "Validation loss: 0.2775645997170528....\n",
      "-----------------------------------\n",
      "Training loss: 0.3125667054787042....\n",
      "Validation loss: 0.27749306975115934....\n",
      "-----------------------------------\n",
      "Training loss: 0.31246545966613326....\n",
      "Validation loss: 0.27742145619439657....\n",
      "-----------------------------------\n",
      "Training loss: 0.31236428859272053....\n",
      "Validation loss: 0.2773499460218307....\n",
      "-----------------------------------\n",
      "Training loss: 0.31226320529617774....\n",
      "Validation loss: 0.27727846683437196....\n",
      "-----------------------------------\n",
      "Training loss: 0.31216220939564776....\n",
      "Validation loss: 0.27720731200215837....\n",
      "-----------------------------------\n",
      "Training loss: 0.31206127589364835....\n",
      "Validation loss: 0.277135992732635....\n",
      "-----------------------------------\n",
      "Training loss: 0.3119604287044274....\n",
      "Validation loss: 0.2770647613632439....\n",
      "-----------------------------------\n",
      "Training loss: 0.3118596794279227....\n",
      "Validation loss: 0.27699384762603685....\n",
      "-----------------------------------\n",
      "Training loss: 0.3117590101454199....\n",
      "Validation loss: 0.27692268263127073....\n",
      "-----------------------------------\n",
      "Training loss: 0.3116584369521555....\n",
      "Validation loss: 0.27685163811786495....\n",
      "-----------------------------------\n",
      "Training loss: 0.3115579879350921....\n",
      "Validation loss: 0.2767808624655939....\n",
      "-----------------------------------\n",
      "Training loss: 0.31145762404912397....\n",
      "Validation loss: 0.2767098941256793....\n",
      "-----------------------------------\n",
      "Training loss: 0.3113573384317682....\n",
      "Validation loss: 0.2766390285596209....\n",
      "-----------------------------------\n",
      "Training loss: 0.3112571400291496....\n",
      "Validation loss: 0.2765682154437283....\n",
      "-----------------------------------\n",
      "Training loss: 0.3111569988992621....\n",
      "Validation loss: 0.27649766546886595....\n",
      "-----------------------------------\n",
      "Training loss: 0.31105689895489114....\n",
      "Validation loss: 0.276426948001583....\n",
      "-----------------------------------\n",
      "Training loss: 0.31095688173417346....\n",
      "Validation loss: 0.2763563096053071....\n",
      "-----------------------------------\n",
      "Training loss: 0.3108569600089887....\n",
      "Validation loss: 0.27628599368095036....\n",
      "-----------------------------------\n",
      "Training loss: 0.3107571385777056....\n",
      "Validation loss: 0.2762154789962547....\n",
      "-----------------------------------\n",
      "Training loss: 0.31065738298903267....\n",
      "Validation loss: 0.27614503363278914....\n",
      "-----------------------------------\n",
      "Training loss: 0.31055769868070554....\n",
      "Validation loss: 0.2760746905706196....\n",
      "-----------------------------------\n",
      "Training loss: 0.3104581098920156....\n",
      "Validation loss: 0.2760046470183785....\n",
      "-----------------------------------\n",
      "Training loss: 0.31035860486013267....\n",
      "Validation loss: 0.27593444378269044....\n",
      "-----------------------------------\n",
      "Training loss: 0.31025917157509364....\n",
      "Validation loss: 0.27586428050404505....\n",
      "-----------------------------------\n",
      "Training loss: 0.3101598168744998....\n",
      "Validation loss: 0.2757944652456169....\n",
      "-----------------------------------\n",
      "Training loss: 0.3100605406511123....\n",
      "Validation loss: 0.2757244351366376....\n",
      "-----------------------------------\n",
      "Training loss: 0.3099613396674537....\n",
      "Validation loss: 0.2756544786800185....\n",
      "-----------------------------------\n",
      "Training loss: 0.309862227924255....\n",
      "Validation loss: 0.2755846033439757....\n",
      "-----------------------------------\n",
      "Training loss: 0.30976320254897033....\n",
      "Validation loss: 0.2755150396683156....\n",
      "-----------------------------------\n",
      "Training loss: 0.30966424477668386....\n",
      "Validation loss: 0.27544527028800075....\n",
      "-----------------------------------\n",
      "Training loss: 0.30956536569604565....\n",
      "Validation loss: 0.27537557693052356....\n",
      "-----------------------------------\n",
      "Training loss: 0.3094665742824493....\n",
      "Validation loss: 0.2753059677445718....\n",
      "-----------------------------------\n",
      "Training loss: 0.3093678860612417....\n",
      "Validation loss: 0.27523665062736946....\n",
      "-----------------------------------\n",
      "Training loss: 0.30926927468020715....\n",
      "Validation loss: 0.2751671369684144....\n",
      "-----------------------------------\n",
      "Training loss: 0.30917072165611054....\n",
      "Validation loss: 0.27509764705912176....\n",
      "-----------------------------------\n",
      "Training loss: 0.3090722551096013....\n",
      "Validation loss: 0.2750285397090855....\n",
      "-----------------------------------\n",
      "Training loss: 0.30897386543821176....\n",
      "Validation loss: 0.2749591860380835....\n",
      "-----------------------------------\n",
      "Training loss: 0.308875556205007....\n",
      "Validation loss: 0.2748899150083905....\n",
      "-----------------------------------\n",
      "Training loss: 0.30877734305001253....\n",
      "Validation loss: 0.27482096232182357....\n",
      "-----------------------------------\n",
      "Training loss: 0.308679183263453....\n",
      "Validation loss: 0.2747518318485966....\n",
      "-----------------------------------\n",
      "Training loss: 0.308581093941649....\n",
      "Validation loss: 0.2746828047269528....\n",
      "-----------------------------------\n",
      "Training loss: 0.3084830898482169....\n",
      "Validation loss: 0.27461405221518576....\n",
      "-----------------------------------\n",
      "Training loss: 0.30838515193467253....\n",
      "Validation loss: 0.27454511926235753....\n",
      "-----------------------------------\n",
      "Training loss: 0.3082872944027786....\n",
      "Validation loss: 0.2744762390380024....\n",
      "-----------------------------------\n",
      "Training loss: 0.3081895511279888....\n",
      "Validation loss: 0.27440770080501564....\n",
      "-----------------------------------\n",
      "Training loss: 0.3080918763939464....\n",
      "Validation loss: 0.2743389174773496....\n",
      "-----------------------------------\n",
      "Training loss: 0.3079942817742751....\n",
      "Validation loss: 0.27427017686657....\n",
      "-----------------------------------\n",
      "Training loss: 0.30789678194433434....\n",
      "Validation loss: 0.2742017650429668....\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training and validating\n",
    "Classify_digits = NN(0.01)\n",
    "Classify_digits.add_layer(50,256,'relu')\n",
    "Classify_digits.add_layer(10,50,'softmax')\n",
    "t_loss,v_loss= train_and_validate(Classify_digits,X_train_arr,Y_train_arr,n=3)\n",
    "epochs = range(0,2250)\n",
    "plt.plot(epochs, t_loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, v_loss, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adc545c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.94      0.89       499\n",
      "           2       0.90      0.89      0.90       500\n",
      "           3       0.87      0.89      0.88       500\n",
      "           4       0.89      0.92      0.91       500\n",
      "           5       0.89      0.87      0.88       500\n",
      "           6       0.93      0.93      0.93       500\n",
      "           7       0.92      0.93      0.93       500\n",
      "           8       0.86      0.80      0.83       500\n",
      "           9       0.90      0.86      0.88       500\n",
      "          10       0.96      0.94      0.95       500\n",
      "\n",
      "    accuracy                           0.90      4999\n",
      "   macro avg       0.90      0.90      0.90      4999\n",
      "weighted avg       0.90      0.90      0.90      4999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model,X):\n",
    "    Xt=X.T\n",
    "    Y_predicted = model.predict(Xt)\n",
    "    numbs=[]\n",
    "    for i in range(4999):\n",
    "        s=np.argmax(Y_predicted[:,i])\n",
    "        s+=1\n",
    "        numbs.append(s)\n",
    "    return classification_report(Y_test_arr, numbs)\n",
    "\n",
    "X_test_arr= np.array(X_test)\n",
    "report = test(Classify_digits,X_test_arr)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56e9dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion(distortion_level, data, n_samples):\n",
    "    x=data\n",
    "    pixels_sz= int(distortion_level*256)\n",
    "    for s in range(n_samples):\n",
    "        pixels= np.random.randint(256, size=pixels_sz)\n",
    "        for i in pixels:\n",
    "            x[s,i]= int(np.random.randint(2, size=1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ff2f2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.5938830349422948....\n",
      "Validation loss: 2.589044313585624....\n",
      "-----------------------------------\n",
      "Training loss: 2.5842696214922145....\n",
      "Validation loss: 2.580062586061082....\n",
      "-----------------------------------\n",
      "Training loss: 2.5749861658388795....\n",
      "Validation loss: 2.5713840145858993....\n",
      "-----------------------------------\n",
      "Training loss: 2.566015641084627....\n",
      "Validation loss: 2.562973681709582....\n",
      "-----------------------------------\n",
      "Training loss: 2.557326466240563....\n",
      "Validation loss: 2.5548191758015864....\n",
      "-----------------------------------\n",
      "Training loss: 2.5489186837463094....\n",
      "Validation loss: 2.546915916346295....\n",
      "-----------------------------------\n",
      "Training loss: 2.5407885737325517....\n",
      "Validation loss: 2.53924927490594....\n",
      "-----------------------------------\n",
      "Training loss: 2.5329100711648382....\n",
      "Validation loss: 2.531795474627491....\n",
      "-----------------------------------\n",
      "Training loss: 2.5252570065149027....\n",
      "Validation loss: 2.524551616697042....\n",
      "-----------------------------------\n",
      "Training loss: 2.5178163727056773....\n",
      "Validation loss: 2.5175228683373745....\n",
      "-----------------------------------\n",
      "Training loss: 2.5105851938970827....\n",
      "Validation loss: 2.510683203456337....\n",
      "-----------------------------------\n",
      "Training loss: 2.5035422294180942....\n",
      "Validation loss: 2.5040146045429434....\n",
      "-----------------------------------\n",
      "Training loss: 2.496680584083508....\n",
      "Validation loss: 2.497512300881838....\n",
      "-----------------------------------\n",
      "Training loss: 2.4899905278087697....\n",
      "Validation loss: 2.4911560705915115....\n",
      "-----------------------------------\n",
      "Training loss: 2.4834582138564674....\n",
      "Validation loss: 2.484953237410173....\n",
      "-----------------------------------\n",
      "Training loss: 2.477085868941385....\n",
      "Validation loss: 2.4788928760746702....\n",
      "-----------------------------------\n",
      "Training loss: 2.4708647901740934....\n",
      "Validation loss: 2.4729653044054256....\n",
      "-----------------------------------\n",
      "Training loss: 2.4647784011098715....\n",
      "Validation loss: 2.4671703217749....\n",
      "-----------------------------------\n",
      "Training loss: 2.458823576853237....\n",
      "Validation loss: 2.4615200126183656....\n",
      "-----------------------------------\n",
      "Training loss: 2.4530100076747154....\n",
      "Validation loss: 2.455984957924221....\n",
      "-----------------------------------\n",
      "Training loss: 2.447328586071818....\n",
      "Validation loss: 2.450571078101191....\n",
      "-----------------------------------\n",
      "Training loss: 2.441771544770552....\n",
      "Validation loss: 2.445268088497344....\n",
      "-----------------------------------\n",
      "Training loss: 2.436319962554729....\n",
      "Validation loss: 2.440079892123686....\n",
      "-----------------------------------\n",
      "Training loss: 2.4309873855275796....\n",
      "Validation loss: 2.4349995122389214....\n",
      "-----------------------------------\n",
      "Training loss: 2.4257666343820246....\n",
      "Validation loss: 2.430011209241958....\n",
      "-----------------------------------\n",
      "Training loss: 2.420640152091052....\n",
      "Validation loss: 2.425113044707794....\n",
      "-----------------------------------\n",
      "Training loss: 2.415605955196245....\n",
      "Validation loss: 2.4203031361902223....\n",
      "-----------------------------------\n",
      "Training loss: 2.410661683726158....\n",
      "Validation loss: 2.4155838851348816....\n",
      "-----------------------------------\n",
      "Training loss: 2.405798037749617....\n",
      "Validation loss: 2.4109382694143493....\n",
      "-----------------------------------\n",
      "Training loss: 2.4010097041414227....\n",
      "Validation loss: 2.4063677105589614....\n",
      "-----------------------------------\n",
      "Training loss: 2.3963005513867133....\n",
      "Validation loss: 2.401868663264939....\n",
      "-----------------------------------\n",
      "Training loss: 2.39166596804591....\n",
      "Validation loss: 2.3974384425464876....\n",
      "-----------------------------------\n",
      "Training loss: 2.3870944797603615....\n",
      "Validation loss: 2.393084362066211....\n",
      "-----------------------------------\n",
      "Training loss: 2.382602831834809....\n",
      "Validation loss: 2.3888018446807955....\n",
      "-----------------------------------\n",
      "Training loss: 2.3781824933518836....\n",
      "Validation loss: 2.3845856227337237....\n",
      "-----------------------------------\n",
      "Training loss: 2.3738230528236755....\n",
      "Validation loss: 2.3804356857582154....\n",
      "-----------------------------------\n",
      "Training loss: 2.3695274548136718....\n",
      "Validation loss: 2.3763555508550573....\n",
      "-----------------------------------\n",
      "Training loss: 2.3652978624944443....\n",
      "Validation loss: 2.3723314523084236....\n",
      "-----------------------------------\n",
      "Training loss: 2.3611235921216314....\n",
      "Validation loss: 2.368359595220015....\n",
      "-----------------------------------\n",
      "Training loss: 2.357004661070624....\n",
      "Validation loss: 2.364440318922241....\n",
      "-----------------------------------\n",
      "Training loss: 2.3529425313497097....\n",
      "Validation loss: 2.3605808750552737....\n",
      "-----------------------------------\n",
      "Training loss: 2.3489333977393576....\n",
      "Validation loss: 2.35676900271273....\n",
      "-----------------------------------\n",
      "Training loss: 2.344974374382248....\n",
      "Validation loss: 2.353012255819905....\n",
      "-----------------------------------\n",
      "Training loss: 2.341066700367756....\n",
      "Validation loss: 2.3493034054921753....\n",
      "-----------------------------------\n",
      "Training loss: 2.3371995818008746....\n",
      "Validation loss: 2.3456460603882863....\n",
      "-----------------------------------\n",
      "Training loss: 2.333373777150142....\n",
      "Validation loss: 2.342027541483938....\n",
      "-----------------------------------\n",
      "Training loss: 2.3295919454580476....\n",
      "Validation loss: 2.3384585277929926....\n",
      "-----------------------------------\n",
      "Training loss: 2.3258641686791584....\n",
      "Validation loss: 2.3349345929095064....\n",
      "-----------------------------------\n",
      "Training loss: 2.3221771932292294....\n",
      "Validation loss: 2.331452514621191....\n",
      "-----------------------------------\n",
      "Training loss: 2.3185313691181175....\n",
      "Validation loss: 2.3280101078528985....\n",
      "-----------------------------------\n",
      "Training loss: 2.314927345126649....\n",
      "Validation loss: 2.3246021173668323....\n",
      "-----------------------------------\n",
      "Training loss: 2.311365056226747....\n",
      "Validation loss: 2.3212190769747605....\n",
      "-----------------------------------\n",
      "Training loss: 2.3078434087343926....\n",
      "Validation loss: 2.317871538937678....\n",
      "-----------------------------------\n",
      "Training loss: 2.304356685970539....\n",
      "Validation loss: 2.314560487676823....\n",
      "-----------------------------------\n",
      "Training loss: 2.300906856890743....\n",
      "Validation loss: 2.3112765536631996....\n",
      "-----------------------------------\n",
      "Training loss: 2.2974906840889346....\n",
      "Validation loss: 2.3080199374615327....\n",
      "-----------------------------------\n",
      "Training loss: 2.2941047821991027....\n",
      "Validation loss: 2.3047895814588673....\n",
      "-----------------------------------\n",
      "Training loss: 2.2907484999770613....\n",
      "Validation loss: 2.301587783480537....\n",
      "-----------------------------------\n",
      "Training loss: 2.2874216359721666....\n",
      "Validation loss: 2.2984103381764345....\n",
      "-----------------------------------\n",
      "Training loss: 2.2841218561073857....\n",
      "Validation loss: 2.2952550678793213....\n",
      "-----------------------------------\n",
      "Training loss: 2.280849578857178....\n",
      "Validation loss: 2.292124639957309....\n",
      "-----------------------------------\n",
      "Training loss: 2.2776051930198813....\n",
      "Validation loss: 2.2890143991938667....\n",
      "-----------------------------------\n",
      "Training loss: 2.27438168196045....\n",
      "Validation loss: 2.2859189495877406....\n",
      "-----------------------------------\n",
      "Training loss: 2.27117434791179....\n",
      "Validation loss: 2.28284856674796....\n",
      "-----------------------------------\n",
      "Training loss: 2.267989663681985....\n",
      "Validation loss: 2.27979792532298....\n",
      "-----------------------------------\n",
      "Training loss: 2.264821974088965....\n",
      "Validation loss: 2.2767683262435456....\n",
      "-----------------------------------\n",
      "Training loss: 2.261675197741022....\n",
      "Validation loss: 2.2737623661441595....\n",
      "-----------------------------------\n",
      "Training loss: 2.258556165460357....\n",
      "Validation loss: 2.2707826769139685....\n",
      "-----------------------------------\n",
      "Training loss: 2.2554589976663126....\n",
      "Validation loss: 2.2678204869259453....\n",
      "-----------------------------------\n",
      "Training loss: 2.2523730727155717....\n",
      "Validation loss: 2.264871382925517....\n",
      "-----------------------------------\n",
      "Training loss: 2.24929957049507....\n",
      "Validation loss: 2.261939760608518....\n",
      "-----------------------------------\n",
      "Training loss: 2.2462475921984257....\n",
      "Validation loss: 2.25901535979321....\n",
      "-----------------------------------\n",
      "Training loss: 2.2432066911855024....\n",
      "Validation loss: 2.256104740417132....\n",
      "-----------------------------------\n",
      "Training loss: 2.240180312963832....\n",
      "Validation loss: 2.2532079036930512....\n",
      "-----------------------------------\n",
      "Training loss: 2.2371752723840954....\n",
      "Validation loss: 2.2503278287515207....\n",
      "-----------------------------------\n",
      "Training loss: 2.234188996176094....\n",
      "Validation loss: 2.247464206538738....\n",
      "-----------------------------------\n",
      "Training loss: 2.2312143471570547....\n",
      "Validation loss: 2.244616394155023....\n",
      "-----------------------------------\n",
      "Training loss: 2.228256089212676....\n",
      "Validation loss: 2.241782989209144....\n",
      "-----------------------------------\n",
      "Training loss: 2.225311149339447....\n",
      "Validation loss: 2.2389626780765894....\n",
      "-----------------------------------\n",
      "Training loss: 2.2223800999408447....\n",
      "Validation loss: 2.2361508170359983....\n",
      "-----------------------------------\n",
      "Training loss: 2.2194616631474178....\n",
      "Validation loss: 2.2333509645448317....\n",
      "-----------------------------------\n",
      "Training loss: 2.2165518425039696....\n",
      "Validation loss: 2.2305603836976053....\n",
      "-----------------------------------\n",
      "Training loss: 2.213652349482422....\n",
      "Validation loss: 2.2277809450279893....\n",
      "-----------------------------------\n",
      "Training loss: 2.210771911091791....\n",
      "Validation loss: 2.2250161570449696....\n",
      "-----------------------------------\n",
      "Training loss: 2.2079036094631816....\n",
      "Validation loss: 2.222273995737454....\n",
      "-----------------------------------\n",
      "Training loss: 2.2050550167458067....\n",
      "Validation loss: 2.219538820502383....\n",
      "-----------------------------------\n",
      "Training loss: 2.202217648514105....\n",
      "Validation loss: 2.216808518824276....\n",
      "-----------------------------------\n",
      "Training loss: 2.1993924138314322....\n",
      "Validation loss: 2.214086718701869....\n",
      "-----------------------------------\n",
      "Training loss: 2.196578557968542....\n",
      "Validation loss: 2.211369525518602....\n",
      "-----------------------------------\n",
      "Training loss: 2.1937726635071204....\n",
      "Validation loss: 2.2086650310500118....\n",
      "-----------------------------------\n",
      "Training loss: 2.1909717034994145....\n",
      "Validation loss: 2.2059662301095715....\n",
      "-----------------------------------\n",
      "Training loss: 2.1881743128895823....\n",
      "Validation loss: 2.2032818756636443....\n",
      "-----------------------------------\n",
      "Training loss: 2.18538461370677....\n",
      "Validation loss: 2.2006047535199627....\n",
      "-----------------------------------\n",
      "Training loss: 2.182600771330504....\n",
      "Validation loss: 2.197938073144335....\n",
      "-----------------------------------\n",
      "Training loss: 2.1798286020031084....\n",
      "Validation loss: 2.195281073468234....\n",
      "-----------------------------------\n",
      "Training loss: 2.177065836476299....\n",
      "Validation loss: 2.1926330502672546....\n",
      "-----------------------------------\n",
      "Training loss: 2.1743157667869077....\n",
      "Validation loss: 2.189994706215478....\n",
      "-----------------------------------\n",
      "Training loss: 2.1715807042472384....\n",
      "Validation loss: 2.187365932069468....\n",
      "-----------------------------------\n",
      "Training loss: 2.1688532928523316....\n",
      "Validation loss: 2.184747149811237....\n",
      "-----------------------------------\n",
      "Training loss: 2.1661262120882694....\n",
      "Validation loss: 2.182133352388581....\n",
      "-----------------------------------\n",
      "Training loss: 2.163400173225776....\n",
      "Validation loss: 2.179526319030456....\n",
      "-----------------------------------\n",
      "Training loss: 2.1606776872901285....\n",
      "Validation loss: 2.1769297383690707....\n",
      "-----------------------------------\n",
      "Training loss: 2.1579647287587935....\n",
      "Validation loss: 2.174342557048863....\n",
      "-----------------------------------\n",
      "Training loss: 2.155259853762016....\n",
      "Validation loss: 2.171758116714775....\n",
      "-----------------------------------\n",
      "Training loss: 2.1525625974860865....\n",
      "Validation loss: 2.1691806021141913....\n",
      "-----------------------------------\n",
      "Training loss: 2.149869703047766....\n",
      "Validation loss: 2.166612638977883....\n",
      "-----------------------------------\n",
      "Training loss: 2.1471853980075797....\n",
      "Validation loss: 2.1640475429107706....\n",
      "-----------------------------------\n",
      "Training loss: 2.1445040044331933....\n",
      "Validation loss: 2.1614914634487685....\n",
      "-----------------------------------\n",
      "Training loss: 2.141829793605444....\n",
      "Validation loss: 2.1589372248320924....\n",
      "-----------------------------------\n",
      "Training loss: 2.1391614714588005....\n",
      "Validation loss: 2.1563882377825....\n",
      "-----------------------------------\n",
      "Training loss: 2.136501456136833....\n",
      "Validation loss: 2.153841062939531....\n",
      "-----------------------------------\n",
      "Training loss: 2.1338465576548815....\n",
      "Validation loss: 2.151302437497567....\n",
      "-----------------------------------\n",
      "Training loss: 2.131197082452904....\n",
      "Validation loss: 2.1487672323817724....\n",
      "-----------------------------------\n",
      "Training loss: 2.1285477792042666....\n",
      "Validation loss: 2.1462354435527367....\n",
      "-----------------------------------\n",
      "Training loss: 2.125900432301525....\n",
      "Validation loss: 2.1437070112903385....\n",
      "-----------------------------------\n",
      "Training loss: 2.1232561309209985....\n",
      "Validation loss: 2.1411776730799943....\n",
      "-----------------------------------\n",
      "Training loss: 2.1206155089996224....\n",
      "Validation loss: 2.1386495404311447....\n",
      "-----------------------------------\n",
      "Training loss: 2.1179786412927912....\n",
      "Validation loss: 2.136125612910194....\n",
      "-----------------------------------\n",
      "Training loss: 2.1153446801622304....\n",
      "Validation loss: 2.133606671425881....\n",
      "-----------------------------------\n",
      "Training loss: 2.112713368477447....\n",
      "Validation loss: 2.1310894798346927....\n",
      "-----------------------------------\n",
      "Training loss: 2.110088413422538....\n",
      "Validation loss: 2.1285695827168385....\n",
      "-----------------------------------\n",
      "Training loss: 2.1074672316132355....\n",
      "Validation loss: 2.126052396661738....\n",
      "-----------------------------------\n",
      "Training loss: 2.1048502845951247....\n",
      "Validation loss: 2.12353764888005....\n",
      "-----------------------------------\n",
      "Training loss: 2.102233691141473....\n",
      "Validation loss: 2.1210281980284424....\n",
      "-----------------------------------\n",
      "Training loss: 2.0996200992708336....\n",
      "Validation loss: 2.1185231526034114....\n",
      "-----------------------------------\n",
      "Training loss: 2.097012158445449....\n",
      "Validation loss: 2.1160235704384758....\n",
      "-----------------------------------\n",
      "Training loss: 2.0944068739696924....\n",
      "Validation loss: 2.113527379021417....\n",
      "-----------------------------------\n",
      "Training loss: 2.091800797977795....\n",
      "Validation loss: 2.1110363360316367....\n",
      "-----------------------------------\n",
      "Training loss: 2.0891944127666373....\n",
      "Validation loss: 2.1085477136603448....\n",
      "-----------------------------------\n",
      "Training loss: 2.0865907955305847....\n",
      "Validation loss: 2.1060588442352124....\n",
      "-----------------------------------\n",
      "Training loss: 2.083985217463832....\n",
      "Validation loss: 2.103572527134961....\n",
      "-----------------------------------\n",
      "Training loss: 2.0813838459303873....\n",
      "Validation loss: 2.1010866777533033....\n",
      "-----------------------------------\n",
      "Training loss: 2.078782615870212....\n",
      "Validation loss: 2.0986056295225675....\n",
      "-----------------------------------\n",
      "Training loss: 2.076183795479962....\n",
      "Validation loss: 2.096124454679533....\n",
      "-----------------------------------\n",
      "Training loss: 2.073587322460251....\n",
      "Validation loss: 2.093648471747856....\n",
      "-----------------------------------\n",
      "Training loss: 2.0709936335604686....\n",
      "Validation loss: 2.0911752837052346....\n",
      "-----------------------------------\n",
      "Training loss: 2.0683991501440198....\n",
      "Validation loss: 2.088703776815585....\n",
      "-----------------------------------\n",
      "Training loss: 2.065803749478303....\n",
      "Validation loss: 2.0862307837048544....\n",
      "-----------------------------------\n",
      "Training loss: 2.0632083317950998....\n",
      "Validation loss: 2.0837535778789436....\n",
      "-----------------------------------\n",
      "Training loss: 2.060611991550399....\n",
      "Validation loss: 2.081279670354003....\n",
      "-----------------------------------\n",
      "Training loss: 2.0580189746070365....\n",
      "Validation loss: 2.0788085156079044....\n",
      "-----------------------------------\n",
      "Training loss: 2.055427457888207....\n",
      "Validation loss: 2.0763417949970444....\n",
      "-----------------------------------\n",
      "Training loss: 2.0528382259632374....\n",
      "Validation loss: 2.073873932208484....\n",
      "-----------------------------------\n",
      "Training loss: 2.0502480166771044....\n",
      "Validation loss: 2.071405915939122....\n",
      "-----------------------------------\n",
      "Training loss: 2.0476596909012006....\n",
      "Validation loss: 2.068938665621255....\n",
      "-----------------------------------\n",
      "Training loss: 2.045069894212095....\n",
      "Validation loss: 2.066471912979042....\n",
      "-----------------------------------\n",
      "Training loss: 2.042478273619532....\n",
      "Validation loss: 2.064004968516298....\n",
      "-----------------------------------\n",
      "Training loss: 2.039886162144748....\n",
      "Validation loss: 2.061533622828169....\n",
      "-----------------------------------\n",
      "Training loss: 2.037289401888121....\n",
      "Validation loss: 2.059059737196887....\n",
      "-----------------------------------\n",
      "Training loss: 2.0346901822221892....\n",
      "Validation loss: 2.0565864889909977....\n",
      "-----------------------------------\n",
      "Training loss: 2.032092663465159....\n",
      "Validation loss: 2.054113513842016....\n",
      "-----------------------------------\n",
      "Training loss: 2.0294985936568977....\n",
      "Validation loss: 2.051638390152012....\n",
      "-----------------------------------\n",
      "Training loss: 2.026905676182612....\n",
      "Validation loss: 2.049164053032777....\n",
      "-----------------------------------\n",
      "Training loss: 2.024312071040242....\n",
      "Validation loss: 2.046688952940101....\n",
      "-----------------------------------\n",
      "Training loss: 2.021717774025036....\n",
      "Validation loss: 2.044208660504798....\n",
      "-----------------------------------\n",
      "Training loss: 2.0191228101556336....\n",
      "Validation loss: 2.041726064267551....\n",
      "-----------------------------------\n",
      "Training loss: 2.016527297700141....\n",
      "Validation loss: 2.0392485322753435....\n",
      "-----------------------------------\n",
      "Training loss: 2.0139346073768754....\n",
      "Validation loss: 2.036773762560045....\n",
      "-----------------------------------\n",
      "Training loss: 2.0113449415420788....\n",
      "Validation loss: 2.03429980034173....\n",
      "-----------------------------------\n",
      "Training loss: 2.0087550549344924....\n",
      "Validation loss: 2.0318215452908097....\n",
      "-----------------------------------\n",
      "Training loss: 2.0061614474435814....\n",
      "Validation loss: 2.0293390874828696....\n",
      "-----------------------------------\n",
      "Training loss: 2.0035685096421076....\n",
      "Validation loss: 2.0268503750730864....\n",
      "-----------------------------------\n",
      "Training loss: 2.000971083915725....\n",
      "Validation loss: 2.0243567770209103....\n",
      "-----------------------------------\n",
      "Training loss: 1.9983737460744708....\n",
      "Validation loss: 2.0218642948347174....\n",
      "-----------------------------------\n",
      "Training loss: 1.99577889373049....\n",
      "Validation loss: 2.0193729634897504....\n",
      "-----------------------------------\n",
      "Training loss: 1.9931843508992455....\n",
      "Validation loss: 2.0168785236145923....\n",
      "-----------------------------------\n",
      "Training loss: 1.9905864390609813....\n",
      "Validation loss: 2.0143823664115454....\n",
      "-----------------------------------\n",
      "Training loss: 1.9879856697749192....\n",
      "Validation loss: 2.011885228488448....\n",
      "-----------------------------------\n",
      "Training loss: 1.9853852864470842....\n",
      "Validation loss: 2.0093842335423457....\n",
      "-----------------------------------\n",
      "Training loss: 1.9827851474158433....\n",
      "Validation loss: 2.0068809465496247....\n",
      "-----------------------------------\n",
      "Training loss: 1.980188797932878....\n",
      "Validation loss: 2.004375049530124....\n",
      "-----------------------------------\n",
      "Training loss: 1.9775968275389368....\n",
      "Validation loss: 2.001867282012134....\n",
      "-----------------------------------\n",
      "Training loss: 1.9750050670166117....\n",
      "Validation loss: 1.999360792468487....\n",
      "-----------------------------------\n",
      "Training loss: 1.9724134460578306....\n",
      "Validation loss: 1.9968563394438374....\n",
      "-----------------------------------\n",
      "Training loss: 1.969823534486552....\n",
      "Validation loss: 1.9943571010551122....\n",
      "-----------------------------------\n",
      "Training loss: 1.9672370028926....\n",
      "Validation loss: 1.991858257013239....\n",
      "-----------------------------------\n",
      "Training loss: 1.964650976038559....\n",
      "Validation loss: 1.9893613200081222....\n",
      "-----------------------------------\n",
      "Training loss: 1.9620669139972409....\n",
      "Validation loss: 1.986862308509958....\n",
      "-----------------------------------\n",
      "Training loss: 1.9594837201443187....\n",
      "Validation loss: 1.9843639858602722....\n",
      "-----------------------------------\n",
      "Training loss: 1.9569015144542594....\n",
      "Validation loss: 1.981865982679199....\n",
      "-----------------------------------\n",
      "Training loss: 1.9543170096009879....\n",
      "Validation loss: 1.9793653497901524....\n",
      "-----------------------------------\n",
      "Training loss: 1.9517290184568126....\n",
      "Validation loss: 1.9768602178943262....\n",
      "-----------------------------------\n",
      "Training loss: 1.949134433698303....\n",
      "Validation loss: 1.9743533249698422....\n",
      "-----------------------------------\n",
      "Training loss: 1.9465376521348716....\n",
      "Validation loss: 1.9718467869789216....\n",
      "-----------------------------------\n",
      "Training loss: 1.943940363619604....\n",
      "Validation loss: 1.9693383969497342....\n",
      "-----------------------------------\n",
      "Training loss: 1.9413409849436036....\n",
      "Validation loss: 1.966831416291684....\n",
      "-----------------------------------\n",
      "Training loss: 1.9387417982074424....\n",
      "Validation loss: 1.9643211117059336....\n",
      "-----------------------------------\n",
      "Training loss: 1.936142786775417....\n",
      "Validation loss: 1.9618083295915985....\n",
      "-----------------------------------\n",
      "Training loss: 1.9335429138067899....\n",
      "Validation loss: 1.9592964202871987....\n",
      "-----------------------------------\n",
      "Training loss: 1.9309422019361036....\n",
      "Validation loss: 1.956782046751424....\n",
      "-----------------------------------\n",
      "Training loss: 1.9283397180920687....\n",
      "Validation loss: 1.9542720975602976....\n",
      "-----------------------------------\n",
      "Training loss: 1.9257425150211496....\n",
      "Validation loss: 1.9517615094665144....\n",
      "-----------------------------------\n",
      "Training loss: 1.9231424096405971....\n",
      "Validation loss: 1.9492506574671629....\n",
      "-----------------------------------\n",
      "Training loss: 1.9205371907803033....\n",
      "Validation loss: 1.94673658491486....\n",
      "-----------------------------------\n",
      "Training loss: 1.9179254876633984....\n",
      "Validation loss: 1.9442214498496415....\n",
      "-----------------------------------\n",
      "Training loss: 1.9153119361051596....\n",
      "Validation loss: 1.9417048860425457....\n",
      "-----------------------------------\n",
      "Training loss: 1.912697247878723....\n",
      "Validation loss: 1.9391841952392912....\n",
      "-----------------------------------\n",
      "Training loss: 1.9100839421318911....\n",
      "Validation loss: 1.9366661716372464....\n",
      "-----------------------------------\n",
      "Training loss: 1.907469879086393....\n",
      "Validation loss: 1.9341495072555972....\n",
      "-----------------------------------\n",
      "Training loss: 1.9048543949051002....\n",
      "Validation loss: 1.9316318277569808....\n",
      "-----------------------------------\n",
      "Training loss: 1.9022390778970832....\n",
      "Validation loss: 1.9291112342677628....\n",
      "-----------------------------------\n",
      "Training loss: 1.8996207162401249....\n",
      "Validation loss: 1.9265873741375585....\n",
      "-----------------------------------\n",
      "Training loss: 1.897000866405946....\n",
      "Validation loss: 1.9240618595230503....\n",
      "-----------------------------------\n",
      "Training loss: 1.894382163546693....\n",
      "Validation loss: 1.9215376351394975....\n",
      "-----------------------------------\n",
      "Training loss: 1.8917680793330334....\n",
      "Validation loss: 1.9190149505404097....\n",
      "-----------------------------------\n",
      "Training loss: 1.8891564042202076....\n",
      "Validation loss: 1.9164916265888663....\n",
      "-----------------------------------\n",
      "Training loss: 1.8865451320308524....\n",
      "Validation loss: 1.9139701683004404....\n",
      "-----------------------------------\n",
      "Training loss: 1.8839348367244....\n",
      "Validation loss: 1.9114523504196697....\n",
      "-----------------------------------\n",
      "Training loss: 1.8813270547889243....\n",
      "Validation loss: 1.9089373647997736....\n",
      "-----------------------------------\n",
      "Training loss: 1.8787210591319878....\n",
      "Validation loss: 1.9064207875958112....\n",
      "-----------------------------------\n",
      "Training loss: 1.876115456201347....\n",
      "Validation loss: 1.9039015971968238....\n",
      "-----------------------------------\n",
      "Training loss: 1.8735071666400194....\n",
      "Validation loss: 1.9013827021430738....\n",
      "-----------------------------------\n",
      "Training loss: 1.8708971057500428....\n",
      "Validation loss: 1.898865519830114....\n",
      "-----------------------------------\n",
      "Training loss: 1.8682868040051692....\n",
      "Validation loss: 1.8963493922228385....\n",
      "-----------------------------------\n",
      "Training loss: 1.8656785648232022....\n",
      "Validation loss: 1.8938325214039855....\n",
      "-----------------------------------\n",
      "Training loss: 1.863069951198148....\n",
      "Validation loss: 1.8913149609769868....\n",
      "-----------------------------------\n",
      "Training loss: 1.8604628792890405....\n",
      "Validation loss: 1.8887979246530011....\n",
      "-----------------------------------\n",
      "Training loss: 1.857858376178478....\n",
      "Validation loss: 1.8862838609340646....\n",
      "-----------------------------------\n",
      "Training loss: 1.8552563939310573....\n",
      "Validation loss: 1.8837715536664297....\n",
      "-----------------------------------\n",
      "Training loss: 1.852654809463174....\n",
      "Validation loss: 1.8812604572064797....\n",
      "-----------------------------------\n",
      "Training loss: 1.8500512118789523....\n",
      "Validation loss: 1.8787499781078194....\n",
      "-----------------------------------\n",
      "Training loss: 1.847447890751503....\n",
      "Validation loss: 1.8762368021346965....\n",
      "-----------------------------------\n",
      "Training loss: 1.8448455205942471....\n",
      "Validation loss: 1.873723981543553....\n",
      "-----------------------------------\n",
      "Training loss: 1.8422429719465423....\n",
      "Validation loss: 1.871209423443974....\n",
      "-----------------------------------\n",
      "Training loss: 1.83963653921366....\n",
      "Validation loss: 1.8686953398545243....\n",
      "-----------------------------------\n",
      "Training loss: 1.8370303235877419....\n",
      "Validation loss: 1.8661835456228728....\n",
      "-----------------------------------\n",
      "Training loss: 1.8344247463478043....\n",
      "Validation loss: 1.8636709104329618....\n",
      "-----------------------------------\n",
      "Training loss: 1.8318175301618034....\n",
      "Validation loss: 1.8611606263371894....\n",
      "-----------------------------------\n",
      "Training loss: 1.8292114958059464....\n",
      "Validation loss: 1.8586511489045976....\n",
      "-----------------------------------\n",
      "Training loss: 1.8266067510707447....\n",
      "Validation loss: 1.8561407284489593....\n",
      "-----------------------------------\n",
      "Training loss: 1.8240001327412152....\n",
      "Validation loss: 1.853626006837274....\n",
      "-----------------------------------\n",
      "Training loss: 1.8213910414583578....\n",
      "Validation loss: 1.8511117446815781....\n",
      "-----------------------------------\n",
      "Training loss: 1.818782416024069....\n",
      "Validation loss: 1.8485981214829312....\n",
      "-----------------------------------\n",
      "Training loss: 1.8161756601192027....\n",
      "Validation loss: 1.846084850903466....\n",
      "-----------------------------------\n",
      "Training loss: 1.8135699628461117....\n",
      "Validation loss: 1.843574893658908....\n",
      "-----------------------------------\n",
      "Training loss: 1.8109651326321425....\n",
      "Validation loss: 1.8410675486245245....\n",
      "-----------------------------------\n",
      "Training loss: 1.80836118999391....\n",
      "Validation loss: 1.838561607629843....\n",
      "-----------------------------------\n",
      "Training loss: 1.8057580342879556....\n",
      "Validation loss: 1.8360560655665583....\n",
      "-----------------------------------\n",
      "Training loss: 1.8031553205871547....\n",
      "Validation loss: 1.8335521902068437....\n",
      "-----------------------------------\n",
      "Training loss: 1.800552959563188....\n",
      "Validation loss: 1.8310473077241074....\n",
      "-----------------------------------\n",
      "Training loss: 1.7979508691660469....\n",
      "Validation loss: 1.8285408959301792....\n",
      "-----------------------------------\n",
      "Training loss: 1.7953489277098003....\n",
      "Validation loss: 1.8260365682575295....\n",
      "-----------------------------------\n",
      "Training loss: 1.7927504437007897....\n",
      "Validation loss: 1.8235319499329308....\n",
      "-----------------------------------\n",
      "Training loss: 1.7901521282064095....\n",
      "Validation loss: 1.8210294915044176....\n",
      "-----------------------------------\n",
      "Training loss: 1.78755430626948....\n",
      "Validation loss: 1.8185302363632854....\n",
      "-----------------------------------\n",
      "Training loss: 1.7849581626854103....\n",
      "Validation loss: 1.816030628654064....\n",
      "-----------------------------------\n",
      "Training loss: 1.7823619799107404....\n",
      "Validation loss: 1.813530600648458....\n",
      "-----------------------------------\n",
      "Training loss: 1.7797625772912327....\n",
      "Validation loss: 1.8110280981863955....\n",
      "-----------------------------------\n",
      "Training loss: 1.7771618445168584....\n",
      "Validation loss: 1.8085258264156372....\n",
      "-----------------------------------\n",
      "Training loss: 1.7745612518505307....\n",
      "Validation loss: 1.8060239835370053....\n",
      "-----------------------------------\n",
      "Training loss: 1.7719600185849698....\n",
      "Validation loss: 1.8035248941968174....\n",
      "-----------------------------------\n",
      "Training loss: 1.769361146876023....\n",
      "Validation loss: 1.801026042940966....\n",
      "-----------------------------------\n",
      "Training loss: 1.7667604664996825....\n",
      "Validation loss: 1.7985265204886522....\n",
      "-----------------------------------\n",
      "Training loss: 1.7641584063512261....\n",
      "Validation loss: 1.796027306860977....\n",
      "-----------------------------------\n",
      "Training loss: 1.7615559127671527....\n",
      "Validation loss: 1.7935330391929174....\n",
      "-----------------------------------\n",
      "Training loss: 1.7589566662148362....\n",
      "Validation loss: 1.7910427898671317....\n",
      "-----------------------------------\n",
      "Training loss: 1.7563616990660618....\n",
      "Validation loss: 1.7885526040849464....\n",
      "-----------------------------------\n",
      "Training loss: 1.7537670859640664....\n",
      "Validation loss: 1.786064949685643....\n",
      "-----------------------------------\n",
      "Training loss: 1.7511751827045763....\n",
      "Validation loss: 1.7835795831875199....\n",
      "-----------------------------------\n",
      "Training loss: 1.748584313287768....\n",
      "Validation loss: 1.7810982644681872....\n",
      "-----------------------------------\n",
      "Training loss: 1.7459973803503333....\n",
      "Validation loss: 1.7786169978244106....\n",
      "-----------------------------------\n",
      "Training loss: 1.7434102859248555....\n",
      "Validation loss: 1.7761358590683518....\n",
      "-----------------------------------\n",
      "Training loss: 1.740824686745305....\n",
      "Validation loss: 1.7736549657458514....\n",
      "-----------------------------------\n",
      "Training loss: 1.7382418282686463....\n",
      "Validation loss: 1.7711724474818813....\n",
      "-----------------------------------\n",
      "Training loss: 1.7356592823746082....\n",
      "Validation loss: 1.7686898092776642....\n",
      "-----------------------------------\n",
      "Training loss: 1.7330774565058469....\n",
      "Validation loss: 1.7662057851763888....\n",
      "-----------------------------------\n",
      "Training loss: 1.7304960776590257....\n",
      "Validation loss: 1.7637178650923884....\n",
      "-----------------------------------\n",
      "Training loss: 1.7279120823692944....\n",
      "Validation loss: 1.761229676389104....\n",
      "-----------------------------------\n",
      "Training loss: 1.7253263920212991....\n",
      "Validation loss: 1.7587435739691428....\n",
      "-----------------------------------\n",
      "Training loss: 1.7227423536249973....\n",
      "Validation loss: 1.756260111997216....\n",
      "-----------------------------------\n",
      "Training loss: 1.7201597771020989....\n",
      "Validation loss: 1.7537747789937848....\n",
      "-----------------------------------\n",
      "Training loss: 1.7175765552679279....\n",
      "Validation loss: 1.7512910394579417....\n",
      "-----------------------------------\n",
      "Training loss: 1.714994872769189....\n",
      "Validation loss: 1.7488114912511752....\n",
      "-----------------------------------\n",
      "Training loss: 1.712415936924706....\n",
      "Validation loss: 1.7463320082523957....\n",
      "-----------------------------------\n",
      "Training loss: 1.709840803931673....\n",
      "Validation loss: 1.7438571569890216....\n",
      "-----------------------------------\n",
      "Training loss: 1.7072708762623234....\n",
      "Validation loss: 1.741383628730559....\n",
      "-----------------------------------\n",
      "Training loss: 1.7046983899380397....\n",
      "Validation loss: 1.7389101908484337....\n",
      "-----------------------------------\n",
      "Training loss: 1.7021245878867037....\n",
      "Validation loss: 1.7364370799588864....\n",
      "-----------------------------------\n",
      "Training loss: 1.6995498973749836....\n",
      "Validation loss: 1.7339657363956118....\n",
      "-----------------------------------\n",
      "Training loss: 1.6969801530651873....\n",
      "Validation loss: 1.7314975045779255....\n",
      "-----------------------------------\n",
      "Training loss: 1.694413015515037....\n",
      "Validation loss: 1.7290317420672785....\n",
      "-----------------------------------\n",
      "Training loss: 1.6918477032734653....\n",
      "Validation loss: 1.726566423652097....\n",
      "-----------------------------------\n",
      "Training loss: 1.6892817642615265....\n",
      "Validation loss: 1.724102922383943....\n",
      "-----------------------------------\n",
      "Training loss: 1.6867198376833088....\n",
      "Validation loss: 1.7216431368308354....\n",
      "-----------------------------------\n",
      "Training loss: 1.684160146752142....\n",
      "Validation loss: 1.7191853494785196....\n",
      "-----------------------------------\n",
      "Training loss: 1.681604144285592....\n",
      "Validation loss: 1.7167282239231376....\n",
      "-----------------------------------\n",
      "Training loss: 1.679051139414536....\n",
      "Validation loss: 1.7142740625798687....\n",
      "-----------------------------------\n",
      "Training loss: 1.6764988767839446....\n",
      "Validation loss: 1.711821390550551....\n",
      "-----------------------------------\n",
      "Training loss: 1.673947043507309....\n",
      "Validation loss: 1.709370320958329....\n",
      "-----------------------------------\n",
      "Training loss: 1.6713957103010435....\n",
      "Validation loss: 1.70691924677969....\n",
      "-----------------------------------\n",
      "Training loss: 1.6688451488958262....\n",
      "Validation loss: 1.7044693064480971....\n",
      "-----------------------------------\n",
      "Training loss: 1.666297216166837....\n",
      "Validation loss: 1.7020227592904467....\n",
      "-----------------------------------\n",
      "Training loss: 1.6637498458510933....\n",
      "Validation loss: 1.6995786410974139....\n",
      "-----------------------------------\n",
      "Training loss: 1.6612034689410147....\n",
      "Validation loss: 1.6971321598473956....\n",
      "-----------------------------------\n",
      "Training loss: 1.658657434357819....\n",
      "Validation loss: 1.694688582244953....\n",
      "-----------------------------------\n",
      "Training loss: 1.6561149806065576....\n",
      "Validation loss: 1.692247741951127....\n",
      "-----------------------------------\n",
      "Training loss: 1.653576415453826....\n",
      "Validation loss: 1.6898104010529755....\n",
      "-----------------------------------\n",
      "Training loss: 1.6510408343225695....\n",
      "Validation loss: 1.6873748879469312....\n",
      "-----------------------------------\n",
      "Training loss: 1.6485074580046044....\n",
      "Validation loss: 1.6849366819629468....\n",
      "-----------------------------------\n",
      "Training loss: 1.6459751063760315....\n",
      "Validation loss: 1.6825005984783634....\n",
      "-----------------------------------\n",
      "Training loss: 1.6434454134055743....\n",
      "Validation loss: 1.6800668873797402....\n",
      "-----------------------------------\n",
      "Training loss: 1.6409166245272933....\n",
      "Validation loss: 1.6776377985179542....\n",
      "-----------------------------------\n",
      "Training loss: 1.6383897763554909....\n",
      "Validation loss: 1.6752078551384235....\n",
      "-----------------------------------\n",
      "Training loss: 1.6358606532375595....\n",
      "Validation loss: 1.6727785821417507....\n",
      "-----------------------------------\n",
      "Training loss: 1.6333317758624133....\n",
      "Validation loss: 1.6703519867443288....\n",
      "-----------------------------------\n",
      "Training loss: 1.6308043631526847....\n",
      "Validation loss: 1.6679281902250787....\n",
      "-----------------------------------\n",
      "Training loss: 1.6282807765884464....\n",
      "Validation loss: 1.6655087942591702....\n",
      "-----------------------------------\n",
      "Training loss: 1.625761215369877....\n",
      "Validation loss: 1.6630943288553395....\n",
      "-----------------------------------\n",
      "Training loss: 1.6232450064265656....\n",
      "Validation loss: 1.6606804331568492....\n",
      "-----------------------------------\n",
      "Training loss: 1.620730102554459....\n",
      "Validation loss: 1.65826688816468....\n",
      "-----------------------------------\n",
      "Training loss: 1.618217474407157....\n",
      "Validation loss: 1.6558551252558291....\n",
      "-----------------------------------\n",
      "Training loss: 1.6157070190129403....\n",
      "Validation loss: 1.6534458135301004....\n",
      "-----------------------------------\n",
      "Training loss: 1.613199060134074....\n",
      "Validation loss: 1.6510385212602172....\n",
      "-----------------------------------\n",
      "Training loss: 1.6106931515922263....\n",
      "Validation loss: 1.6486357288021836....\n",
      "-----------------------------------\n",
      "Training loss: 1.6081919810858492....\n",
      "Validation loss: 1.6462371978138475....\n",
      "-----------------------------------\n",
      "Training loss: 1.6056950377088395....\n",
      "Validation loss: 1.643839927338854....\n",
      "-----------------------------------\n",
      "Training loss: 1.6031996669297897....\n",
      "Validation loss: 1.6414445465063037....\n",
      "-----------------------------------\n",
      "Training loss: 1.6007076911666396....\n",
      "Validation loss: 1.6390539568603704....\n",
      "-----------------------------------\n",
      "Training loss: 1.5982216745635307....\n",
      "Validation loss: 1.636665265536881....\n",
      "-----------------------------------\n",
      "Training loss: 1.5957366505828572....\n",
      "Validation loss: 1.6342793571060572....\n",
      "-----------------------------------\n",
      "Training loss: 1.593253473007459....\n",
      "Validation loss: 1.631896904670884....\n",
      "-----------------------------------\n",
      "Training loss: 1.5907736773281578....\n",
      "Validation loss: 1.629518725370428....\n",
      "-----------------------------------\n",
      "Training loss: 1.5882963900784228....\n",
      "Validation loss: 1.6271438532367744....\n",
      "-----------------------------------\n",
      "Training loss: 1.5858227614073233....\n",
      "Validation loss: 1.6247738534455365....\n",
      "-----------------------------------\n",
      "Training loss: 1.5833548786714275....\n",
      "Validation loss: 1.622405121168515....\n",
      "-----------------------------------\n",
      "Training loss: 1.5808901130235846....\n",
      "Validation loss: 1.6200386914668343....\n",
      "-----------------------------------\n",
      "Training loss: 1.5784273789240664....\n",
      "Validation loss: 1.6176755936297829....\n",
      "-----------------------------------\n",
      "Training loss: 1.5759665813146972....\n",
      "Validation loss: 1.6153149811652898....\n",
      "-----------------------------------\n",
      "Training loss: 1.5735076376546093....\n",
      "Validation loss: 1.6129570466170122....\n",
      "-----------------------------------\n",
      "Training loss: 1.5710497491740403....\n",
      "Validation loss: 1.6106030871165367....\n",
      "-----------------------------------\n",
      "Training loss: 1.5685953269588215....\n",
      "Validation loss: 1.6082536742963978....\n",
      "-----------------------------------\n",
      "Training loss: 1.5661459903776742....\n",
      "Validation loss: 1.6059056101968034....\n",
      "-----------------------------------\n",
      "Training loss: 1.5636985467792572....\n",
      "Validation loss: 1.6035566103360428....\n",
      "-----------------------------------\n",
      "Training loss: 1.5612506194178597....\n",
      "Validation loss: 1.6012094620108903....\n",
      "-----------------------------------\n",
      "Training loss: 1.5588063584706089....\n",
      "Validation loss: 1.5988651126440345....\n",
      "-----------------------------------\n",
      "Training loss: 1.5563649873367447....\n",
      "Validation loss: 1.5965205808898835....\n",
      "-----------------------------------\n",
      "Training loss: 1.5539239381719563....\n",
      "Validation loss: 1.5941794036219372....\n",
      "-----------------------------------\n",
      "Training loss: 1.5514883190842752....\n",
      "Validation loss: 1.5918404679994662....\n",
      "-----------------------------------\n",
      "Training loss: 1.5490559466415605....\n",
      "Validation loss: 1.5895036168482244....\n",
      "-----------------------------------\n",
      "Training loss: 1.546627143811706....\n",
      "Validation loss: 1.5871711337767118....\n",
      "-----------------------------------\n",
      "Training loss: 1.5442029708214575....\n",
      "Validation loss: 1.5848417187908888....\n",
      "-----------------------------------\n",
      "Training loss: 1.5417825018328342....\n",
      "Validation loss: 1.5825162878065602....\n",
      "-----------------------------------\n",
      "Training loss: 1.5393671689204065....\n",
      "Validation loss: 1.5801943840615187....\n",
      "-----------------------------------\n",
      "Training loss: 1.5369545351757268....\n",
      "Validation loss: 1.5778768214644348....\n",
      "-----------------------------------\n",
      "Training loss: 1.5345462455735412....\n",
      "Validation loss: 1.5755627020100544....\n",
      "-----------------------------------\n",
      "Training loss: 1.5321403004743994....\n",
      "Validation loss: 1.5732506255279077....\n",
      "-----------------------------------\n",
      "Training loss: 1.5297359642454018....\n",
      "Validation loss: 1.570941474841615....\n",
      "-----------------------------------\n",
      "Training loss: 1.527334280000082....\n",
      "Validation loss: 1.5686357155368686....\n",
      "-----------------------------------\n",
      "Training loss: 1.5249372821395235....\n",
      "Validation loss: 1.5663333537656823....\n",
      "-----------------------------------\n",
      "Training loss: 1.522544484920018....\n",
      "Validation loss: 1.5640337730678484....\n",
      "-----------------------------------\n",
      "Training loss: 1.5201539868770255....\n",
      "Validation loss: 1.5617353979363346....\n",
      "-----------------------------------\n",
      "Training loss: 1.5177654446084559....\n",
      "Validation loss: 1.5594394149401332....\n",
      "-----------------------------------\n",
      "Training loss: 1.5153797564546774....\n",
      "Validation loss: 1.5571473914791591....\n",
      "-----------------------------------\n",
      "Training loss: 1.5129970616346415....\n",
      "Validation loss: 1.5548569311939828....\n",
      "-----------------------------------\n",
      "Training loss: 1.5106152887898954....\n",
      "Validation loss: 1.5525704681892545....\n",
      "-----------------------------------\n",
      "Training loss: 1.508238634093384....\n",
      "Validation loss: 1.5502888138528774....\n",
      "-----------------------------------\n",
      "Training loss: 1.505868724195925....\n",
      "Validation loss: 1.5480116763329443....\n",
      "-----------------------------------\n",
      "Training loss: 1.5035031354391255....\n",
      "Validation loss: 1.5457393310459293....\n",
      "-----------------------------------\n",
      "Training loss: 1.5011420376180515....\n",
      "Validation loss: 1.5434718697027832....\n",
      "-----------------------------------\n",
      "Training loss: 1.4987872247506606....\n",
      "Validation loss: 1.541210103837647....\n",
      "-----------------------------------\n",
      "Training loss: 1.496438593553855....\n",
      "Validation loss: 1.5389520460021604....\n",
      "-----------------------------------\n",
      "Training loss: 1.494092620973435....\n",
      "Validation loss: 1.536697472193317....\n",
      "-----------------------------------\n",
      "Training loss: 1.491751037262688....\n",
      "Validation loss: 1.5344459673012822....\n",
      "-----------------------------------\n",
      "Training loss: 1.4894107602135083....\n",
      "Validation loss: 1.5321972615260466....\n",
      "-----------------------------------\n",
      "Training loss: 1.4870722887115753....\n",
      "Validation loss: 1.5299516341639265....\n",
      "-----------------------------------\n",
      "Training loss: 1.4847366309895111....\n",
      "Validation loss: 1.527709167907882....\n",
      "-----------------------------------\n",
      "Training loss: 1.482404266613041....\n",
      "Validation loss: 1.5254688063650383....\n",
      "-----------------------------------\n",
      "Training loss: 1.4800743280850535....\n",
      "Validation loss: 1.523232074344789....\n",
      "-----------------------------------\n",
      "Training loss: 1.4777481875209955....\n",
      "Validation loss: 1.5209982321228994....\n",
      "-----------------------------------\n",
      "Training loss: 1.4754243115265262....\n",
      "Validation loss: 1.5187665342557715....\n",
      "-----------------------------------\n",
      "Training loss: 1.4731037615673441....\n",
      "Validation loss: 1.5165382957910003....\n",
      "-----------------------------------\n",
      "Training loss: 1.4707863118816749....\n",
      "Validation loss: 1.5143127343580889....\n",
      "-----------------------------------\n",
      "Training loss: 1.4684735442856813....\n",
      "Validation loss: 1.5120909273694796....\n",
      "-----------------------------------\n",
      "Training loss: 1.466166576099187....\n",
      "Validation loss: 1.5098729405416176....\n",
      "-----------------------------------\n",
      "Training loss: 1.4638640518199477....\n",
      "Validation loss: 1.50765961631984....\n",
      "-----------------------------------\n",
      "Training loss: 1.4615671481955244....\n",
      "Validation loss: 1.5054513059523325....\n",
      "-----------------------------------\n",
      "Training loss: 1.4592751315500692....\n",
      "Validation loss: 1.5032464229430518....\n",
      "-----------------------------------\n",
      "Training loss: 1.4569872663911396....\n",
      "Validation loss: 1.5010466881633704....\n",
      "-----------------------------------\n",
      "Training loss: 1.454703460977206....\n",
      "Validation loss: 1.4988516083084142....\n",
      "-----------------------------------\n",
      "Training loss: 1.4524223892190828....\n",
      "Validation loss: 1.496659809979535....\n",
      "-----------------------------------\n",
      "Training loss: 1.450143288342785....\n",
      "Validation loss: 1.4944716395192474....\n",
      "-----------------------------------\n",
      "Training loss: 1.447868361468537....\n",
      "Validation loss: 1.4922872245781196....\n",
      "-----------------------------------\n",
      "Training loss: 1.445597538765372....\n",
      "Validation loss: 1.4901049453234845....\n",
      "-----------------------------------\n",
      "Training loss: 1.4433306600416946....\n",
      "Validation loss: 1.487925456449887....\n",
      "-----------------------------------\n",
      "Training loss: 1.4410649577568362....\n",
      "Validation loss: 1.4857505731245029....\n",
      "-----------------------------------\n",
      "Training loss: 1.4388032407299614....\n",
      "Validation loss: 1.4835810930450617....\n",
      "-----------------------------------\n",
      "Training loss: 1.4365462351321192....\n",
      "Validation loss: 1.4814177163352702....\n",
      "-----------------------------------\n",
      "Training loss: 1.4342941887633454....\n",
      "Validation loss: 1.4792577785976624....\n",
      "-----------------------------------\n",
      "Training loss: 1.4320463447108922....\n",
      "Validation loss: 1.4771010671270033....\n",
      "-----------------------------------\n",
      "Training loss: 1.429803052342588....\n",
      "Validation loss: 1.474948299792204....\n",
      "-----------------------------------\n",
      "Training loss: 1.4275647080577776....\n",
      "Validation loss: 1.4728016105830055....\n",
      "-----------------------------------\n",
      "Training loss: 1.4253310985170131....\n",
      "Validation loss: 1.4706591128267033....\n",
      "-----------------------------------\n",
      "Training loss: 1.4231020565545616....\n",
      "Validation loss: 1.4685214825832384....\n",
      "-----------------------------------\n",
      "Training loss: 1.4208781874815277....\n",
      "Validation loss: 1.4663877454668892....\n",
      "-----------------------------------\n",
      "Training loss: 1.4186566594273817....\n",
      "Validation loss: 1.4642578256654342....\n",
      "-----------------------------------\n",
      "Training loss: 1.4164384809385993....\n",
      "Validation loss: 1.4621327139196238....\n",
      "-----------------------------------\n",
      "Training loss: 1.4142239856346963....\n",
      "Validation loss: 1.4600138256628352....\n",
      "-----------------------------------\n",
      "Training loss: 1.4120139888387688....\n",
      "Validation loss: 1.4579001383038066....\n",
      "-----------------------------------\n",
      "Training loss: 1.4098090919616963....\n",
      "Validation loss: 1.4557895613509164....\n",
      "-----------------------------------\n",
      "Training loss: 1.4076064595831057....\n",
      "Validation loss: 1.4536820982901237....\n",
      "-----------------------------------\n",
      "Training loss: 1.4054069028249825....\n",
      "Validation loss: 1.4515792294377243....\n",
      "-----------------------------------\n",
      "Training loss: 1.4032108490705666....\n",
      "Validation loss: 1.4494814655222572....\n",
      "-----------------------------------\n",
      "Training loss: 1.4010191148382418....\n",
      "Validation loss: 1.447386559932981....\n",
      "-----------------------------------\n",
      "Training loss: 1.3988293648966108....\n",
      "Validation loss: 1.4452962241662781....\n",
      "-----------------------------------\n",
      "Training loss: 1.3966428629422674....\n",
      "Validation loss: 1.4432099857061986....\n",
      "-----------------------------------\n",
      "Training loss: 1.3944598728952062....\n",
      "Validation loss: 1.4411272849843186....\n",
      "-----------------------------------\n",
      "Training loss: 1.392280098435958....\n",
      "Validation loss: 1.4390497801819526....\n",
      "-----------------------------------\n",
      "Training loss: 1.3901033022536176....\n",
      "Validation loss: 1.4369769047847967....\n",
      "-----------------------------------\n",
      "Training loss: 1.3879297596383167....\n",
      "Validation loss: 1.4349090674985001....\n",
      "-----------------------------------\n",
      "Training loss: 1.3857595730671555....\n",
      "Validation loss: 1.4328442589543329....\n",
      "-----------------------------------\n",
      "Training loss: 1.3835938648825234....\n",
      "Validation loss: 1.4307838745084065....\n",
      "-----------------------------------\n",
      "Training loss: 1.3814324828865785....\n",
      "Validation loss: 1.428726932771683....\n",
      "-----------------------------------\n",
      "Training loss: 1.3792743745275227....\n",
      "Validation loss: 1.4266723299647064....\n",
      "-----------------------------------\n",
      "Training loss: 1.377119394261737....\n",
      "Validation loss: 1.424623470739342....\n",
      "-----------------------------------\n",
      "Training loss: 1.374969540534665....\n",
      "Validation loss: 1.4225802860858854....\n",
      "-----------------------------------\n",
      "Training loss: 1.372824644495071....\n",
      "Validation loss: 1.4205422522165723....\n",
      "-----------------------------------\n",
      "Training loss: 1.3706855271091685....\n",
      "Validation loss: 1.4185077616474047....\n",
      "-----------------------------------\n",
      "Training loss: 1.368551262025945....\n",
      "Validation loss: 1.4164775731508683....\n",
      "-----------------------------------\n",
      "Training loss: 1.3664211247256008....\n",
      "Validation loss: 1.4144505956758702....\n",
      "-----------------------------------\n",
      "Training loss: 1.3642960632672119....\n",
      "Validation loss: 1.4124270439778697....\n",
      "-----------------------------------\n",
      "Training loss: 1.3621754603288962....\n",
      "Validation loss: 1.4104079837257195....\n",
      "-----------------------------------\n",
      "Training loss: 1.360059302156875....\n",
      "Validation loss: 1.4083924627740487....\n",
      "-----------------------------------\n",
      "Training loss: 1.3579464584062473....\n",
      "Validation loss: 1.4063801411219106....\n",
      "-----------------------------------\n",
      "Training loss: 1.3558375979015775....\n",
      "Validation loss: 1.4043716264876134....\n",
      "-----------------------------------\n",
      "Training loss: 1.3537344020869826....\n",
      "Validation loss: 1.4023672187433944....\n",
      "-----------------------------------\n",
      "Training loss: 1.3516359633503212....\n",
      "Validation loss: 1.4003663794650583....\n",
      "-----------------------------------\n",
      "Training loss: 1.3495406552615736....\n",
      "Validation loss: 1.3983685158741443....\n",
      "-----------------------------------\n",
      "Training loss: 1.3474487731649953....\n",
      "Validation loss: 1.3963737665954616....\n",
      "-----------------------------------\n",
      "Training loss: 1.3453606563325293....\n",
      "Validation loss: 1.3943835916183211....\n",
      "-----------------------------------\n",
      "Training loss: 1.343277833367692....\n",
      "Validation loss: 1.3923979615216677....\n",
      "-----------------------------------\n",
      "Training loss: 1.3411992334654512....\n",
      "Validation loss: 1.3904160960723935....\n",
      "-----------------------------------\n",
      "Training loss: 1.3391241380858854....\n",
      "Validation loss: 1.388438147397104....\n",
      "-----------------------------------\n",
      "Training loss: 1.3370537084698968....\n",
      "Validation loss: 1.3864646378684355....\n",
      "-----------------------------------\n",
      "Training loss: 1.334988524751052....\n",
      "Validation loss: 1.3844952380024136....\n",
      "-----------------------------------\n",
      "Training loss: 1.3329273171198388....\n",
      "Validation loss: 1.382528487902944....\n",
      "-----------------------------------\n",
      "Training loss: 1.3308700092097552....\n",
      "Validation loss: 1.3805655008840885....\n",
      "-----------------------------------\n",
      "Training loss: 1.3288166257904714....\n",
      "Validation loss: 1.3786069136689243....\n",
      "-----------------------------------\n",
      "Training loss: 1.3267670569941024....\n",
      "Validation loss: 1.3766528092045753....\n",
      "-----------------------------------\n",
      "Training loss: 1.3247205017315788....\n",
      "Validation loss: 1.3747026138790952....\n",
      "-----------------------------------\n",
      "Training loss: 1.322676675180243....\n",
      "Validation loss: 1.3727561185101314....\n",
      "-----------------------------------\n",
      "Training loss: 1.3206370020082896....\n",
      "Validation loss: 1.370813415624988....\n",
      "-----------------------------------\n",
      "Training loss: 1.3186017336555245....\n",
      "Validation loss: 1.3688749091075705....\n",
      "-----------------------------------\n",
      "Training loss: 1.3165698526090557....\n",
      "Validation loss: 1.3669398855345951....\n",
      "-----------------------------------\n",
      "Training loss: 1.3145418714420412....\n",
      "Validation loss: 1.365009648882873....\n",
      "-----------------------------------\n",
      "Training loss: 1.3125197279147005....\n",
      "Validation loss: 1.3630840730657945....\n",
      "-----------------------------------\n",
      "Training loss: 1.3105012919762575....\n",
      "Validation loss: 1.3611635420122226....\n",
      "-----------------------------------\n",
      "Training loss: 1.3084863487337919....\n",
      "Validation loss: 1.3592475081503719....\n",
      "-----------------------------------\n",
      "Training loss: 1.3064747479071073....\n",
      "Validation loss: 1.357335874596668....\n",
      "-----------------------------------\n",
      "Training loss: 1.3044684255480146....\n",
      "Validation loss: 1.355425451019698....\n",
      "-----------------------------------\n",
      "Training loss: 1.3024672450169437....\n",
      "Validation loss: 1.3535190980296903....\n",
      "-----------------------------------\n",
      "Training loss: 1.300471013990509....\n",
      "Validation loss: 1.3516170250747954....\n",
      "-----------------------------------\n",
      "Training loss: 1.2984794174419507....\n",
      "Validation loss: 1.3497188742605843....\n",
      "-----------------------------------\n",
      "Training loss: 1.2964920137711407....\n",
      "Validation loss: 1.3478246067156983....\n",
      "-----------------------------------\n",
      "Training loss: 1.2945091544225151....\n",
      "Validation loss: 1.34593295532731....\n",
      "-----------------------------------\n",
      "Training loss: 1.2925307760898863....\n",
      "Validation loss: 1.3440450387290916....\n",
      "-----------------------------------\n",
      "Training loss: 1.2905563723204987....\n",
      "Validation loss: 1.342161849658789....\n",
      "-----------------------------------\n",
      "Training loss: 1.2885850814511244....\n",
      "Validation loss: 1.3402833650306853....\n",
      "-----------------------------------\n",
      "Training loss: 1.2866169895290829....\n",
      "Validation loss: 1.338408564065923....\n",
      "-----------------------------------\n",
      "Training loss: 1.2846545066968282....\n",
      "Validation loss: 1.336538189971167....\n",
      "-----------------------------------\n",
      "Training loss: 1.282698447089302....\n",
      "Validation loss: 1.3346717124765268....\n",
      "-----------------------------------\n",
      "Training loss: 1.2807462793400703....\n",
      "Validation loss: 1.3328104411659438....\n",
      "-----------------------------------\n",
      "Training loss: 1.2787989850361305....\n",
      "Validation loss: 1.3309539035875737....\n",
      "-----------------------------------\n",
      "Training loss: 1.2768565178601892....\n",
      "Validation loss: 1.329100432554495....\n",
      "-----------------------------------\n",
      "Training loss: 1.2749190239288193....\n",
      "Validation loss: 1.3272519759928192....\n",
      "-----------------------------------\n",
      "Training loss: 1.272986233991561....\n",
      "Validation loss: 1.3254064346076002....\n",
      "-----------------------------------\n",
      "Training loss: 1.2710566836030057....\n",
      "Validation loss: 1.3235641204193216....\n",
      "-----------------------------------\n",
      "Training loss: 1.2691299940682346....\n",
      "Validation loss: 1.3217262999016488....\n",
      "-----------------------------------\n",
      "Training loss: 1.2672081804507522....\n",
      "Validation loss: 1.3198934365643353....\n",
      "-----------------------------------\n",
      "Training loss: 1.2652904072272966....\n",
      "Validation loss: 1.3180663361956013....\n",
      "-----------------------------------\n",
      "Training loss: 1.263377744544983....\n",
      "Validation loss: 1.3162430294146361....\n",
      "-----------------------------------\n",
      "Training loss: 1.2614694230391343....\n",
      "Validation loss: 1.314424559964777....\n",
      "-----------------------------------\n",
      "Training loss: 1.2595669404510215....\n",
      "Validation loss: 1.3126116436589423....\n",
      "-----------------------------------\n",
      "Training loss: 1.2576705818305083....\n",
      "Validation loss: 1.3108039290937317....\n",
      "-----------------------------------\n",
      "Training loss: 1.2557794978889....\n",
      "Validation loss: 1.309001895757369....\n",
      "-----------------------------------\n",
      "Training loss: 1.2538931955376316....\n",
      "Validation loss: 1.307203446725232....\n",
      "-----------------------------------\n",
      "Training loss: 1.25201027412636....\n",
      "Validation loss: 1.305408483975827....\n",
      "-----------------------------------\n",
      "Training loss: 1.2501309216352217....\n",
      "Validation loss: 1.3036167338348648....\n",
      "-----------------------------------\n",
      "Training loss: 1.2482552415940262....\n",
      "Validation loss: 1.3018287131000703....\n",
      "-----------------------------------\n",
      "Training loss: 1.2463838040714237....\n",
      "Validation loss: 1.300045105374628....\n",
      "-----------------------------------\n",
      "Training loss: 1.2445162774956164....\n",
      "Validation loss: 1.2982659862149442....\n",
      "-----------------------------------\n",
      "Training loss: 1.2426528185388845....\n",
      "Validation loss: 1.296491415495853....\n",
      "-----------------------------------\n",
      "Training loss: 1.2407947430622503....\n",
      "Validation loss: 1.294719678507438....\n",
      "-----------------------------------\n",
      "Training loss: 1.2389415298173885....\n",
      "Validation loss: 1.2929513009482971....\n",
      "-----------------------------------\n",
      "Training loss: 1.2370923884207055....\n",
      "Validation loss: 1.2911874348318038....\n",
      "-----------------------------------\n",
      "Training loss: 1.2352475382847723....\n",
      "Validation loss: 1.2894270453252556....\n",
      "-----------------------------------\n",
      "Training loss: 1.23340587513406....\n",
      "Validation loss: 1.2876702322755982....\n",
      "-----------------------------------\n",
      "Training loss: 1.2315691945447702....\n",
      "Validation loss: 1.285917810425074....\n",
      "-----------------------------------\n",
      "Training loss: 1.2297366250922586....\n",
      "Validation loss: 1.2841691597272542....\n",
      "-----------------------------------\n",
      "Training loss: 1.2279080208088804....\n",
      "Validation loss: 1.2824251127044985....\n",
      "-----------------------------------\n",
      "Training loss: 1.2260833098161668....\n",
      "Validation loss: 1.2806860156321063....\n",
      "-----------------------------------\n",
      "Training loss: 1.2242631494608798....\n",
      "Validation loss: 1.2789507247209226....\n",
      "-----------------------------------\n",
      "Training loss: 1.222447166070836....\n",
      "Validation loss: 1.277218515865331....\n",
      "-----------------------------------\n",
      "Training loss: 1.220635484657531....\n",
      "Validation loss: 1.2754907797980346....\n",
      "-----------------------------------\n",
      "Training loss: 1.2188292218674397....\n",
      "Validation loss: 1.2737674958459853....\n",
      "-----------------------------------\n",
      "Training loss: 1.2170286937595807....\n",
      "Validation loss: 1.2720485155673158....\n",
      "-----------------------------------\n",
      "Training loss: 1.2152341814326324....\n",
      "Validation loss: 1.2703349826450805....\n",
      "-----------------------------------\n",
      "Training loss: 1.213444740222435....\n",
      "Validation loss: 1.2686250188990913....\n",
      "-----------------------------------\n",
      "Training loss: 1.211659226059084....\n",
      "Validation loss: 1.2669184916567362....\n",
      "-----------------------------------\n",
      "Training loss: 1.2098779925203629....\n",
      "Validation loss: 1.2652164714805887....\n",
      "-----------------------------------\n",
      "Training loss: 1.2081009823343931....\n",
      "Validation loss: 1.2635179357585138....\n",
      "-----------------------------------\n",
      "Training loss: 1.2063277029142052....\n",
      "Validation loss: 1.2618233668544225....\n",
      "-----------------------------------\n",
      "Training loss: 1.2045584833599545....\n",
      "Validation loss: 1.2601331133594287....\n",
      "-----------------------------------\n",
      "Training loss: 1.2027932623302031....\n",
      "Validation loss: 1.2584468914564841....\n",
      "-----------------------------------\n",
      "Training loss: 1.2010319235354523....\n",
      "Validation loss: 1.256765021974273....\n",
      "-----------------------------------\n",
      "Training loss: 1.1992744403719806....\n",
      "Validation loss: 1.2550873957242625....\n",
      "-----------------------------------\n",
      "Training loss: 1.1975220100672226....\n",
      "Validation loss: 1.253414796751717....\n",
      "-----------------------------------\n",
      "Training loss: 1.1957739262440545....\n",
      "Validation loss: 1.251745164403766....\n",
      "-----------------------------------\n",
      "Training loss: 1.1940289783101399....\n",
      "Validation loss: 1.250079911175554....\n",
      "-----------------------------------\n",
      "Training loss: 1.192289353461977....\n",
      "Validation loss: 1.2484193100745702....\n",
      "-----------------------------------\n",
      "Training loss: 1.190554463299608....\n",
      "Validation loss: 1.2467615337254014....\n",
      "-----------------------------------\n",
      "Training loss: 1.1888235526795035....\n",
      "Validation loss: 1.2451080113206923....\n",
      "-----------------------------------\n",
      "Training loss: 1.1870974207226046....\n",
      "Validation loss: 1.2434583514904387....\n",
      "-----------------------------------\n",
      "Training loss: 1.1853755562547823....\n",
      "Validation loss: 1.2418132606252548....\n",
      "-----------------------------------\n",
      "Training loss: 1.1836585061669878....\n",
      "Validation loss: 1.2401718573378238....\n",
      "-----------------------------------\n",
      "Training loss: 1.1819461729774836....\n",
      "Validation loss: 1.2385352262524154....\n",
      "-----------------------------------\n",
      "Training loss: 1.1802383364149174....\n",
      "Validation loss: 1.236904326833295....\n",
      "-----------------------------------\n",
      "Training loss: 1.1785358271355195....\n",
      "Validation loss: 1.2352776934281915....\n",
      "-----------------------------------\n",
      "Training loss: 1.1768379391139903....\n",
      "Validation loss: 1.2336551856094398....\n",
      "-----------------------------------\n",
      "Training loss: 1.1751443054778474....\n",
      "Validation loss: 1.2320370499097313....\n",
      "-----------------------------------\n",
      "Training loss: 1.1734550397619796....\n",
      "Validation loss: 1.230424215176904....\n",
      "-----------------------------------\n",
      "Training loss: 1.1717714070890157....\n",
      "Validation loss: 1.2288145478995751....\n",
      "-----------------------------------\n",
      "Training loss: 1.170093030888944....\n",
      "Validation loss: 1.2272100938170605....\n",
      "-----------------------------------\n",
      "Training loss: 1.1684200041730288....\n",
      "Validation loss: 1.2256100454426395....\n",
      "-----------------------------------\n",
      "Training loss: 1.1667513665973974....\n",
      "Validation loss: 1.2240141786062804....\n",
      "-----------------------------------\n",
      "Training loss: 1.1650875792541586....\n",
      "Validation loss: 1.2224217358822544....\n",
      "-----------------------------------\n",
      "Training loss: 1.1634285206249761....\n",
      "Validation loss: 1.2208334163996661....\n",
      "-----------------------------------\n",
      "Training loss: 1.1617743908585945....\n",
      "Validation loss: 1.2192484578514777....\n",
      "-----------------------------------\n",
      "Training loss: 1.160123966247669....\n",
      "Validation loss: 1.2176673197455585....\n",
      "-----------------------------------\n",
      "Training loss: 1.158477266442204....\n",
      "Validation loss: 1.2160899189032877....\n",
      "-----------------------------------\n",
      "Training loss: 1.15683446966524....\n",
      "Validation loss: 1.2145169716463238....\n",
      "-----------------------------------\n",
      "Training loss: 1.155196356611776....\n",
      "Validation loss: 1.2129486227206598....\n",
      "-----------------------------------\n",
      "Training loss: 1.1535627095862786....\n",
      "Validation loss: 1.2113838028812283....\n",
      "-----------------------------------\n",
      "Training loss: 1.1519334199611884....\n",
      "Validation loss: 1.2098226389056947....\n",
      "-----------------------------------\n",
      "Training loss: 1.1503089964204032....\n",
      "Validation loss: 1.2082652075218774....\n",
      "-----------------------------------\n",
      "Training loss: 1.1486889112959193....\n",
      "Validation loss: 1.2067117238127039....\n",
      "-----------------------------------\n",
      "Training loss: 1.1470728659354061....\n",
      "Validation loss: 1.2051624239389735....\n",
      "-----------------------------------\n",
      "Training loss: 1.1454609639438542....\n",
      "Validation loss: 1.203617774796833....\n",
      "-----------------------------------\n",
      "Training loss: 1.1438533648037021....\n",
      "Validation loss: 1.2020765296940676....\n",
      "-----------------------------------\n",
      "Training loss: 1.142249322742158....\n",
      "Validation loss: 1.2005398036087485....\n",
      "-----------------------------------\n",
      "Training loss: 1.1406503202275589....\n",
      "Validation loss: 1.1990059473410422....\n",
      "-----------------------------------\n",
      "Training loss: 1.1390553514158335....\n",
      "Validation loss: 1.1974762909928123....\n",
      "-----------------------------------\n",
      "Training loss: 1.1374653512910473....\n",
      "Validation loss: 1.195951288132161....\n",
      "-----------------------------------\n",
      "Training loss: 1.1358802902652958....\n",
      "Validation loss: 1.194431357508399....\n",
      "-----------------------------------\n",
      "Training loss: 1.134300629663183....\n",
      "Validation loss: 1.1929155165474494....\n",
      "-----------------------------------\n",
      "Training loss: 1.132725515258826....\n",
      "Validation loss: 1.1914037394373773....\n",
      "-----------------------------------\n",
      "Training loss: 1.1311547048138495....\n",
      "Validation loss: 1.1898956504166882....\n",
      "-----------------------------------\n",
      "Training loss: 1.1295879686683552....\n",
      "Validation loss: 1.188391357314879....\n",
      "-----------------------------------\n",
      "Training loss: 1.1280246511102763....\n",
      "Validation loss: 1.186890129777861....\n",
      "-----------------------------------\n",
      "Training loss: 1.126465092536208....\n",
      "Validation loss: 1.1853926916220172....\n",
      "-----------------------------------\n",
      "Training loss: 1.1249095941851635....\n",
      "Validation loss: 1.1838986464411432....\n",
      "-----------------------------------\n",
      "Training loss: 1.123358300462851....\n",
      "Validation loss: 1.1824093769957704....\n",
      "-----------------------------------\n",
      "Training loss: 1.1218122006242979....\n",
      "Validation loss: 1.1809241338213414....\n",
      "-----------------------------------\n",
      "Training loss: 1.1202702670533535....\n",
      "Validation loss: 1.1794423726513237....\n",
      "-----------------------------------\n",
      "Training loss: 1.118731539617771....\n",
      "Validation loss: 1.177963885870225....\n",
      "-----------------------------------\n",
      "Training loss: 1.117195798682477....\n",
      "Validation loss: 1.176488954428246....\n",
      "-----------------------------------\n",
      "Training loss: 1.1156636511408093....\n",
      "Validation loss: 1.1750178035012278....\n",
      "-----------------------------------\n",
      "Training loss: 1.1141354145822058....\n",
      "Validation loss: 1.1735506116207766....\n",
      "-----------------------------------\n",
      "Training loss: 1.112611702286465....\n",
      "Validation loss: 1.1720874718944545....\n",
      "-----------------------------------\n",
      "Training loss: 1.1110921134580958....\n",
      "Validation loss: 1.1706281243496794....\n",
      "-----------------------------------\n",
      "Training loss: 1.1095761983381884....\n",
      "Validation loss: 1.1691725272521531....\n",
      "-----------------------------------\n",
      "Training loss: 1.1080642126089353....\n",
      "Validation loss: 1.1677211215528924....\n",
      "-----------------------------------\n",
      "Training loss: 1.1065559513821566....\n",
      "Validation loss: 1.1662731945433444....\n",
      "-----------------------------------\n",
      "Training loss: 1.1050513829468656....\n",
      "Validation loss: 1.1648300560262217....\n",
      "-----------------------------------\n",
      "Training loss: 1.103551420500451....\n",
      "Validation loss: 1.1633914336603761....\n",
      "-----------------------------------\n",
      "Training loss: 1.1020554324378569....\n",
      "Validation loss: 1.1619568619571914....\n",
      "-----------------------------------\n",
      "Training loss: 1.1005631445345192....\n",
      "Validation loss: 1.160526339317098....\n",
      "-----------------------------------\n",
      "Training loss: 1.0990743604825735....\n",
      "Validation loss: 1.1590999266716067....\n",
      "-----------------------------------\n",
      "Training loss: 1.0975897063382143....\n",
      "Validation loss: 1.157677808763404....\n",
      "-----------------------------------\n",
      "Training loss: 1.0961099111214685....\n",
      "Validation loss: 1.1562602337448433....\n",
      "-----------------------------------\n",
      "Training loss: 1.0946341477715935....\n",
      "Validation loss: 1.1548463489740866....\n",
      "-----------------------------------\n",
      "Training loss: 1.0931621878046012....\n",
      "Validation loss: 1.153436216816397....\n",
      "-----------------------------------\n",
      "Training loss: 1.0916942528796585....\n",
      "Validation loss: 1.1520301865857057....\n",
      "-----------------------------------\n",
      "Training loss: 1.0902302329972293....\n",
      "Validation loss: 1.1506279629137575....\n",
      "-----------------------------------\n",
      "Training loss: 1.088769643504856....\n",
      "Validation loss: 1.1492294154926195....\n",
      "-----------------------------------\n",
      "Training loss: 1.0873132923927973....\n",
      "Validation loss: 1.1478347497985077....\n",
      "-----------------------------------\n",
      "Training loss: 1.0858607719156852....\n",
      "Validation loss: 1.1464442731395081....\n",
      "-----------------------------------\n",
      "Training loss: 1.0844123204696527....\n",
      "Validation loss: 1.145057789804593....\n",
      "-----------------------------------\n",
      "Training loss: 1.0829680901482692....\n",
      "Validation loss: 1.1436748470230738....\n",
      "-----------------------------------\n",
      "Training loss: 1.0815272184873665....\n",
      "Validation loss: 1.1422959098770313....\n",
      "-----------------------------------\n",
      "Training loss: 1.0800901213948344....\n",
      "Validation loss: 1.1409205532840865....\n",
      "-----------------------------------\n",
      "Training loss: 1.0786565761194593....\n",
      "Validation loss: 1.1395479620573037....\n",
      "-----------------------------------\n",
      "Training loss: 1.077225801253919....\n",
      "Validation loss: 1.1381784772383805....\n",
      "-----------------------------------\n",
      "Training loss: 1.0757986824082173....\n",
      "Validation loss: 1.1368129234630584....\n",
      "-----------------------------------\n",
      "Training loss: 1.0743753926273583....\n",
      "Validation loss: 1.1354503279511072....\n",
      "-----------------------------------\n",
      "Training loss: 1.0729559923833454....\n",
      "Validation loss: 1.1340916262084424....\n",
      "-----------------------------------\n",
      "Training loss: 1.0715411769104526....\n",
      "Validation loss: 1.1327364621843596....\n",
      "-----------------------------------\n",
      "Training loss: 1.0701304426973484....\n",
      "Validation loss: 1.131385239742875....\n",
      "-----------------------------------\n",
      "Training loss: 1.0687237414908186....\n",
      "Validation loss: 1.1300380851470655....\n",
      "-----------------------------------\n",
      "Training loss: 1.0673207355869032....\n",
      "Validation loss: 1.128694801718524....\n",
      "-----------------------------------\n",
      "Training loss: 1.0659215622088245....\n",
      "Validation loss: 1.127355664241316....\n",
      "-----------------------------------\n",
      "Training loss: 1.0645259515833079....\n",
      "Validation loss: 1.1260199650988127....\n",
      "-----------------------------------\n",
      "Training loss: 1.0631337067424502....\n",
      "Validation loss: 1.1246879450061302....\n",
      "-----------------------------------\n",
      "Training loss: 1.061745660401854....\n",
      "Validation loss: 1.1233599287583402....\n",
      "-----------------------------------\n",
      "Training loss: 1.060361661447201....\n",
      "Validation loss: 1.12203445454325....\n",
      "-----------------------------------\n",
      "Training loss: 1.0589810098008399....\n",
      "Validation loss: 1.1207126937946572....\n",
      "-----------------------------------\n",
      "Training loss: 1.0576039527180818....\n",
      "Validation loss: 1.1193938020749514....\n",
      "-----------------------------------\n",
      "Training loss: 1.0562303504101929....\n",
      "Validation loss: 1.1180788592648474....\n",
      "-----------------------------------\n",
      "Training loss: 1.0548603611138803....\n",
      "Validation loss: 1.1167675855139096....\n",
      "-----------------------------------\n",
      "Training loss: 1.0534934932139566....\n",
      "Validation loss: 1.1154595341014648....\n",
      "-----------------------------------\n",
      "Training loss: 1.0521300822308872....\n",
      "Validation loss: 1.1141553002425448....\n",
      "-----------------------------------\n",
      "Training loss: 1.0507703746566708....\n",
      "Validation loss: 1.1128537206685116....\n",
      "-----------------------------------\n",
      "Training loss: 1.0494136388897715....\n",
      "Validation loss: 1.1115545691018807....\n",
      "-----------------------------------\n",
      "Training loss: 1.0480598606011422....\n",
      "Validation loss: 1.1102584151988986....\n",
      "-----------------------------------\n",
      "Training loss: 1.0467095798809878....\n",
      "Validation loss: 1.108965391587656....\n",
      "-----------------------------------\n",
      "Training loss: 1.045362976217307....\n",
      "Validation loss: 1.1076763512095582....\n",
      "-----------------------------------\n",
      "Training loss: 1.044020549800587....\n",
      "Validation loss: 1.1063912511427771....\n",
      "-----------------------------------\n",
      "Training loss: 1.0426817954985816....\n",
      "Validation loss: 1.1051087894173024....\n",
      "-----------------------------------\n",
      "Training loss: 1.0413450193105211....\n",
      "Validation loss: 1.1038289456681711....\n",
      "-----------------------------------\n",
      "Training loss: 1.04001124768314....\n",
      "Validation loss: 1.1025525013525932....\n",
      "-----------------------------------\n",
      "Training loss: 1.038680466919805....\n",
      "Validation loss: 1.1012796959884512....\n",
      "-----------------------------------\n",
      "Training loss: 1.0373528528384375....\n",
      "Validation loss: 1.1000111578854344....\n",
      "-----------------------------------\n",
      "Training loss: 1.036029387143843....\n",
      "Validation loss: 1.098745745996861....\n",
      "-----------------------------------\n",
      "Training loss: 1.0347094730904287....\n",
      "Validation loss: 1.0974841395344224....\n",
      "-----------------------------------\n",
      "Training loss: 1.0333935053453218....\n",
      "Validation loss: 1.0962256520105815....\n",
      "-----------------------------------\n",
      "Training loss: 1.0320815406633286....\n",
      "Validation loss: 1.094970223615012....\n",
      "-----------------------------------\n",
      "Training loss: 1.030773251882925....\n",
      "Validation loss: 1.0937185734481354....\n",
      "-----------------------------------\n",
      "Training loss: 1.0294684877549558....\n",
      "Validation loss: 1.092470596824369....\n",
      "-----------------------------------\n",
      "Training loss: 1.0281672424462847....\n",
      "Validation loss: 1.0912258361750522....\n",
      "-----------------------------------\n",
      "Training loss: 1.0268682180003352....\n",
      "Validation loss: 1.0899842766203567....\n",
      "-----------------------------------\n",
      "Training loss: 1.0255722670798062....\n",
      "Validation loss: 1.0887465047268903....\n",
      "-----------------------------------\n",
      "Training loss: 1.0242798150655716....\n",
      "Validation loss: 1.087511710490208....\n",
      "-----------------------------------\n",
      "Training loss: 1.02299073922425....\n",
      "Validation loss: 1.086279991547457....\n",
      "-----------------------------------\n",
      "Training loss: 1.0217048139025053....\n",
      "Validation loss: 1.0850511675457308....\n",
      "-----------------------------------\n",
      "Training loss: 1.020422336491807....\n",
      "Validation loss: 1.0838249780866334....\n",
      "-----------------------------------\n",
      "Training loss: 1.0191431647120317....\n",
      "Validation loss: 1.0826020073112432....\n",
      "-----------------------------------\n",
      "Training loss: 1.017866939894869....\n",
      "Validation loss: 1.081381446200549....\n",
      "-----------------------------------\n",
      "Training loss: 1.01659311149827....\n",
      "Validation loss: 1.0801637859005322....\n",
      "-----------------------------------\n",
      "Training loss: 1.0153222092445038....\n",
      "Validation loss: 1.0789490998191213....\n",
      "-----------------------------------\n",
      "Training loss: 1.014054167979298....\n",
      "Validation loss: 1.07773788318058....\n",
      "-----------------------------------\n",
      "Training loss: 1.0127894438857683....\n",
      "Validation loss: 1.0765301288527007....\n",
      "-----------------------------------\n",
      "Training loss: 1.0115282489073267....\n",
      "Validation loss: 1.0753260223098275....\n",
      "-----------------------------------\n",
      "Training loss: 1.010270647753136....\n",
      "Validation loss: 1.074124664377915....\n",
      "-----------------------------------\n",
      "Training loss: 1.0090158979072466....\n",
      "Validation loss: 1.0729266613927637....\n",
      "-----------------------------------\n",
      "Training loss: 1.0077643924286557....\n",
      "Validation loss: 1.0717316320851995....\n",
      "-----------------------------------\n",
      "Training loss: 1.0065161151789448....\n",
      "Validation loss: 1.0705398652563787....\n",
      "-----------------------------------\n",
      "Training loss: 1.0052710732029928....\n",
      "Validation loss: 1.0693516433808694....\n",
      "-----------------------------------\n",
      "Training loss: 1.0040295393314707....\n",
      "Validation loss: 1.0681664625861176....\n",
      "-----------------------------------\n",
      "Training loss: 1.0027911042366278....\n",
      "Validation loss: 1.0669842991543579....\n",
      "-----------------------------------\n",
      "Training loss: 1.001555941186823....\n",
      "Validation loss: 1.0658056321580782....\n",
      "-----------------------------------\n",
      "Training loss: 1.0003244575635482....\n",
      "Validation loss: 1.0646300441185323....\n",
      "-----------------------------------\n",
      "Training loss: 0.9990961311376982....\n",
      "Validation loss: 1.0634576030351677....\n",
      "-----------------------------------\n",
      "Training loss: 0.9978710161588679....\n",
      "Validation loss: 1.0622883915423926....\n",
      "-----------------------------------\n",
      "Training loss: 0.9966491721149063....\n",
      "Validation loss: 1.0611220919791915....\n",
      "-----------------------------------\n",
      "Training loss: 0.9954304441426287....\n",
      "Validation loss: 1.059958580885396....\n",
      "-----------------------------------\n",
      "Training loss: 0.9942146561746688....\n",
      "Validation loss: 1.0587980606142324....\n",
      "-----------------------------------\n",
      "Training loss: 0.9930018197711298....\n",
      "Validation loss: 1.057640387656577....\n",
      "-----------------------------------\n",
      "Training loss: 0.9917917607967783....\n",
      "Validation loss: 1.0564855053441538....\n",
      "-----------------------------------\n",
      "Training loss: 0.9905845174998648....\n",
      "Validation loss: 1.0553336512385028....\n",
      "-----------------------------------\n",
      "Training loss: 0.9893801018192293....\n",
      "Validation loss: 1.054184860178952....\n",
      "-----------------------------------\n",
      "Training loss: 0.9881785120036816....\n",
      "Validation loss: 1.0530396062901026....\n",
      "-----------------------------------\n",
      "Training loss: 0.9869803165926933....\n",
      "Validation loss: 1.051897389968792....\n",
      "-----------------------------------\n",
      "Training loss: 0.9857846874099687....\n",
      "Validation loss: 1.0507583544610821....\n",
      "-----------------------------------\n",
      "Training loss: 0.9845919292292634....\n",
      "Validation loss: 1.0496218445207555....\n",
      "-----------------------------------\n",
      "Training loss: 0.983401758274404....\n",
      "Validation loss: 1.0484885453461348....\n",
      "-----------------------------------\n",
      "Training loss: 0.9822146889268406....\n",
      "Validation loss: 1.047358307764745....\n",
      "-----------------------------------\n",
      "Training loss: 0.9810305748674416....\n",
      "Validation loss: 1.0462310501406868....\n",
      "-----------------------------------\n",
      "Training loss: 0.9798493929061673....\n",
      "Validation loss: 1.0451063732347068....\n",
      "-----------------------------------\n",
      "Training loss: 0.9786713901049696....\n",
      "Validation loss: 1.0439845515493409....\n",
      "-----------------------------------\n",
      "Training loss: 0.977496696074711....\n",
      "Validation loss: 1.0428654793145613....\n",
      "-----------------------------------\n",
      "Training loss: 0.9763250218381975....\n",
      "Validation loss: 1.0417494075385463....\n",
      "-----------------------------------\n",
      "Training loss: 0.9751565299410115....\n",
      "Validation loss: 1.0406365113714746....\n",
      "-----------------------------------\n",
      "Training loss: 0.9739910193739872....\n",
      "Validation loss: 1.0395267919587312....\n",
      "-----------------------------------\n",
      "Training loss: 0.9728284637182069....\n",
      "Validation loss: 1.038419903935586....\n",
      "-----------------------------------\n",
      "Training loss: 0.9716685201472679....\n",
      "Validation loss: 1.0373162621478271....\n",
      "-----------------------------------\n",
      "Training loss: 0.9705119428629564....\n",
      "Validation loss: 1.0362157614907777....\n",
      "-----------------------------------\n",
      "Training loss: 0.9693586366407762....\n",
      "Validation loss: 1.0351185204342075....\n",
      "-----------------------------------\n",
      "Training loss: 0.9682084821983892....\n",
      "Validation loss: 1.034024213370097....\n",
      "-----------------------------------\n",
      "Training loss: 0.9670613461274409....\n",
      "Validation loss: 1.03293311287124....\n",
      "-----------------------------------\n",
      "Training loss: 0.9659174259176093....\n",
      "Validation loss: 1.0318446606380145....\n",
      "-----------------------------------\n",
      "Training loss: 0.9647765576180687....\n",
      "Validation loss: 1.0307594650510483....\n",
      "-----------------------------------\n",
      "Training loss: 0.9636391033214887....\n",
      "Validation loss: 1.0296770331672649....\n",
      "-----------------------------------\n",
      "Training loss: 0.9625046786862794....\n",
      "Validation loss: 1.0285978525807788....\n",
      "-----------------------------------\n",
      "Training loss: 0.9613730671002704....\n",
      "Validation loss: 1.0275213160847612....\n",
      "-----------------------------------\n",
      "Training loss: 0.9602444240435745....\n",
      "Validation loss: 1.0264475952037169....\n",
      "-----------------------------------\n",
      "Training loss: 0.9591189097655669....\n",
      "Validation loss: 1.025376959104824....\n",
      "-----------------------------------\n",
      "Training loss: 0.9579966528750157....\n",
      "Validation loss: 1.0243090637465593....\n",
      "-----------------------------------\n",
      "Training loss: 0.9568772173103179....\n",
      "Validation loss: 1.0232435096798174....\n",
      "-----------------------------------\n",
      "Training loss: 0.955760250722452....\n",
      "Validation loss: 1.0221811556784584....\n",
      "-----------------------------------\n",
      "Training loss: 0.9546461415076609....\n",
      "Validation loss: 1.021121853655973....\n",
      "-----------------------------------\n",
      "Training loss: 0.9535347600581732....\n",
      "Validation loss: 1.0200651910674794....\n",
      "-----------------------------------\n",
      "Training loss: 0.9524256647396837....\n",
      "Validation loss: 1.0190113121330746....\n",
      "-----------------------------------\n",
      "Training loss: 0.9513192184297792....\n",
      "Validation loss: 1.0179603233194985....\n",
      "-----------------------------------\n",
      "Training loss: 0.9502159302602892....\n",
      "Validation loss: 1.0169120047133233....\n",
      "-----------------------------------\n",
      "Training loss: 0.9491153245682261....\n",
      "Validation loss: 1.015866301627937....\n",
      "-----------------------------------\n",
      "Training loss: 0.948017227710188....\n",
      "Validation loss: 1.0148231184698326....\n",
      "-----------------------------------\n",
      "Training loss: 0.9469218364079545....\n",
      "Validation loss: 1.0137827009514024....\n",
      "-----------------------------------\n",
      "Training loss: 0.9458294582721819....\n",
      "Validation loss: 1.0127449069413887....\n",
      "-----------------------------------\n",
      "Training loss: 0.9447398692580542....\n",
      "Validation loss: 1.0117100565317076....\n",
      "-----------------------------------\n",
      "Training loss: 0.9436534456031921....\n",
      "Validation loss: 1.010677480922728....\n",
      "-----------------------------------\n",
      "Training loss: 0.9425701213423442....\n",
      "Validation loss: 1.009647841949113....\n",
      "-----------------------------------\n",
      "Training loss: 0.9414897126720151....\n",
      "Validation loss: 1.0086207498445878....\n",
      "-----------------------------------\n",
      "Training loss: 0.9404119459377418....\n",
      "Validation loss: 1.0075965066848207....\n",
      "-----------------------------------\n",
      "Training loss: 0.9393371059948807....\n",
      "Validation loss: 1.0065747105357266....\n",
      "-----------------------------------\n",
      "Training loss: 0.9382644575689636....\n",
      "Validation loss: 1.005555476026721....\n",
      "-----------------------------------\n",
      "Training loss: 0.9371942685246345....\n",
      "Validation loss: 1.004538926402561....\n",
      "-----------------------------------\n",
      "Training loss: 0.9361268164675145....\n",
      "Validation loss: 1.0035250621775966....\n",
      "-----------------------------------\n",
      "Training loss: 0.9350615121357997....\n",
      "Validation loss: 1.0025137602802823....\n",
      "-----------------------------------\n",
      "Training loss: 0.9339985657110981....\n",
      "Validation loss: 1.0015049399166953....\n",
      "-----------------------------------\n",
      "Training loss: 0.9329381054686723....\n",
      "Validation loss: 1.0004987108358967....\n",
      "-----------------------------------\n",
      "Training loss: 0.9318801499867789....\n",
      "Validation loss: 0.99949517654481....\n",
      "-----------------------------------\n",
      "Training loss: 0.9308250707479817....\n",
      "Validation loss: 0.9984945415941351....\n",
      "-----------------------------------\n",
      "Training loss: 0.9297724968958753....\n",
      "Validation loss: 0.9974964806293081....\n",
      "-----------------------------------\n",
      "Training loss: 0.9287221030454055....\n",
      "Validation loss: 0.9965007604634397....\n",
      "-----------------------------------\n",
      "Training loss: 0.9276741952336166....\n",
      "Validation loss: 0.9955078390616636....\n",
      "-----------------------------------\n",
      "Training loss: 0.9266290969918106....\n",
      "Validation loss: 0.9945174063605123....\n",
      "-----------------------------------\n",
      "Training loss: 0.9255865226000519....\n",
      "Validation loss: 0.9935293389422425....\n",
      "-----------------------------------\n",
      "Training loss: 0.9245465793021923....\n",
      "Validation loss: 0.9925439254039293....\n",
      "-----------------------------------\n",
      "Training loss: 0.9235090996872612....\n",
      "Validation loss: 0.9915610358443723....\n",
      "-----------------------------------\n",
      "Training loss: 0.922474083616418....\n",
      "Validation loss: 0.9905807963925204....\n",
      "-----------------------------------\n",
      "Training loss: 0.9214416588185397....\n",
      "Validation loss: 0.9896029680241234....\n",
      "-----------------------------------\n",
      "Training loss: 0.920411303490843....\n",
      "Validation loss: 0.9886275959246905....\n",
      "-----------------------------------\n",
      "Training loss: 0.9193835242359704....\n",
      "Validation loss: 0.9876544173886361....\n",
      "-----------------------------------\n",
      "Training loss: 0.9183582855266941....\n",
      "Validation loss: 0.986683913515343....\n",
      "-----------------------------------\n",
      "Training loss: 0.9173357163280059....\n",
      "Validation loss: 0.9857160003755746....\n",
      "-----------------------------------\n",
      "Training loss: 0.916315615942385....\n",
      "Validation loss: 0.9847504550618007....\n",
      "-----------------------------------\n",
      "Training loss: 0.9152982803796734....\n",
      "Validation loss: 0.9837874568343449....\n",
      "-----------------------------------\n",
      "Training loss: 0.9142836253828837....\n",
      "Validation loss: 0.9828275042052628....\n",
      "-----------------------------------\n",
      "Training loss: 0.9132715208839557....\n",
      "Validation loss: 0.9818699664145477....\n",
      "-----------------------------------\n",
      "Training loss: 0.912262142867167....\n",
      "Validation loss: 0.9809145516869042....\n",
      "-----------------------------------\n",
      "Training loss: 0.9112552340938168....\n",
      "Validation loss: 0.979961597008467....\n",
      "-----------------------------------\n",
      "Training loss: 0.9102508457198187....\n",
      "Validation loss: 0.9790111299006037....\n",
      "-----------------------------------\n",
      "Training loss: 0.9092487501609754....\n",
      "Validation loss: 0.978062748735947....\n",
      "-----------------------------------\n",
      "Training loss: 0.9082487920050566....\n",
      "Validation loss: 0.9771167202215097....\n",
      "-----------------------------------\n",
      "Training loss: 0.9072514485287423....\n",
      "Validation loss: 0.9761725048793342....\n",
      "-----------------------------------\n",
      "Training loss: 0.9062559982210266....\n",
      "Validation loss: 0.9752307180075231....\n",
      "-----------------------------------\n",
      "Training loss: 0.9052629468021572....\n",
      "Validation loss: 0.9742910987917899....\n",
      "-----------------------------------\n",
      "Training loss: 0.9042720089906293....\n",
      "Validation loss: 0.9733538756105643....\n",
      "-----------------------------------\n",
      "Training loss: 0.9032838154167161....\n",
      "Validation loss: 0.9724193522647507....\n",
      "-----------------------------------\n",
      "Training loss: 0.902298436162081....\n",
      "Validation loss: 0.9714874426607127....\n",
      "-----------------------------------\n",
      "Training loss: 0.9013156337237945....\n",
      "Validation loss: 0.970557923663306....\n",
      "-----------------------------------\n",
      "Training loss: 0.9003352487690569....\n",
      "Validation loss: 0.9696309224354432....\n",
      "-----------------------------------\n",
      "Training loss: 0.899357292514661....\n",
      "Validation loss: 0.9687063441610951....\n",
      "-----------------------------------\n",
      "Training loss: 0.8983819856653209....\n",
      "Validation loss: 0.9677838327691225....\n",
      "-----------------------------------\n",
      "Training loss: 0.8974090700668031....\n",
      "Validation loss: 0.9668636639159449....\n",
      "-----------------------------------\n",
      "Training loss: 0.896438514259993....\n",
      "Validation loss: 0.9659460602414827....\n",
      "-----------------------------------\n",
      "Training loss: 0.8954705998378243....\n",
      "Validation loss: 0.9650306945535938....\n",
      "-----------------------------------\n",
      "Training loss: 0.8945048929654844....\n",
      "Validation loss: 0.9641180036184333....\n",
      "-----------------------------------\n",
      "Training loss: 0.8935419895515282....\n",
      "Validation loss: 0.9632073453397568....\n",
      "-----------------------------------\n",
      "Training loss: 0.8925814048950889....\n",
      "Validation loss: 0.962299169987079....\n",
      "-----------------------------------\n",
      "Training loss: 0.8916233206963597....\n",
      "Validation loss: 0.9613935479632283....\n",
      "-----------------------------------\n",
      "Training loss: 0.8906678633912217....\n",
      "Validation loss: 0.9604903787604528....\n",
      "-----------------------------------\n",
      "Training loss: 0.8897147568358232....\n",
      "Validation loss: 0.9595895317335765....\n",
      "-----------------------------------\n",
      "Training loss: 0.8887636122120623....\n",
      "Validation loss: 0.9586912774030258....\n",
      "-----------------------------------\n",
      "Training loss: 0.8878149794536291....\n",
      "Validation loss: 0.9577953502202428....\n",
      "-----------------------------------\n",
      "Training loss: 0.8868689358649305....\n",
      "Validation loss: 0.9569017895160608....\n",
      "-----------------------------------\n",
      "Training loss: 0.8859252255854707....\n",
      "Validation loss: 0.9560102237453978....\n",
      "-----------------------------------\n",
      "Training loss: 0.8849837902991871....\n",
      "Validation loss: 0.9551209617646791....\n",
      "-----------------------------------\n",
      "Training loss: 0.8840445949771039....\n",
      "Validation loss: 0.9542342414805824....\n",
      "-----------------------------------\n",
      "Training loss: 0.8831077927600866....\n",
      "Validation loss: 0.9533495716632374....\n",
      "-----------------------------------\n",
      "Training loss: 0.8821732072192684....\n",
      "Validation loss: 0.9524672429757197....\n",
      "-----------------------------------\n",
      "Training loss: 0.9061277927886665....\n",
      "Validation loss: 0.9018719455630997....\n",
      "-----------------------------------\n",
      "Training loss: 0.9051124628115439....\n",
      "Validation loss: 0.9010400001772507....\n",
      "-----------------------------------\n",
      "Training loss: 0.9041019645982703....\n",
      "Validation loss: 0.9002103282086946....\n",
      "-----------------------------------\n",
      "Training loss: 0.9030956923010495....\n",
      "Validation loss: 0.8993830173738....\n",
      "-----------------------------------\n",
      "Training loss: 0.9020932440973389....\n",
      "Validation loss: 0.898557931362821....\n",
      "-----------------------------------\n",
      "Training loss: 0.9010945356929505....\n",
      "Validation loss: 0.8977350776126666....\n",
      "-----------------------------------\n",
      "Training loss: 0.9000995785027713....\n",
      "Validation loss: 0.8969141764438859....\n",
      "-----------------------------------\n",
      "Training loss: 0.8991081901039061....\n",
      "Validation loss: 0.8960955738447978....\n",
      "-----------------------------------\n",
      "Training loss: 0.8981208948263958....\n",
      "Validation loss: 0.8952788963651906....\n",
      "-----------------------------------\n",
      "Training loss: 0.8971372649006158....\n",
      "Validation loss: 0.8944642541957772....\n",
      "-----------------------------------\n",
      "Training loss: 0.8961570501304189....\n",
      "Validation loss: 0.8936515722905956....\n",
      "-----------------------------------\n",
      "Training loss: 0.8951804814921692....\n",
      "Validation loss: 0.8928409658695373....\n",
      "-----------------------------------\n",
      "Training loss: 0.8942070218849332....\n",
      "Validation loss: 0.8920323617201262....\n",
      "-----------------------------------\n",
      "Training loss: 0.893236429931575....\n",
      "Validation loss: 0.8912257885926221....\n",
      "-----------------------------------\n",
      "Training loss: 0.8922688432150834....\n",
      "Validation loss: 0.8904216732759348....\n",
      "-----------------------------------\n",
      "Training loss: 0.8913041571930569....\n",
      "Validation loss: 0.8896192284593586....\n",
      "-----------------------------------\n",
      "Training loss: 0.8903417336173828....\n",
      "Validation loss: 0.8888179911382785....\n",
      "-----------------------------------\n",
      "Training loss: 0.889381995837418....\n",
      "Validation loss: 0.8880176701176191....\n",
      "-----------------------------------\n",
      "Training loss: 0.8884248595098766....\n",
      "Validation loss: 0.8872190092987018....\n",
      "-----------------------------------\n",
      "Training loss: 0.8874704921979654....\n",
      "Validation loss: 0.8864219498631305....\n",
      "-----------------------------------\n",
      "Training loss: 0.886518428830356....\n",
      "Validation loss: 0.885626651075924....\n",
      "-----------------------------------\n",
      "Training loss: 0.8855688353754977....\n",
      "Validation loss: 0.8848330424871956....\n",
      "-----------------------------------\n",
      "Training loss: 0.8846216306779722....\n",
      "Validation loss: 0.8840412558630703....\n",
      "-----------------------------------\n",
      "Training loss: 0.8836767237070994....\n",
      "Validation loss: 0.8832510632256834....\n",
      "-----------------------------------\n",
      "Training loss: 0.8827344507024852....\n",
      "Validation loss: 0.8824627180161899....\n",
      "-----------------------------------\n",
      "Training loss: 0.881794384717119....\n",
      "Validation loss: 0.881675972741356....\n",
      "-----------------------------------\n",
      "Training loss: 0.8808565664193261....\n",
      "Validation loss: 0.8808906974732901....\n",
      "-----------------------------------\n",
      "Training loss: 0.8799211026714187....\n",
      "Validation loss: 0.8801072631038329....\n",
      "-----------------------------------\n",
      "Training loss: 0.8789878306011829....\n",
      "Validation loss: 0.8793254934507686....\n",
      "-----------------------------------\n",
      "Training loss: 0.8780568939519098....\n",
      "Validation loss: 0.8785454308165452....\n",
      "-----------------------------------\n",
      "Training loss: 0.877128086861347....\n",
      "Validation loss: 0.877767159977916....\n",
      "-----------------------------------\n",
      "Training loss: 0.8762015375566459....\n",
      "Validation loss: 0.8769906284712755....\n",
      "-----------------------------------\n",
      "Training loss: 0.8752771609090536....\n",
      "Validation loss: 0.8762160621899712....\n",
      "-----------------------------------\n",
      "Training loss: 0.8743548158447528....\n",
      "Validation loss: 0.8754433049829581....\n",
      "-----------------------------------\n",
      "Training loss: 0.8734347985849406....\n",
      "Validation loss: 0.8746724795512473....\n",
      "-----------------------------------\n",
      "Training loss: 0.8725172172849539....\n",
      "Validation loss: 0.87390382756898....\n",
      "-----------------------------------\n",
      "Training loss: 0.8716024086343506....\n",
      "Validation loss: 0.8731370886137197....\n",
      "-----------------------------------\n",
      "Training loss: 0.8706901519970623....\n",
      "Validation loss: 0.8723725839181085....\n",
      "-----------------------------------\n",
      "Training loss: 0.8697804037608148....\n",
      "Validation loss: 0.8716100641747471....\n",
      "-----------------------------------\n",
      "Training loss: 0.868872704157891....\n",
      "Validation loss: 0.870849664516157....\n",
      "-----------------------------------\n",
      "Training loss: 0.8679677475687533....\n",
      "Validation loss: 0.8700915410261983....\n",
      "-----------------------------------\n",
      "Training loss: 0.8670651915316208....\n",
      "Validation loss: 0.869335525127297....\n",
      "-----------------------------------\n",
      "Training loss: 0.866165392432975....\n",
      "Validation loss: 0.8685812929881093....\n",
      "-----------------------------------\n",
      "Training loss: 0.8652680621586177....\n",
      "Validation loss: 0.8678289594777575....\n",
      "-----------------------------------\n",
      "Training loss: 0.8643731389142972....\n",
      "Validation loss: 0.8670784940022671....\n",
      "-----------------------------------\n",
      "Training loss: 0.8634805919791377....\n",
      "Validation loss: 0.8663297015609478....\n",
      "-----------------------------------\n",
      "Training loss: 0.8625904146030132....\n",
      "Validation loss: 0.8655827138116811....\n",
      "-----------------------------------\n",
      "Training loss: 0.8617024246446936....\n",
      "Validation loss: 0.8648377231886641....\n",
      "-----------------------------------\n",
      "Training loss: 0.860816767430645....\n",
      "Validation loss: 0.8640948045107235....\n",
      "-----------------------------------\n",
      "Training loss: 0.8599337678983189....\n",
      "Validation loss: 0.8633540839471484....\n",
      "-----------------------------------\n",
      "Training loss: 0.8590534494594781....\n",
      "Validation loss: 0.862615027943717....\n",
      "-----------------------------------\n",
      "Training loss: 0.858175616300081....\n",
      "Validation loss: 0.8618781957653439....\n",
      "-----------------------------------\n",
      "Training loss: 0.857300256436126....\n",
      "Validation loss: 0.8611434848465174....\n",
      "-----------------------------------\n",
      "Training loss: 0.8564270387455026....\n",
      "Validation loss: 0.8604106408114083....\n",
      "-----------------------------------\n",
      "Training loss: 0.8555559524591642....\n",
      "Validation loss: 0.8596796849695133....\n",
      "-----------------------------------\n",
      "Training loss: 0.854687096814156....\n",
      "Validation loss: 0.8589508764072514....\n",
      "-----------------------------------\n",
      "Training loss: 0.8538208267837323....\n",
      "Validation loss: 0.8582238401304346....\n",
      "-----------------------------------\n",
      "Training loss: 0.8529568338906227....\n",
      "Validation loss: 0.8574985686789701....\n",
      "-----------------------------------\n",
      "Training loss: 0.8520951042951267....\n",
      "Validation loss: 0.8567752066843486....\n",
      "-----------------------------------\n",
      "Training loss: 0.8512355931912248....\n",
      "Validation loss: 0.8560537556336335....\n",
      "-----------------------------------\n",
      "Training loss: 0.8503783071044956....\n",
      "Validation loss: 0.8553341714986072....\n",
      "-----------------------------------\n",
      "Training loss: 0.8495230772633974....\n",
      "Validation loss: 0.8546164560584649....\n",
      "-----------------------------------\n",
      "Training loss: 0.8486698558953301....\n",
      "Validation loss: 0.853900618518364....\n",
      "-----------------------------------\n",
      "Training loss: 0.8478188734895191....\n",
      "Validation loss: 0.8531865972530602....\n",
      "-----------------------------------\n",
      "Training loss: 0.8469700318812099....\n",
      "Validation loss: 0.8524745817651821....\n",
      "-----------------------------------\n",
      "Training loss: 0.8461233087371047....\n",
      "Validation loss: 0.8517644734325461....\n",
      "-----------------------------------\n",
      "Training loss: 0.8452785760130698....\n",
      "Validation loss: 0.8510562043191057....\n",
      "-----------------------------------\n",
      "Training loss: 0.8444355080030587....\n",
      "Validation loss: 0.8503496795237256....\n",
      "-----------------------------------\n",
      "Training loss: 0.8435943171935373....\n",
      "Validation loss: 0.8496447105989018....\n",
      "-----------------------------------\n",
      "Training loss: 0.8427553483515804....\n",
      "Validation loss: 0.8489413437497281....\n",
      "-----------------------------------\n",
      "Training loss: 0.841918374881523....\n",
      "Validation loss: 0.8482399887524172....\n",
      "-----------------------------------\n",
      "Training loss: 0.8410834140046399....\n",
      "Validation loss: 0.8475404716137532....\n",
      "-----------------------------------\n",
      "Training loss: 0.8402503194264118....\n",
      "Validation loss: 0.8468426176143337....\n",
      "-----------------------------------\n",
      "Training loss: 0.8394192782910657....\n",
      "Validation loss: 0.8461462405867843....\n",
      "-----------------------------------\n",
      "Training loss: 0.8385902193919677....\n",
      "Validation loss: 0.8454513644613609....\n",
      "-----------------------------------\n",
      "Training loss: 0.8377630547967633....\n",
      "Validation loss: 0.8447581570309999....\n",
      "-----------------------------------\n",
      "Training loss: 0.8369374119079669....\n",
      "Validation loss: 0.8440666185393789....\n",
      "-----------------------------------\n",
      "Training loss: 0.8361136594730875....\n",
      "Validation loss: 0.8433765244662841....\n",
      "-----------------------------------\n",
      "Training loss: 0.8352919436910805....\n",
      "Validation loss: 0.8426881200801443....\n",
      "-----------------------------------\n",
      "Training loss: 0.834472062535537....\n",
      "Validation loss: 0.8420015533098093....\n",
      "-----------------------------------\n",
      "Training loss: 0.8336541710850021....\n",
      "Validation loss: 0.8413168768463167....\n",
      "-----------------------------------\n",
      "Training loss: 0.8328381556052936....\n",
      "Validation loss: 0.8406339137236619....\n",
      "-----------------------------------\n",
      "Training loss: 0.8320240654562359....\n",
      "Validation loss: 0.8399520608156502....\n",
      "-----------------------------------\n",
      "Training loss: 0.8312121471779559....\n",
      "Validation loss: 0.8392720525043875....\n",
      "-----------------------------------\n",
      "Training loss: 0.8304023193104171....\n",
      "Validation loss: 0.8385935976907979....\n",
      "-----------------------------------\n",
      "Training loss: 0.8295943549098891....\n",
      "Validation loss: 0.8379167630035627....\n",
      "-----------------------------------\n",
      "Training loss: 0.8287882967345098....\n",
      "Validation loss: 0.8372416205483839....\n",
      "-----------------------------------\n",
      "Training loss: 0.8279842724813578....\n",
      "Validation loss: 0.8365684255207606....\n",
      "-----------------------------------\n",
      "Training loss: 0.8271827227622971....\n",
      "Validation loss: 0.8358968579443625....\n",
      "-----------------------------------\n",
      "Training loss: 0.8263832692021296....\n",
      "Validation loss: 0.8352271279179511....\n",
      "-----------------------------------\n",
      "Training loss: 0.825585741211584....\n",
      "Validation loss: 0.8345591479318086....\n",
      "-----------------------------------\n",
      "Training loss: 0.8247902490669337....\n",
      "Validation loss: 0.8338929255857281....\n",
      "-----------------------------------\n",
      "Training loss: 0.8239969088239467....\n",
      "Validation loss: 0.8332281902910168....\n",
      "-----------------------------------\n",
      "Training loss: 0.823205067256056....\n",
      "Validation loss: 0.8325650713466217....\n",
      "-----------------------------------\n",
      "Training loss: 0.8224151024908336....\n",
      "Validation loss: 0.8319035680529807....\n",
      "-----------------------------------\n",
      "Training loss: 0.821626916137034....\n",
      "Validation loss: 0.8312435699789255....\n",
      "-----------------------------------\n",
      "Training loss: 0.8208406051921326....\n",
      "Validation loss: 0.830585291549249....\n",
      "-----------------------------------\n",
      "Training loss: 0.8200563817976192....\n",
      "Validation loss: 0.8299285255028553....\n",
      "-----------------------------------\n",
      "Training loss: 0.8192740818709104....\n",
      "Validation loss: 0.82927336418112....\n",
      "-----------------------------------\n",
      "Training loss: 0.8184937403898271....\n",
      "Validation loss: 0.8286199939560666....\n",
      "-----------------------------------\n",
      "Training loss: 0.8177155972206442....\n",
      "Validation loss: 0.8279680408705001....\n",
      "-----------------------------------\n",
      "Training loss: 0.8169393701321473....\n",
      "Validation loss: 0.8273176974792846....\n",
      "-----------------------------------\n",
      "Training loss: 0.8161650549503328....\n",
      "Validation loss: 0.8266688080182474....\n",
      "-----------------------------------\n",
      "Training loss: 0.8153924985452075....\n",
      "Validation loss: 0.8260214388303388....\n",
      "-----------------------------------\n",
      "Training loss: 0.8146219336976601....\n",
      "Validation loss: 0.8253758043535395....\n",
      "-----------------------------------\n",
      "Training loss: 0.8138533493226269....\n",
      "Validation loss: 0.8247317639563355....\n",
      "-----------------------------------\n",
      "Training loss: 0.8130864729211947....\n",
      "Validation loss: 0.8240891431146982....\n",
      "-----------------------------------\n",
      "Training loss: 0.8123214367562865....\n",
      "Validation loss: 0.8234479590377962....\n",
      "-----------------------------------\n",
      "Training loss: 0.8115580758640814....\n",
      "Validation loss: 0.822808439264728....\n",
      "-----------------------------------\n",
      "Training loss: 0.8107964398017559....\n",
      "Validation loss: 0.8221705424067415....\n",
      "-----------------------------------\n",
      "Training loss: 0.8100366072865075....\n",
      "Validation loss: 0.8215341636094515....\n",
      "-----------------------------------\n",
      "Training loss: 0.8092787973193132....\n",
      "Validation loss: 0.8208991648381339....\n",
      "-----------------------------------\n",
      "Training loss: 0.8085227379238092....\n",
      "Validation loss: 0.8202658314507117....\n",
      "-----------------------------------\n",
      "Training loss: 0.8077682099318954....\n",
      "Validation loss: 0.8196341772203054....\n",
      "-----------------------------------\n",
      "Training loss: 0.8070153784457982....\n",
      "Validation loss: 0.8190042118810641....\n",
      "-----------------------------------\n",
      "Training loss: 0.8062645028520153....\n",
      "Validation loss: 0.8183760314235925....\n",
      "-----------------------------------\n",
      "Training loss: 0.8055154277703105....\n",
      "Validation loss: 0.8177495682242991....\n",
      "-----------------------------------\n",
      "Training loss: 0.8047683578100929....\n",
      "Validation loss: 0.8171247402593169....\n",
      "-----------------------------------\n",
      "Training loss: 0.8040232427944538....\n",
      "Validation loss: 0.8165012473653397....\n",
      "-----------------------------------\n",
      "Training loss: 0.8032799143106096....\n",
      "Validation loss: 0.8158791639655595....\n",
      "-----------------------------------\n",
      "Training loss: 0.8025384205724904....\n",
      "Validation loss: 0.8152584952503675....\n",
      "-----------------------------------\n",
      "Training loss: 0.8017987357620612....\n",
      "Validation loss: 0.814639269301695....\n",
      "-----------------------------------\n",
      "Training loss: 0.8010608336703349....\n",
      "Validation loss: 0.8140215543588477....\n",
      "-----------------------------------\n",
      "Training loss: 0.8003246524226173....\n",
      "Validation loss: 0.8134053538651597....\n",
      "-----------------------------------\n",
      "Training loss: 0.7995900592906292....\n",
      "Validation loss: 0.8127905653443136....\n",
      "-----------------------------------\n",
      "Training loss: 0.7988571205249689....\n",
      "Validation loss: 0.8121773147003152....\n",
      "-----------------------------------\n",
      "Training loss: 0.7981258771198665....\n",
      "Validation loss: 0.8115653677553971....\n",
      "-----------------------------------\n",
      "Training loss: 0.7973964202374797....\n",
      "Validation loss: 0.8109549342130901....\n",
      "-----------------------------------\n",
      "Training loss: 0.796668542244252....\n",
      "Validation loss: 0.8103459959179015....\n",
      "-----------------------------------\n",
      "Training loss: 0.7959423072770322....\n",
      "Validation loss: 0.8097384880601274....\n",
      "-----------------------------------\n",
      "Training loss: 0.7952177537301593....\n",
      "Validation loss: 0.8091322735892957....\n",
      "-----------------------------------\n",
      "Training loss: 0.7944949075865075....\n",
      "Validation loss: 0.8085275595180655....\n",
      "-----------------------------------\n",
      "Training loss: 0.79377384290368....\n",
      "Validation loss: 0.8079240712731591....\n",
      "-----------------------------------\n",
      "Training loss: 0.7930544215276598....\n",
      "Validation loss: 0.8073221287917721....\n",
      "-----------------------------------\n",
      "Training loss: 0.792336653319941....\n",
      "Validation loss: 0.8067215320669905....\n",
      "-----------------------------------\n",
      "Training loss: 0.7916205473181788....\n",
      "Validation loss: 0.8061222097737066....\n",
      "-----------------------------------\n",
      "Training loss: 0.7909060400210857....\n",
      "Validation loss: 0.8055244734649454....\n",
      "-----------------------------------\n",
      "Training loss: 0.7901930426383389....\n",
      "Validation loss: 0.8049282733565947....\n",
      "-----------------------------------\n",
      "Training loss: 0.7894817675515865....\n",
      "Validation loss: 0.8043333904456444....\n",
      "-----------------------------------\n",
      "Training loss: 0.7887721557053436....\n",
      "Validation loss: 0.8037400377022178....\n",
      "-----------------------------------\n",
      "Training loss: 0.7880643088840115....\n",
      "Validation loss: 0.8031481617723429....\n",
      "-----------------------------------\n",
      "Training loss: 0.7873576818684559....\n",
      "Validation loss: 0.8025579514620342....\n",
      "-----------------------------------\n",
      "Training loss: 0.7866524992084538....\n",
      "Validation loss: 0.8019691639472262....\n",
      "-----------------------------------\n",
      "Training loss: 0.7859486124849271....\n",
      "Validation loss: 0.801381830149114....\n",
      "-----------------------------------\n",
      "Training loss: 0.7852462147969658....\n",
      "Validation loss: 0.800795906692294....\n",
      "-----------------------------------\n",
      "Training loss: 0.784545391521298....\n",
      "Validation loss: 0.8002117171019189....\n",
      "-----------------------------------\n",
      "Training loss: 0.783846102971757....\n",
      "Validation loss: 0.7996289453878731....\n",
      "-----------------------------------\n",
      "Training loss: 0.7831485267544239....\n",
      "Validation loss: 0.7990475540140048....\n",
      "-----------------------------------\n",
      "Training loss: 0.7824525308534339....\n",
      "Validation loss: 0.7984677061585741....\n",
      "-----------------------------------\n",
      "Training loss: 0.7817579789836175....\n",
      "Validation loss: 0.7978893595765836....\n",
      "-----------------------------------\n",
      "Training loss: 0.7810650847681404....\n",
      "Validation loss: 0.7973123827770592....\n",
      "-----------------------------------\n",
      "Training loss: 0.7803734598375959....\n",
      "Validation loss: 0.7967367390371591....\n",
      "-----------------------------------\n",
      "Training loss: 0.7796834155315929....\n",
      "Validation loss: 0.7961625389939541....\n",
      "-----------------------------------\n",
      "Training loss: 0.7789951216767871....\n",
      "Validation loss: 0.7955898402810604....\n",
      "-----------------------------------\n",
      "Training loss: 0.7783085550210459....\n",
      "Validation loss: 0.795018475042276....\n",
      "-----------------------------------\n",
      "Training loss: 0.7776235271207893....\n",
      "Validation loss: 0.7944485416626451....\n",
      "-----------------------------------\n",
      "Training loss: 0.7769398690832775....\n",
      "Validation loss: 0.793880011626654....\n",
      "-----------------------------------\n",
      "Training loss: 0.7762576774371523....\n",
      "Validation loss: 0.7933129761493567....\n",
      "-----------------------------------\n",
      "Training loss: 0.7755772347091634....\n",
      "Validation loss: 0.7927473891654697....\n",
      "-----------------------------------\n",
      "Training loss: 0.7748983837483756....\n",
      "Validation loss: 0.7921831670673045....\n",
      "-----------------------------------\n",
      "Training loss: 0.774221119539946....\n",
      "Validation loss: 0.791620279518497....\n",
      "-----------------------------------\n",
      "Training loss: 0.7735453583989845....\n",
      "Validation loss: 0.7910587106391768....\n",
      "-----------------------------------\n",
      "Training loss: 0.7728713435356875....\n",
      "Validation loss: 0.7904983476081509....\n",
      "-----------------------------------\n",
      "Training loss: 0.772198876218497....\n",
      "Validation loss: 0.7899393189612772....\n",
      "-----------------------------------\n",
      "Training loss: 0.771527939370413....\n",
      "Validation loss: 0.7893815959065535....\n",
      "-----------------------------------\n",
      "Training loss: 0.7708584966894773....\n",
      "Validation loss: 0.7888251601335332....\n",
      "-----------------------------------\n",
      "Training loss: 0.770190356279828....\n",
      "Validation loss: 0.7882698593066824....\n",
      "-----------------------------------\n",
      "Training loss: 0.769523588684181....\n",
      "Validation loss: 0.7877158584824375....\n",
      "-----------------------------------\n",
      "Training loss: 0.7688583061031742....\n",
      "Validation loss: 0.787163116380186....\n",
      "-----------------------------------\n",
      "Training loss: 0.7681946054761928....\n",
      "Validation loss: 0.7866115753083359....\n",
      "-----------------------------------\n",
      "Training loss: 0.7675323601335424....\n",
      "Validation loss: 0.7860611167475184....\n",
      "-----------------------------------\n",
      "Training loss: 0.7668717189286426....\n",
      "Validation loss: 0.7855118242692833....\n",
      "-----------------------------------\n",
      "Training loss: 0.7662124801033997....\n",
      "Validation loss: 0.7849638230062447....\n",
      "-----------------------------------\n",
      "Training loss: 0.7655545927692254....\n",
      "Validation loss: 0.7844172920064106....\n",
      "-----------------------------------\n",
      "Training loss: 0.7648978911282813....\n",
      "Validation loss: 0.7838721171500793....\n",
      "-----------------------------------\n",
      "Training loss: 0.7642426500593312....\n",
      "Validation loss: 0.7833281048795898....\n",
      "-----------------------------------\n",
      "Training loss: 0.7635887991734587....\n",
      "Validation loss: 0.7827852850709767....\n",
      "-----------------------------------\n",
      "Training loss: 0.7629364086631003....\n",
      "Validation loss: 0.7822437442028002....\n",
      "-----------------------------------\n",
      "Training loss: 0.7622853869462343....\n",
      "Validation loss: 0.781703478247734....\n",
      "-----------------------------------\n",
      "Training loss: 0.7616358197198722....\n",
      "Validation loss: 0.7811644883754377....\n",
      "-----------------------------------\n",
      "Training loss: 0.7609877570165711....\n",
      "Validation loss: 0.780626713036414....\n",
      "-----------------------------------\n",
      "Training loss: 0.7603410065794116....\n",
      "Validation loss: 0.780090130454469....\n",
      "-----------------------------------\n",
      "Training loss: 0.7596955525798786....\n",
      "Validation loss: 0.7795546751354911....\n",
      "-----------------------------------\n",
      "Training loss: 0.7590514113682834....\n",
      "Validation loss: 0.7790204899923854....\n",
      "-----------------------------------\n",
      "Training loss: 0.7584088112598527....\n",
      "Validation loss: 0.778487506876578....\n",
      "-----------------------------------\n",
      "Training loss: 0.7577674540527738....\n",
      "Validation loss: 0.7779556702042596....\n",
      "-----------------------------------\n",
      "Training loss: 0.7571275727581243....\n",
      "Validation loss: 0.777425124693272....\n",
      "-----------------------------------\n",
      "Training loss: 0.7564893037756233....\n",
      "Validation loss: 0.7768958641501538....\n",
      "-----------------------------------\n",
      "Training loss: 0.7558524849394691....\n",
      "Validation loss: 0.7763678831842865....\n",
      "-----------------------------------\n",
      "Training loss: 0.755217079223672....\n",
      "Validation loss: 0.7758411390937047....\n",
      "-----------------------------------\n",
      "Training loss: 0.7545829635544441....\n",
      "Validation loss: 0.7753156115612685....\n",
      "-----------------------------------\n",
      "Training loss: 0.7539500995061026....\n",
      "Validation loss: 0.7747911934037107....\n",
      "-----------------------------------\n",
      "Training loss: 0.7533185237948016....\n",
      "Validation loss: 0.7742681826230671....\n",
      "-----------------------------------\n",
      "Training loss: 0.7526883460127397....\n",
      "Validation loss: 0.7737464954841348....\n",
      "-----------------------------------\n",
      "Training loss: 0.752059417262203....\n",
      "Validation loss: 0.7732259806179245....\n",
      "-----------------------------------\n",
      "Training loss: 0.7514319250276504....\n",
      "Validation loss: 0.7727065309579687....\n",
      "-----------------------------------\n",
      "Training loss: 0.7508057332838063....\n",
      "Validation loss: 0.7721881206505307....\n",
      "-----------------------------------\n",
      "Training loss: 0.7501807371335073....\n",
      "Validation loss: 0.7716708921177827....\n",
      "-----------------------------------\n",
      "Training loss: 0.7495570951718169....\n",
      "Validation loss: 0.7711547937121149....\n",
      "-----------------------------------\n",
      "Training loss: 0.7489347612404803....\n",
      "Validation loss: 0.7706398777165093....\n",
      "-----------------------------------\n",
      "Training loss: 0.7483138743187492....\n",
      "Validation loss: 0.7701261891641552....\n",
      "-----------------------------------\n",
      "Training loss: 0.7476944244436329....\n",
      "Validation loss: 0.7696137471847364....\n",
      "-----------------------------------\n",
      "Training loss: 0.7470764035363334....\n",
      "Validation loss: 0.7691026177418464....\n",
      "-----------------------------------\n",
      "Training loss: 0.7464597286752426....\n",
      "Validation loss: 0.7685925365061014....\n",
      "-----------------------------------\n",
      "Training loss: 0.7458444468380894....\n",
      "Validation loss: 0.7680836255014163....\n",
      "-----------------------------------\n",
      "Training loss: 0.7452304152805073....\n",
      "Validation loss: 0.7675757418835434....\n",
      "-----------------------------------\n",
      "Training loss: 0.7446175503320799....\n",
      "Validation loss: 0.7670688881149864....\n",
      "-----------------------------------\n",
      "Training loss: 0.7440059925293036....\n",
      "Validation loss: 0.7665631801599985....\n",
      "-----------------------------------\n",
      "Training loss: 0.7433958383659851....\n",
      "Validation loss: 0.7660587266026432....\n",
      "-----------------------------------\n",
      "Training loss: 0.7427870508354216....\n",
      "Validation loss: 0.7655551055048729....\n",
      "-----------------------------------\n",
      "Training loss: 0.742179259166166....\n",
      "Validation loss: 0.7650524112847579....\n",
      "-----------------------------------\n",
      "Training loss: 0.7415727901880534....\n",
      "Validation loss: 0.7645506022039242....\n",
      "-----------------------------------\n",
      "Training loss: 0.74096762454295....\n",
      "Validation loss: 0.7640499565198474....\n",
      "-----------------------------------\n",
      "Training loss: 0.7403638551468759....\n",
      "Validation loss: 0.7635500444893207....\n",
      "-----------------------------------\n",
      "Training loss: 0.7397613330014855....\n",
      "Validation loss: 0.7630510600417212....\n",
      "-----------------------------------\n",
      "Training loss: 0.7391600293684055....\n",
      "Validation loss: 0.762553169302128....\n",
      "-----------------------------------\n",
      "Training loss: 0.7385600341497462....\n",
      "Validation loss: 0.7620563520560776....\n",
      "-----------------------------------\n",
      "Training loss: 0.7379613867243972....\n",
      "Validation loss: 0.7615605277199292....\n",
      "-----------------------------------\n",
      "Training loss: 0.7373640726560993....\n",
      "Validation loss: 0.7610658933550107....\n",
      "-----------------------------------\n",
      "Training loss: 0.7367679026688286....\n",
      "Validation loss: 0.7605723574693927....\n",
      "-----------------------------------\n",
      "Training loss: 0.7361729119618391....\n",
      "Validation loss: 0.7600799985121367....\n",
      "-----------------------------------\n",
      "Training loss: 0.7355791852884143....\n",
      "Validation loss: 0.759588712191061....\n",
      "-----------------------------------\n",
      "Training loss: 0.7349865928742434....\n",
      "Validation loss: 0.7590985517665931....\n",
      "-----------------------------------\n",
      "Training loss: 0.7343950996765983....\n",
      "Validation loss: 0.7586095554496465....\n",
      "-----------------------------------\n",
      "Training loss: 0.7338046093903243....\n",
      "Validation loss: 0.7581215363453656....\n",
      "-----------------------------------\n",
      "Training loss: 0.7332152963415739....\n",
      "Validation loss: 0.757634671673396....\n",
      "-----------------------------------\n",
      "Training loss: 0.7326271949850449....\n",
      "Validation loss: 0.7571487192174963....\n",
      "-----------------------------------\n",
      "Training loss: 0.7320404094757459....\n",
      "Validation loss: 0.7566636378783059....\n",
      "-----------------------------------\n",
      "Training loss: 0.7314549305717808....\n",
      "Validation loss: 0.7561797683021158....\n",
      "-----------------------------------\n",
      "Training loss: 0.7308707411438827....\n",
      "Validation loss: 0.7556968276261747....\n",
      "-----------------------------------\n",
      "Training loss: 0.7302878533144983....\n",
      "Validation loss: 0.7552152020951969....\n",
      "-----------------------------------\n",
      "Training loss: 0.729706230906501....\n",
      "Validation loss: 0.7547346151382552....\n",
      "-----------------------------------\n",
      "Training loss: 0.7291257401830422....\n",
      "Validation loss: 0.7542551058698428....\n",
      "-----------------------------------\n",
      "Training loss: 0.7285463619599412....\n",
      "Validation loss: 0.7537766556619616....\n",
      "-----------------------------------\n",
      "Training loss: 0.7279681005641063....\n",
      "Validation loss: 0.7532993762306603....\n",
      "-----------------------------------\n",
      "Training loss: 0.7273909900049793....\n",
      "Validation loss: 0.7528232050824074....\n",
      "-----------------------------------\n",
      "Training loss: 0.7268147756697301....\n",
      "Validation loss: 0.7523482414613606....\n",
      "-----------------------------------\n",
      "Training loss: 0.7262396941607195....\n",
      "Validation loss: 0.7518744605762843....\n",
      "-----------------------------------\n",
      "Training loss: 0.725666014075692....\n",
      "Validation loss: 0.7514018481818591....\n",
      "-----------------------------------\n",
      "Training loss: 0.725093636168094....\n",
      "Validation loss: 0.7509303030141052....\n",
      "-----------------------------------\n",
      "Training loss: 0.7245224296681826....\n",
      "Validation loss: 0.7504597751748329....\n",
      "-----------------------------------\n",
      "Training loss: 0.7239523708206096....\n",
      "Validation loss: 0.7499903586610035....\n",
      "-----------------------------------\n",
      "Training loss: 0.7233836353332704....\n",
      "Validation loss: 0.7495219911915877....\n",
      "-----------------------------------\n",
      "Training loss: 0.7228160896032723....\n",
      "Validation loss: 0.7490544303175195....\n",
      "-----------------------------------\n",
      "Training loss: 0.722249560092441....\n",
      "Validation loss: 0.7485879709225424....\n",
      "-----------------------------------\n",
      "Training loss: 0.7216841751850009....\n",
      "Validation loss: 0.7481226652740884....\n",
      "-----------------------------------\n",
      "Training loss: 0.7211200078304326....\n",
      "Validation loss: 0.7476584061448788....\n",
      "-----------------------------------\n",
      "Training loss: 0.7205570310911278....\n",
      "Validation loss: 0.7471949503350162....\n",
      "-----------------------------------\n",
      "Training loss: 0.7199951993501487....\n",
      "Validation loss: 0.7467324889605738....\n",
      "-----------------------------------\n",
      "Training loss: 0.719434475022489....\n",
      "Validation loss: 0.7462711506769313....\n",
      "-----------------------------------\n",
      "Training loss: 0.7188749258676201....\n",
      "Validation loss: 0.7458108350163319....\n",
      "-----------------------------------\n",
      "Training loss: 0.7183164102600399....\n",
      "Validation loss: 0.7453514097762501....\n",
      "-----------------------------------\n",
      "Training loss: 0.717759137883549....\n",
      "Validation loss: 0.7448929473379101....\n",
      "-----------------------------------\n",
      "Training loss: 0.717203074315989....\n",
      "Validation loss: 0.7444354869390961....\n",
      "-----------------------------------\n",
      "Training loss: 0.7166480432433902....\n",
      "Validation loss: 0.7439790145206755....\n",
      "-----------------------------------\n",
      "Training loss: 0.716094195402632....\n",
      "Validation loss: 0.7435235579594969....\n",
      "-----------------------------------\n",
      "Training loss: 0.7155415021156293....\n",
      "Validation loss: 0.7430690913965111....\n",
      "-----------------------------------\n",
      "Training loss: 0.7149898387226038....\n",
      "Validation loss: 0.742615670749591....\n",
      "-----------------------------------\n",
      "Training loss: 0.7144392706578163....\n",
      "Validation loss: 0.7421631440442875....\n",
      "-----------------------------------\n",
      "Training loss: 0.71388973104362....\n",
      "Validation loss: 0.7417115842573364....\n",
      "-----------------------------------\n",
      "Training loss: 0.7133411896845587....\n",
      "Validation loss: 0.7412608356361797....\n",
      "-----------------------------------\n",
      "Training loss: 0.7127935756194881....\n",
      "Validation loss: 0.7408110263131589....\n",
      "-----------------------------------\n",
      "Training loss: 0.7122470924590302....\n",
      "Validation loss: 0.7403621132290473....\n",
      "-----------------------------------\n",
      "Training loss: 0.7117017264120068....\n",
      "Validation loss: 0.7399140404357045....\n",
      "-----------------------------------\n",
      "Training loss: 0.7111574033787962....\n",
      "Validation loss: 0.7394670515002636....\n",
      "-----------------------------------\n",
      "Training loss: 0.7106141976424685....\n",
      "Validation loss: 0.7390209883123733....\n",
      "-----------------------------------\n",
      "Training loss: 0.7100721872864836....\n",
      "Validation loss: 0.7385758945076262....\n",
      "-----------------------------------\n",
      "Training loss: 0.7095313931343198....\n",
      "Validation loss: 0.7381318803017799....\n",
      "-----------------------------------\n",
      "Training loss: 0.7089917242194038....\n",
      "Validation loss: 0.7376887838623429....\n",
      "-----------------------------------\n",
      "Training loss: 0.7084531592814803....\n",
      "Validation loss: 0.7372467271955986....\n",
      "-----------------------------------\n",
      "Training loss: 0.7079157223097255....\n",
      "Validation loss: 0.7368056748793463....\n",
      "-----------------------------------\n",
      "Training loss: 0.7073794344243193....\n",
      "Validation loss: 0.7363658209829721....\n",
      "-----------------------------------\n",
      "Training loss: 0.7068442764328022....\n",
      "Validation loss: 0.7359266966080342....\n",
      "-----------------------------------\n",
      "Training loss: 0.7063103192463808....\n",
      "Validation loss: 0.7354884016829233....\n",
      "-----------------------------------\n",
      "Training loss: 0.7057774547530535....\n",
      "Validation loss: 0.7350508706583093....\n",
      "-----------------------------------\n",
      "Training loss: 0.7052456308778895....\n",
      "Validation loss: 0.7346142396788549....\n",
      "-----------------------------------\n",
      "Training loss: 0.704714977537308....\n",
      "Validation loss: 0.7341784611419471....\n",
      "-----------------------------------\n",
      "Training loss: 0.7041853909167892....\n",
      "Validation loss: 0.7337436386812906....\n",
      "-----------------------------------\n",
      "Training loss: 0.7036566908818197....\n",
      "Validation loss: 0.7333097186912791....\n",
      "-----------------------------------\n",
      "Training loss: 0.7031289962694187....\n",
      "Validation loss: 0.7328768670209058....\n",
      "-----------------------------------\n",
      "Training loss: 0.7026023550696561....\n",
      "Validation loss: 0.7324448245075277....\n",
      "-----------------------------------\n",
      "Training loss: 0.702076622042178....\n",
      "Validation loss: 0.7320136144228961....\n",
      "-----------------------------------\n",
      "Training loss: 0.7015517656631353....\n",
      "Validation loss: 0.7315830409603229....\n",
      "-----------------------------------\n",
      "Training loss: 0.7010279014345536....\n",
      "Validation loss: 0.7311533051530705....\n",
      "-----------------------------------\n",
      "Training loss: 0.7005050069347841....\n",
      "Validation loss: 0.7307244659036773....\n",
      "-----------------------------------\n",
      "Training loss: 0.6999831581087003....\n",
      "Validation loss: 0.7302965137249574....\n",
      "-----------------------------------\n",
      "Training loss: 0.6994623142156988....\n",
      "Validation loss: 0.7298695233276933....\n",
      "-----------------------------------\n",
      "Training loss: 0.6989424430748018....\n",
      "Validation loss: 0.7294434038235753....\n",
      "-----------------------------------\n",
      "Training loss: 0.6984235249537468....\n",
      "Validation loss: 0.7290183456729369....\n",
      "-----------------------------------\n",
      "Training loss: 0.6979055995546106....\n",
      "Validation loss: 0.7285942125130993....\n",
      "-----------------------------------\n",
      "Training loss: 0.69738871341799....\n",
      "Validation loss: 0.7281710320396174....\n",
      "-----------------------------------\n",
      "Training loss: 0.6968729555961474....\n",
      "Validation loss: 0.7277486363532351....\n",
      "-----------------------------------\n",
      "Training loss: 0.6963582421812082....\n",
      "Validation loss: 0.7273272041937164....\n",
      "-----------------------------------\n",
      "Training loss: 0.6958444847617536....\n",
      "Validation loss: 0.7269068878625998....\n",
      "-----------------------------------\n",
      "Training loss: 0.6953318199283297....\n",
      "Validation loss: 0.7264874162239463....\n",
      "-----------------------------------\n",
      "Training loss: 0.6948199729376339....\n",
      "Validation loss: 0.7260688443299287....\n",
      "-----------------------------------\n",
      "Training loss: 0.6943090695410659....\n",
      "Validation loss: 0.7256509567751228....\n",
      "-----------------------------------\n",
      "Training loss: 0.693799187854501....\n",
      "Validation loss: 0.7252340319261457....\n",
      "-----------------------------------\n",
      "Training loss: 0.6932903539449788....\n",
      "Validation loss: 0.7248179814650921....\n",
      "-----------------------------------\n",
      "Training loss: 0.6927825130628491....\n",
      "Validation loss: 0.7244023627919643....\n",
      "-----------------------------------\n",
      "Training loss: 0.6922757553723619....\n",
      "Validation loss: 0.7239876455112522....\n",
      "-----------------------------------\n",
      "Training loss: 0.691769970620184....\n",
      "Validation loss: 0.7235737640735022....\n",
      "-----------------------------------\n",
      "Training loss: 0.6912650934870733....\n",
      "Validation loss: 0.7231606525975929....\n",
      "-----------------------------------\n",
      "Training loss: 0.6907612655854342....\n",
      "Validation loss: 0.7227483514226952....\n",
      "-----------------------------------\n",
      "Training loss: 0.6902583676450087....\n",
      "Validation loss: 0.722336722910918....\n",
      "-----------------------------------\n",
      "Training loss: 0.6897565027882049....\n",
      "Validation loss: 0.721925995587349....\n",
      "-----------------------------------\n",
      "Training loss: 0.6892556599082778....\n",
      "Validation loss: 0.7215162355032455....\n",
      "-----------------------------------\n",
      "Training loss: 0.6887558292411533....\n",
      "Validation loss: 0.721107379069842....\n",
      "-----------------------------------\n",
      "Training loss: 0.6882569169199404....\n",
      "Validation loss: 0.7206994504953781....\n",
      "-----------------------------------\n",
      "Training loss: 0.6877590513893413....\n",
      "Validation loss: 0.720292478775888....\n",
      "-----------------------------------\n",
      "Training loss: 0.6872621484363981....\n",
      "Validation loss: 0.719886371881901....\n",
      "-----------------------------------\n",
      "Training loss: 0.6867661350871124....\n",
      "Validation loss: 0.7194811021686957....\n",
      "-----------------------------------\n",
      "Training loss: 0.6862709976875656....\n",
      "Validation loss: 0.7190766581655625....\n",
      "-----------------------------------\n",
      "Training loss: 0.6857767623147804....\n",
      "Validation loss: 0.7186728328082264....\n",
      "-----------------------------------\n",
      "Training loss: 0.6852835051905933....\n",
      "Validation loss: 0.7182697738845548....\n",
      "-----------------------------------\n",
      "Training loss: 0.6847912416484095....\n",
      "Validation loss: 0.717867608993073....\n",
      "-----------------------------------\n",
      "Training loss: 0.6842999480485829....\n",
      "Validation loss: 0.7174662304293175....\n",
      "-----------------------------------\n",
      "Training loss: 0.6838096453655353....\n",
      "Validation loss: 0.7170655435521565....\n",
      "-----------------------------------\n",
      "Training loss: 0.6833203245099009....\n",
      "Validation loss: 0.716665608894594....\n",
      "-----------------------------------\n",
      "Training loss: 0.6828319132464512....\n",
      "Validation loss: 0.7162664419819907....\n",
      "-----------------------------------\n",
      "Training loss: 0.6823443251209408....\n",
      "Validation loss: 0.7158680450330751....\n",
      "-----------------------------------\n",
      "Training loss: 0.6818576997582253....\n",
      "Validation loss: 0.7154705523687576....\n",
      "-----------------------------------\n",
      "Training loss: 0.6813720434284622....\n",
      "Validation loss: 0.7150736528315385....\n",
      "-----------------------------------\n",
      "Training loss: 0.6808873144171014....\n",
      "Validation loss: 0.7146778153226504....\n",
      "-----------------------------------\n",
      "Training loss: 0.6804035738717127....\n",
      "Validation loss: 0.7142827048334689....\n",
      "-----------------------------------\n",
      "Training loss: 0.6799207606135867....\n",
      "Validation loss: 0.7138883937160387....\n",
      "-----------------------------------\n",
      "Training loss: 0.6794387577641523....\n",
      "Validation loss: 0.7134948513193533....\n",
      "-----------------------------------\n",
      "Training loss: 0.6789576345191911....\n",
      "Validation loss: 0.7131020376877414....\n",
      "-----------------------------------\n",
      "Training loss: 0.6784772933276341....\n",
      "Validation loss: 0.7127099060424522....\n",
      "-----------------------------------\n",
      "Training loss: 0.6779977689761492....\n",
      "Validation loss: 0.7123189232259469....\n",
      "-----------------------------------\n",
      "Training loss: 0.6775191148860117....\n",
      "Validation loss: 0.7119286340577528....\n",
      "-----------------------------------\n",
      "Training loss: 0.677041383945185....\n",
      "Validation loss: 0.7115390769027338....\n",
      "-----------------------------------\n",
      "Training loss: 0.6765644349561655....\n",
      "Validation loss: 0.7111502236489139....\n",
      "-----------------------------------\n",
      "Training loss: 0.6760884182927331....\n",
      "Validation loss: 0.7107620932154333....\n",
      "-----------------------------------\n",
      "Training loss: 0.6756134886599722....\n",
      "Validation loss: 0.7103747655874083....\n",
      "-----------------------------------\n",
      "Training loss: 0.6751394916865341....\n",
      "Validation loss: 0.7099882913019138....\n",
      "-----------------------------------\n",
      "Training loss: 0.6746664665628115....\n",
      "Validation loss: 0.7096026349582379....\n",
      "-----------------------------------\n",
      "Training loss: 0.6741942909594936....\n",
      "Validation loss: 0.7092176867799742....\n",
      "-----------------------------------\n",
      "Training loss: 0.6737230019899777....\n",
      "Validation loss: 0.7088332354847724....\n",
      "-----------------------------------\n",
      "Training loss: 0.6732525140568286....\n",
      "Validation loss: 0.70844950857281....\n",
      "-----------------------------------\n",
      "Training loss: 0.6727828080715285....\n",
      "Validation loss: 0.7080665004218126....\n",
      "-----------------------------------\n",
      "Training loss: 0.6723139854919289....\n",
      "Validation loss: 0.7076844542859884....\n",
      "-----------------------------------\n",
      "Training loss: 0.6718458677408148....\n",
      "Validation loss: 0.7073031756402323....\n",
      "-----------------------------------\n",
      "Training loss: 0.671378668482518....\n",
      "Validation loss: 0.7069225914670564....\n",
      "-----------------------------------\n",
      "Training loss: 0.6709124346394635....\n",
      "Validation loss: 0.7065429067198118....\n",
      "-----------------------------------\n",
      "Training loss: 0.670447020979822....\n",
      "Validation loss: 0.7061640114871022....\n",
      "-----------------------------------\n",
      "Training loss: 0.6699825184480089....\n",
      "Validation loss: 0.705785886984083....\n",
      "-----------------------------------\n",
      "Training loss: 0.6695188483051016....\n",
      "Validation loss: 0.7054083722929032....\n",
      "-----------------------------------\n",
      "Training loss: 0.6690559897589586....\n",
      "Validation loss: 0.7050316547730229....\n",
      "-----------------------------------\n",
      "Training loss: 0.6685939768584569....\n",
      "Validation loss: 0.7046557154517188....\n",
      "-----------------------------------\n",
      "Training loss: 0.6681328524614264....\n",
      "Validation loss: 0.7042805401913759....\n",
      "-----------------------------------\n",
      "Training loss: 0.6676725564587489....\n",
      "Validation loss: 0.7039059832285143....\n",
      "-----------------------------------\n",
      "Training loss: 0.6672130552840192....\n",
      "Validation loss: 0.703532114831071....\n",
      "-----------------------------------\n",
      "Training loss: 0.6667543174819379....\n",
      "Validation loss: 0.7031589225768408....\n",
      "-----------------------------------\n",
      "Training loss: 0.6662963644146385....\n",
      "Validation loss: 0.7027863298643919....\n",
      "-----------------------------------\n",
      "Training loss: 0.665839157180448....\n",
      "Validation loss: 0.7024144211173975....\n",
      "-----------------------------------\n",
      "Training loss: 0.6653828154135135....\n",
      "Validation loss: 0.7020433245984509....\n",
      "-----------------------------------\n",
      "Training loss: 0.6649273541350169....\n",
      "Validation loss: 0.7016730106181912....\n",
      "-----------------------------------\n",
      "Training loss: 0.6644727731370449....\n",
      "Validation loss: 0.7013034663259222....\n",
      "-----------------------------------\n",
      "Training loss: 0.6640191124822697....\n",
      "Validation loss: 0.7009346748274059....\n",
      "-----------------------------------\n",
      "Training loss: 0.6635662911252015....\n",
      "Validation loss: 0.7005666294168591....\n",
      "-----------------------------------\n",
      "Training loss: 0.6631143829370129....\n",
      "Validation loss: 0.7001992122584477....\n",
      "-----------------------------------\n",
      "Training loss: 0.6626633352687575....\n",
      "Validation loss: 0.6998326067242118....\n",
      "-----------------------------------\n",
      "Training loss: 0.6622130600282886....\n",
      "Validation loss: 0.6994665441116795....\n",
      "-----------------------------------\n",
      "Training loss: 0.6617635426877223....\n",
      "Validation loss: 0.6991011705494227....\n",
      "-----------------------------------\n",
      "Training loss: 0.6613146266350169....\n",
      "Validation loss: 0.6987366445878466....\n",
      "-----------------------------------\n",
      "Training loss: 0.660866495320188....\n",
      "Validation loss: 0.698372815860089....\n",
      "-----------------------------------\n",
      "Training loss: 0.6604192270370277....\n",
      "Validation loss: 0.6980097844295523....\n",
      "-----------------------------------\n",
      "Training loss: 0.6599728655512702....\n",
      "Validation loss: 0.6976475539437549....\n",
      "-----------------------------------\n",
      "Training loss: 0.6595273011463474....\n",
      "Validation loss: 0.6972860598478549....\n",
      "-----------------------------------\n",
      "Training loss: 0.6590824712051867....\n",
      "Validation loss: 0.6969253464951495....\n",
      "-----------------------------------\n",
      "Training loss: 0.6586384456947237....\n",
      "Validation loss: 0.6965653027861538....\n",
      "-----------------------------------\n",
      "Training loss: 0.6581952575346635....\n",
      "Validation loss: 0.6962059915039358....\n",
      "-----------------------------------\n",
      "Training loss: 0.6577528239641496....\n",
      "Validation loss: 0.6958474348204787....\n",
      "-----------------------------------\n",
      "Training loss: 0.6573110644030782....\n",
      "Validation loss: 0.695489673034732....\n",
      "-----------------------------------\n",
      "Training loss: 0.6568700480900175....\n",
      "Validation loss: 0.6951325767830748....\n",
      "-----------------------------------\n",
      "Training loss: 0.6564297924085146....\n",
      "Validation loss: 0.6947760954098464....\n",
      "-----------------------------------\n",
      "Training loss: 0.6559903824207198....\n",
      "Validation loss: 0.6944203188774749....\n",
      "-----------------------------------\n",
      "Training loss: 0.6555517745828028....\n",
      "Validation loss: 0.6940652514977517....\n",
      "-----------------------------------\n",
      "Training loss: 0.6551139706671145....\n",
      "Validation loss: 0.6937108783669425....\n",
      "-----------------------------------\n",
      "Training loss: 0.6546769899942243....\n",
      "Validation loss: 0.6933572346760402....\n",
      "-----------------------------------\n",
      "Training loss: 0.6542407306397031....\n",
      "Validation loss: 0.6930042502668792....\n",
      "-----------------------------------\n",
      "Training loss: 0.6538052766830903....\n",
      "Validation loss: 0.6926520076168324....\n",
      "-----------------------------------\n",
      "Training loss: 0.653370474183295....\n",
      "Validation loss: 0.6923005037060264....\n",
      "-----------------------------------\n",
      "Training loss: 0.652936406188929....\n",
      "Validation loss: 0.6919498320931216....\n",
      "-----------------------------------\n",
      "Training loss: 0.6525030717876361....\n",
      "Validation loss: 0.691599946381871....\n",
      "-----------------------------------\n",
      "Training loss: 0.6520704523634849....\n",
      "Validation loss: 0.6912505424732247....\n",
      "-----------------------------------\n",
      "Training loss: 0.6516385139643197....\n",
      "Validation loss: 0.6909016667614556....\n",
      "-----------------------------------\n",
      "Training loss: 0.6512072101308569....\n",
      "Validation loss: 0.6905533991622959....\n",
      "-----------------------------------\n",
      "Training loss: 0.650776744343629....\n",
      "Validation loss: 0.6902057477635742....\n",
      "-----------------------------------\n",
      "Training loss: 0.6503469663740924....\n",
      "Validation loss: 0.6898587663083202....\n",
      "-----------------------------------\n",
      "Training loss: 0.6499180322446467....\n",
      "Validation loss: 0.6895124419464732....\n",
      "-----------------------------------\n",
      "Training loss: 0.6494899371576986....\n",
      "Validation loss: 0.6891668466395063....\n",
      "-----------------------------------\n",
      "Training loss: 0.6490626413567328....\n",
      "Validation loss: 0.6888218559472818....\n",
      "-----------------------------------\n",
      "Training loss: 0.6486361681073123....\n",
      "Validation loss: 0.6884775240735239....\n",
      "-----------------------------------\n",
      "Training loss: 0.6482103207354405....\n",
      "Validation loss: 0.6881338933628754....\n",
      "-----------------------------------\n",
      "Training loss: 0.647785139649357....\n",
      "Validation loss: 0.6877909199741269....\n",
      "-----------------------------------\n",
      "Training loss: 0.6473605735151462....\n",
      "Validation loss: 0.6874485507752852....\n",
      "-----------------------------------\n",
      "Training loss: 0.6469367966184745....\n",
      "Validation loss: 0.6871068852122876....\n",
      "-----------------------------------\n",
      "Training loss: 0.6465138174560754....\n",
      "Validation loss: 0.6867658870418399....\n",
      "-----------------------------------\n",
      "Training loss: 0.6460916524609822....\n",
      "Validation loss: 0.68642569981024....\n",
      "-----------------------------------\n",
      "Training loss: 0.6456701665050137....\n",
      "Validation loss: 0.6860861428674084....\n",
      "-----------------------------------\n",
      "Training loss: 0.6452492645413714....\n",
      "Validation loss: 0.6857472208977962....\n",
      "-----------------------------------\n",
      "Training loss: 0.6448291559658357....\n",
      "Validation loss: 0.6854089834872511....\n",
      "-----------------------------------\n",
      "Training loss: 0.6444098832271205....\n",
      "Validation loss: 0.685071192125488....\n",
      "-----------------------------------\n",
      "Training loss: 0.6439911549951621....\n",
      "Validation loss: 0.6847340570351819....\n",
      "-----------------------------------\n",
      "Training loss: 0.6435731617330752....\n",
      "Validation loss: 0.6843975735778437....\n",
      "-----------------------------------\n",
      "Training loss: 0.6431559494846704....\n",
      "Validation loss: 0.684061751389628....\n",
      "-----------------------------------\n",
      "Training loss: 0.6427394790108009....\n",
      "Validation loss: 0.6837265565946661....\n",
      "-----------------------------------\n",
      "Training loss: 0.642323738296032....\n",
      "Validation loss: 0.683392013217665....\n",
      "-----------------------------------\n",
      "Training loss: 0.641908687863866....\n",
      "Validation loss: 0.6830579948780438....\n",
      "-----------------------------------\n",
      "Training loss: 0.6414942050364093....\n",
      "Validation loss: 0.6827244490936162....\n",
      "-----------------------------------\n",
      "Training loss: 0.6410804208758257....\n",
      "Validation loss: 0.6823914821753837....\n",
      "-----------------------------------\n",
      "Training loss: 0.6406674586140717....\n",
      "Validation loss: 0.6820591923607737....\n",
      "-----------------------------------\n",
      "Training loss: 0.6402551765271473....\n",
      "Validation loss: 0.681727582114831....\n",
      "-----------------------------------\n",
      "Training loss: 0.639843689281939....\n",
      "Validation loss: 0.6813966370138694....\n",
      "-----------------------------------\n",
      "Training loss: 0.639432990702468....\n",
      "Validation loss: 0.681066289897655....\n",
      "-----------------------------------\n",
      "Training loss: 0.639022907517666....\n",
      "Validation loss: 0.6807366345332853....\n",
      "-----------------------------------\n",
      "Training loss: 0.6386135840987375....\n",
      "Validation loss: 0.6804076770119325....\n",
      "-----------------------------------\n",
      "Training loss: 0.6382049775504268....\n",
      "Validation loss: 0.6800795159882105....\n",
      "-----------------------------------\n",
      "Training loss: 0.6377970835629603....\n",
      "Validation loss: 0.6797520053159506....\n",
      "-----------------------------------\n",
      "Training loss: 0.6373898649883865....\n",
      "Validation loss: 0.6794250925784794....\n",
      "-----------------------------------\n",
      "Training loss: 0.6369833472967518....\n",
      "Validation loss: 0.6790988251608635....\n",
      "-----------------------------------\n",
      "Training loss: 0.6365775396686315....\n",
      "Validation loss: 0.6787731590765919....\n",
      "-----------------------------------\n",
      "Training loss: 0.6361726132690144....\n",
      "Validation loss: 0.6784480534374104....\n",
      "-----------------------------------\n",
      "Training loss: 0.6357684900784325....\n",
      "Validation loss: 0.6781237240539342....\n",
      "-----------------------------------\n",
      "Training loss: 0.6353650044236143....\n",
      "Validation loss: 0.6777999759743616....\n",
      "-----------------------------------\n",
      "Training loss: 0.6349622246200347....\n",
      "Validation loss: 0.6774768169798628....\n",
      "-----------------------------------\n",
      "Training loss: 0.6345600888649964....\n",
      "Validation loss: 0.6771542362403489....\n",
      "-----------------------------------\n",
      "Training loss: 0.6341586940587877....\n",
      "Validation loss: 0.6768322434829619....\n",
      "-----------------------------------\n",
      "Training loss: 0.6337580384820362....\n",
      "Validation loss: 0.6765109246442114....\n",
      "-----------------------------------\n",
      "Training loss: 0.6333580597715283....\n",
      "Validation loss: 0.6761902541571101....\n",
      "-----------------------------------\n",
      "Training loss: 0.6329587730261722....\n",
      "Validation loss: 0.6758703158662989....\n",
      "-----------------------------------\n",
      "Training loss: 0.6325602161207486....\n",
      "Validation loss: 0.6755509943973195....\n",
      "-----------------------------------\n",
      "Training loss: 0.632162273103935....\n",
      "Validation loss: 0.6752322557714392....\n",
      "-----------------------------------\n",
      "Training loss: 0.631764972947933....\n",
      "Validation loss: 0.6749140919578673....\n",
      "-----------------------------------\n",
      "Training loss: 0.6313683184537148....\n",
      "Validation loss: 0.6745966676051828....\n",
      "-----------------------------------\n",
      "Training loss: 0.63097223635878....\n",
      "Validation loss: 0.6742798176609565....\n",
      "-----------------------------------\n",
      "Training loss: 0.6305769084568971....\n",
      "Validation loss: 0.673963521126911....\n",
      "-----------------------------------\n",
      "Training loss: 0.6301822168063499....\n",
      "Validation loss: 0.6736478155298944....\n",
      "-----------------------------------\n",
      "Training loss: 0.6297882046349179....\n",
      "Validation loss: 0.6733326435855177....\n",
      "-----------------------------------\n",
      "Training loss: 0.6293949435787795....\n",
      "Validation loss: 0.6730181476460659....\n",
      "-----------------------------------\n",
      "Training loss: 0.629002379838214....\n",
      "Validation loss: 0.6727042943010504....\n",
      "-----------------------------------\n",
      "Training loss: 0.6286104958046941....\n",
      "Validation loss: 0.6723908964291885....\n",
      "-----------------------------------\n",
      "Training loss: 0.628219268934212....\n",
      "Validation loss: 0.6720782161961701....\n",
      "-----------------------------------\n",
      "Training loss: 0.6278286575327533....\n",
      "Validation loss: 0.6717660290886055....\n",
      "-----------------------------------\n",
      "Training loss: 0.6274387085439038....\n",
      "Validation loss: 0.6714545964843855....\n",
      "-----------------------------------\n",
      "Training loss: 0.6270493131365489....\n",
      "Validation loss: 0.6711435627319761....\n",
      "-----------------------------------\n",
      "Training loss: 0.6266605994867042....\n",
      "Validation loss: 0.6708330525686627....\n",
      "-----------------------------------\n",
      "Training loss: 0.626272598013744....\n",
      "Validation loss: 0.6705230135268705....\n",
      "-----------------------------------\n",
      "Training loss: 0.6258852952513396....\n",
      "Validation loss: 0.6702136350182897....\n",
      "-----------------------------------\n",
      "Training loss: 0.6254986214440431....\n",
      "Validation loss: 0.6699048850306927....\n",
      "-----------------------------------\n",
      "Training loss: 0.6251125903210691....\n",
      "Validation loss: 0.6695966086085526....\n",
      "-----------------------------------\n",
      "Training loss: 0.6247272300694363....\n",
      "Validation loss: 0.6692887572391455....\n",
      "-----------------------------------\n",
      "Training loss: 0.6243424995033326....\n",
      "Validation loss: 0.6689815185082019....\n",
      "-----------------------------------\n",
      "Training loss: 0.6239584860932604....\n",
      "Validation loss: 0.6686747438149202....\n",
      "-----------------------------------\n",
      "Training loss: 0.6235751679615489....\n",
      "Validation loss: 0.6683686449541283....\n",
      "-----------------------------------\n",
      "Training loss: 0.6231925297139619....\n",
      "Validation loss: 0.6680631111690114....\n",
      "-----------------------------------\n",
      "Training loss: 0.6228106166255237....\n",
      "Validation loss: 0.667758174071058....\n",
      "-----------------------------------\n",
      "Training loss: 0.6224293394930644....\n",
      "Validation loss: 0.6674536783419173....\n",
      "-----------------------------------\n",
      "Training loss: 0.6220487069135692....\n",
      "Validation loss: 0.6671497830552593....\n",
      "-----------------------------------\n",
      "Training loss: 0.6216686696819922....\n",
      "Validation loss: 0.6668463871511385....\n",
      "-----------------------------------\n",
      "Training loss: 0.6212892684951933....\n",
      "Validation loss: 0.666543612537854....\n",
      "-----------------------------------\n",
      "Training loss: 0.620910509402558....\n",
      "Validation loss: 0.6662412799508783....\n",
      "-----------------------------------\n",
      "Training loss: 0.6205323713369595....\n",
      "Validation loss: 0.6659395191095131....\n",
      "-----------------------------------\n",
      "Training loss: 0.6201548801091294....\n",
      "Validation loss: 0.665638247488981....\n",
      "-----------------------------------\n",
      "Training loss: 0.6197780013245788....\n",
      "Validation loss: 0.6653375963054947....\n",
      "-----------------------------------\n",
      "Training loss: 0.6194017423333686....\n",
      "Validation loss: 0.6650374978333257....\n",
      "-----------------------------------\n",
      "Training loss: 0.619026117139307....\n",
      "Validation loss: 0.6647377926412514....\n",
      "-----------------------------------\n",
      "Training loss: 0.6186510548832729....\n",
      "Validation loss: 0.6644388109063207....\n",
      "-----------------------------------\n",
      "Training loss: 0.6182766243433404....\n",
      "Validation loss: 0.6641403160478695....\n",
      "-----------------------------------\n",
      "Training loss: 0.6179028611773855....\n",
      "Validation loss: 0.6638425095890073....\n",
      "-----------------------------------\n",
      "Training loss: 0.6175297133577631....\n",
      "Validation loss: 0.6635451342634305....\n",
      "-----------------------------------\n",
      "Training loss: 0.6171571998540163....\n",
      "Validation loss: 0.6632484789203287....\n",
      "-----------------------------------\n",
      "Training loss: 0.6167853724628715....\n",
      "Validation loss: 0.662952361420287....\n",
      "-----------------------------------\n",
      "Training loss: 0.6164142331633268....\n",
      "Validation loss: 0.6626566887464298....\n",
      "-----------------------------------\n",
      "Training loss: 0.6160436990186197....\n",
      "Validation loss: 0.662361652954597....\n",
      "-----------------------------------\n",
      "Training loss: 0.6156737821157029....\n",
      "Validation loss: 0.6620671252067392....\n",
      "-----------------------------------\n",
      "Training loss: 0.6153044702374967....\n",
      "Validation loss: 0.6617731362760728....\n",
      "-----------------------------------\n",
      "Training loss: 0.6149358263243525....\n",
      "Validation loss: 0.661479731136148....\n",
      "-----------------------------------\n",
      "Training loss: 0.6145678042576642....\n",
      "Validation loss: 0.661186937160056....\n",
      "-----------------------------------\n",
      "Training loss: 0.6142003683587468....\n",
      "Validation loss: 0.6608946306127136....\n",
      "-----------------------------------\n",
      "Training loss: 0.6138335275314577....\n",
      "Validation loss: 0.6606030565157015....\n",
      "-----------------------------------\n",
      "Training loss: 0.6134672710498316....\n",
      "Validation loss: 0.6603120255408051....\n",
      "-----------------------------------\n",
      "Training loss: 0.6131016140512103....\n",
      "Validation loss: 0.6600214967783405....\n",
      "-----------------------------------\n",
      "Training loss: 0.6127365898507026....\n",
      "Validation loss: 0.659731361570843....\n",
      "-----------------------------------\n",
      "Training loss: 0.6123722623806322....\n",
      "Validation loss: 0.6594418798476518....\n",
      "-----------------------------------\n",
      "Training loss: 0.6120085389589346....\n",
      "Validation loss: 0.6591529071632328....\n",
      "-----------------------------------\n",
      "Training loss: 0.6116454612485027....\n",
      "Validation loss: 0.6588642139719907....\n",
      "-----------------------------------\n",
      "Training loss: 0.6112830196030018....\n",
      "Validation loss: 0.6585761420833004....\n",
      "-----------------------------------\n",
      "Training loss: 0.6109212275376774....\n",
      "Validation loss: 0.6582885255082683....\n",
      "-----------------------------------\n",
      "Training loss: 0.6105600232535101....\n",
      "Validation loss: 0.6580014584561047....\n",
      "-----------------------------------\n",
      "Training loss: 0.6101992857540602....\n",
      "Validation loss: 0.6577147037380373....\n",
      "-----------------------------------\n",
      "Training loss: 0.6098390592608124....\n",
      "Validation loss: 0.6574285369268866....\n",
      "-----------------------------------\n",
      "Training loss: 0.6094794423879633....\n",
      "Validation loss: 0.6571428260370543....\n",
      "-----------------------------------\n",
      "Training loss: 0.6091204553795699....\n",
      "Validation loss: 0.6568577521990162....\n",
      "-----------------------------------\n",
      "Training loss: 0.6087620804839357....\n",
      "Validation loss: 0.6565731690459946....\n",
      "-----------------------------------\n",
      "Training loss: 0.6084042498992951....\n",
      "Validation loss: 0.6562889886627226....\n",
      "-----------------------------------\n",
      "Training loss: 0.6080470277517254....\n",
      "Validation loss: 0.6560054316281761....\n",
      "-----------------------------------\n",
      "Training loss: 0.6076903788946451....\n",
      "Validation loss: 0.655722357717583....\n",
      "-----------------------------------\n",
      "Training loss: 0.6073343435526173....\n",
      "Validation loss: 0.6554399076796545....\n",
      "-----------------------------------\n",
      "Training loss: 0.6069787808901452....\n",
      "Validation loss: 0.6551577245403262....\n",
      "-----------------------------------\n",
      "Training loss: 0.6066237609845994....\n",
      "Validation loss: 0.6548762027349521....\n",
      "-----------------------------------\n",
      "Training loss: 0.6062693894366545....\n",
      "Validation loss: 0.6545952666615485....\n",
      "-----------------------------------\n",
      "Training loss: 0.6059156292975281....\n",
      "Validation loss: 0.6543147785641854....\n",
      "-----------------------------------\n",
      "Training loss: 0.6055624479590191....\n",
      "Validation loss: 0.6540349262856631....\n",
      "-----------------------------------\n",
      "Training loss: 0.6052098584972476....\n",
      "Validation loss: 0.6537555135715664....\n",
      "-----------------------------------\n",
      "Training loss: 0.6048577523656457....\n",
      "Validation loss: 0.6534767077856061....\n",
      "-----------------------------------\n",
      "Training loss: 0.6045062464179036....\n",
      "Validation loss: 0.6531983588242484....\n",
      "-----------------------------------\n",
      "Training loss: 0.6041553513111078....\n",
      "Validation loss: 0.6529206437845954....\n",
      "-----------------------------------\n",
      "Training loss: 0.6038049871311783....\n",
      "Validation loss: 0.6526434017418646....\n",
      "-----------------------------------\n",
      "Training loss: 0.6034551930543615....\n",
      "Validation loss: 0.6523667522961776....\n",
      "-----------------------------------\n",
      "Training loss: 0.6031059631207557....\n",
      "Validation loss: 0.6520905627959045....\n",
      "-----------------------------------\n",
      "Training loss: 0.6027572775692628....\n",
      "Validation loss: 0.6518149773494163....\n",
      "-----------------------------------\n",
      "Training loss: 0.6024091207482287....\n",
      "Validation loss: 0.651539770301474....\n",
      "-----------------------------------\n",
      "Training loss: 0.60206148343714....\n",
      "Validation loss: 0.6512651814794408....\n",
      "-----------------------------------\n",
      "Training loss: 0.6017144027914973....\n",
      "Validation loss: 0.6509908835679922....\n",
      "-----------------------------------\n",
      "Training loss: 0.6013678857673798....\n",
      "Validation loss: 0.6507172078390886....\n",
      "-----------------------------------\n",
      "Training loss: 0.6010219315548039....\n",
      "Validation loss: 0.6504438249779898....\n",
      "-----------------------------------\n",
      "Training loss: 0.6006765595693779....\n",
      "Validation loss: 0.6501710783457085....\n",
      "-----------------------------------\n",
      "Training loss: 0.6003317686188187....\n",
      "Validation loss: 0.6498987551215981....\n",
      "-----------------------------------\n",
      "Training loss: 0.5999875698568544....\n",
      "Validation loss: 0.6496270314406712....\n",
      "-----------------------------------\n",
      "Training loss: 0.599643912600409....\n",
      "Validation loss: 0.6493557170604692....\n",
      "-----------------------------------\n",
      "Training loss: 0.5993007601617217....\n",
      "Validation loss: 0.6490849448013392....\n",
      "-----------------------------------\n",
      "Training loss: 0.5989580645138681....\n",
      "Validation loss: 0.6488146193155708....\n",
      "-----------------------------------\n",
      "Training loss: 0.5986158710169336....\n",
      "Validation loss: 0.6485448646760115....\n",
      "-----------------------------------\n",
      "Training loss: 0.5982741172191739....\n",
      "Validation loss: 0.6482755333284838....\n",
      "-----------------------------------\n",
      "Training loss: 0.5979329070745323....\n",
      "Validation loss: 0.6480066969809192....\n",
      "-----------------------------------\n",
      "Training loss: 0.5975922870501393....\n",
      "Validation loss: 0.6477384406399922....\n",
      "-----------------------------------\n",
      "Training loss: 0.5972522854338335....\n",
      "Validation loss: 0.6474708174066158....\n",
      "-----------------------------------\n",
      "Training loss: 0.5969128561942779....\n",
      "Validation loss: 0.6472035638313841....\n",
      "-----------------------------------\n",
      "Training loss: 0.5965739777583754....\n",
      "Validation loss: 0.6469367899060378....\n",
      "-----------------------------------\n",
      "Training loss: 0.5962355056135619....\n",
      "Validation loss: 0.6466704120401436....\n",
      "-----------------------------------\n",
      "Training loss: 0.5958975629836202....\n",
      "Validation loss: 0.6464044361357044....\n",
      "-----------------------------------\n",
      "Training loss: 0.595560178650731....\n",
      "Validation loss: 0.64613909620599....\n",
      "-----------------------------------\n",
      "Training loss: 0.5952233175155737....\n",
      "Validation loss: 0.6458740364624187....\n",
      "-----------------------------------\n",
      "Training loss: 0.5948869677574253....\n",
      "Validation loss: 0.645609404005356....\n",
      "-----------------------------------\n",
      "Training loss: 0.5945511731788196....\n",
      "Validation loss: 0.6453451424811942....\n",
      "-----------------------------------\n",
      "Training loss: 0.5942158921149456....\n",
      "Validation loss: 0.6450814397919341....\n",
      "-----------------------------------\n",
      "Training loss: 0.5938812244189196....\n",
      "Validation loss: 0.6448180159746396....\n",
      "-----------------------------------\n",
      "Training loss: 0.5935471370398298....\n",
      "Validation loss: 0.6445552656019897....\n",
      "-----------------------------------\n",
      "Training loss: 0.5932135185493228....\n",
      "Validation loss: 0.6442928295284054....\n",
      "-----------------------------------\n",
      "Training loss: 0.5928804186625379....\n",
      "Validation loss: 0.6440307879572644....\n",
      "-----------------------------------\n",
      "Training loss: 0.5925478473956597....\n",
      "Validation loss: 0.6437692571858138....\n",
      "-----------------------------------\n",
      "Training loss: 0.5922157681198617....\n",
      "Validation loss: 0.6435081990108291....\n",
      "-----------------------------------\n",
      "Training loss: 0.5918841856295624....\n",
      "Validation loss: 0.6432473491013491....\n",
      "-----------------------------------\n",
      "Training loss: 0.5915531601005681....\n",
      "Validation loss: 0.6429872693305809....\n",
      "-----------------------------------\n",
      "Training loss: 0.5912226825598745....\n",
      "Validation loss: 0.6427274703505493....\n",
      "-----------------------------------\n",
      "Training loss: 0.5908927999859879....\n",
      "Validation loss: 0.6424681115058033....\n",
      "-----------------------------------\n",
      "Training loss: 0.5905634743920093....\n",
      "Validation loss: 0.6422091969991566....\n",
      "-----------------------------------\n",
      "Training loss: 0.5902346472373813....\n",
      "Validation loss: 0.6419508680024448....\n",
      "-----------------------------------\n",
      "Training loss: 0.5899063216745021....\n",
      "Validation loss: 0.6416929424754121....\n",
      "-----------------------------------\n",
      "Training loss: 0.5895785112920642....\n",
      "Validation loss: 0.641435555726043....\n",
      "-----------------------------------\n",
      "Training loss: 0.5892513016057185....\n",
      "Validation loss: 0.6411785447882542....\n",
      "-----------------------------------\n",
      "Training loss: 0.588924680906304....\n",
      "Validation loss: 0.64092202129983....\n",
      "-----------------------------------\n",
      "Training loss: 0.5885985692007109....\n",
      "Validation loss: 0.6406660158463237....\n",
      "-----------------------------------\n",
      "Training loss: 0.588273047108469....\n",
      "Validation loss: 0.6404102867746528....\n",
      "-----------------------------------\n",
      "Training loss: 0.5879480566012572....\n",
      "Validation loss: 0.6401551015889....\n",
      "-----------------------------------\n",
      "Training loss: 0.587623525089158....\n",
      "Validation loss: 0.6399003133696017....\n",
      "-----------------------------------\n",
      "Training loss: 0.5872994270135575....\n",
      "Validation loss: 0.6396460215868645....\n",
      "-----------------------------------\n",
      "Training loss: 0.5869758632679608....\n",
      "Validation loss: 0.6393920946121888....\n",
      "-----------------------------------\n",
      "Training loss: 0.5866528389368575....\n",
      "Validation loss: 0.6391387326735294....\n",
      "-----------------------------------\n",
      "Training loss: 0.5863303588221044....\n",
      "Validation loss: 0.6388856765260466....\n",
      "-----------------------------------\n",
      "Training loss: 0.5860083538900501....\n",
      "Validation loss: 0.6386330959146392....\n",
      "-----------------------------------\n",
      "Training loss: 0.585686869119816....\n",
      "Validation loss: 0.6383808660284956....\n",
      "-----------------------------------\n",
      "Training loss: 0.5853658890498378....\n",
      "Validation loss: 0.6381291404423819....\n",
      "-----------------------------------\n",
      "Training loss: 0.5850454192296053....\n",
      "Validation loss: 0.637877840355753....\n",
      "-----------------------------------\n",
      "Training loss: 0.584725443456926....\n",
      "Validation loss: 0.6376270511703823....\n",
      "-----------------------------------\n",
      "Training loss: 0.5844059659031728....\n",
      "Validation loss: 0.6373768066812256....\n",
      "-----------------------------------\n",
      "Training loss: 0.5840870318167483....\n",
      "Validation loss: 0.6371269035242002....\n",
      "-----------------------------------\n",
      "Training loss: 0.5837686221881538....\n",
      "Validation loss: 0.6368776275660757....\n",
      "-----------------------------------\n",
      "Training loss: 0.5834507153915711....\n",
      "Validation loss: 0.63662882649228....\n",
      "-----------------------------------\n",
      "Training loss: 0.5831333777481236....\n",
      "Validation loss: 0.6363802563488052....\n",
      "-----------------------------------\n",
      "Training loss: 0.5828165687507968....\n",
      "Validation loss: 0.6361322719195853....\n",
      "-----------------------------------\n",
      "Training loss: 0.5825002524990938....\n",
      "Validation loss: 0.6358847071982429....\n",
      "-----------------------------------\n",
      "Training loss: 0.5821844363909108....\n",
      "Validation loss: 0.6356374943430256....\n",
      "-----------------------------------\n",
      "Training loss: 0.581869136346569....\n",
      "Validation loss: 0.6353907360159134....\n",
      "-----------------------------------\n",
      "Training loss: 0.5815542668693324....\n",
      "Validation loss: 0.6351444217874158....\n",
      "-----------------------------------\n",
      "Training loss: 0.5812398624489323....\n",
      "Validation loss: 0.6348985426192227....\n",
      "-----------------------------------\n",
      "Training loss: 0.5809258801871742....\n",
      "Validation loss: 0.6346531050006837....\n",
      "-----------------------------------\n",
      "Training loss: 0.5806123872441554....\n",
      "Validation loss: 0.6344080174931842....\n",
      "-----------------------------------\n",
      "Training loss: 0.5802993488547682....\n",
      "Validation loss: 0.634163426469111....\n",
      "-----------------------------------\n",
      "Training loss: 0.5799867711188551....\n",
      "Validation loss: 0.633919292211132....\n",
      "-----------------------------------\n",
      "Training loss: 0.5796746530849539....\n",
      "Validation loss: 0.6336754286110843....\n",
      "-----------------------------------\n",
      "Training loss: 0.5793630162557338....\n",
      "Validation loss: 0.6334320697914643....\n",
      "-----------------------------------\n",
      "Training loss: 0.5790518375995771....\n",
      "Validation loss: 0.6331891562446439....\n",
      "-----------------------------------\n",
      "Training loss: 0.5787411598494951....\n",
      "Validation loss: 0.6329465494241285....\n",
      "-----------------------------------\n",
      "Training loss: 0.5784309532325549....\n",
      "Validation loss: 0.6327045309834107....\n",
      "-----------------------------------\n",
      "Training loss: 0.5781211820899845....\n",
      "Validation loss: 0.6324629600611382....\n",
      "-----------------------------------\n",
      "Training loss: 0.5778119107959138....\n",
      "Validation loss: 0.6322217980102934....\n",
      "-----------------------------------\n",
      "Training loss: 0.577503096044968....\n",
      "Validation loss: 0.6319808622123398....\n",
      "-----------------------------------\n",
      "Training loss: 0.5771947091652274....\n",
      "Validation loss: 0.6317402684233911....\n",
      "-----------------------------------\n",
      "Training loss: 0.5768867525236214....\n",
      "Validation loss: 0.6315000657555413....\n",
      "-----------------------------------\n",
      "Training loss: 0.5765792162254059....\n",
      "Validation loss: 0.631260287971633....\n",
      "-----------------------------------\n",
      "Training loss: 0.5762721235096406....\n",
      "Validation loss: 0.6310206932979822....\n",
      "-----------------------------------\n",
      "Training loss: 0.5759654708171681....\n",
      "Validation loss: 0.6307816352031119....\n",
      "-----------------------------------\n",
      "Training loss: 0.5756592962872522....\n",
      "Validation loss: 0.6305429170241809....\n",
      "-----------------------------------\n",
      "Training loss: 0.5753535958569361....\n",
      "Validation loss: 0.6303045723901209....\n",
      "-----------------------------------\n",
      "Training loss: 0.5750483480884943....\n",
      "Validation loss: 0.6300665415009509....\n",
      "-----------------------------------\n",
      "Training loss: 0.5747435485456149....\n",
      "Validation loss: 0.6298289547436423....\n",
      "-----------------------------------\n",
      "Training loss: 0.574439171940293....\n",
      "Validation loss: 0.6295916990380614....\n",
      "-----------------------------------\n",
      "Training loss: 0.5741352335378238....\n",
      "Validation loss: 0.6293549462965502....\n",
      "-----------------------------------\n",
      "Training loss: 0.5738317822364682....\n",
      "Validation loss: 0.6291185943074019....\n",
      "-----------------------------------\n",
      "Training loss: 0.5735287507110536....\n",
      "Validation loss: 0.6288826643673421....\n",
      "-----------------------------------\n",
      "Training loss: 0.5732261363318376....\n",
      "Validation loss: 0.6286470924266373....\n",
      "-----------------------------------\n",
      "Training loss: 0.5729240268329148....\n",
      "Validation loss: 0.6284121218745038....\n",
      "-----------------------------------\n",
      "Training loss: 0.5726224260375775....\n",
      "Validation loss: 0.6281776202745717....\n",
      "-----------------------------------\n",
      "Training loss: 0.5723212691560678....\n",
      "Validation loss: 0.6279434324456656....\n",
      "-----------------------------------\n",
      "Training loss: 0.5720205737378076....\n",
      "Validation loss: 0.627709732434526....\n",
      "-----------------------------------\n",
      "Training loss: 0.5717203220116278....\n",
      "Validation loss: 0.6274761915923376....\n",
      "-----------------------------------\n",
      "Training loss: 0.5714204296627668....\n",
      "Validation loss: 0.6272430128593528....\n",
      "-----------------------------------\n",
      "Training loss: 0.5711209101303176....\n",
      "Validation loss: 0.627010049320884....\n",
      "-----------------------------------\n",
      "Training loss: 0.5708217159599406....\n",
      "Validation loss: 0.6267775252474326....\n",
      "-----------------------------------\n",
      "Training loss: 0.5705229275703596....\n",
      "Validation loss: 0.6265454189632685....\n",
      "-----------------------------------\n",
      "Training loss: 0.5702245646288255....\n",
      "Validation loss: 0.6263137245091728....\n",
      "-----------------------------------\n",
      "Training loss: 0.5699266506648268....\n",
      "Validation loss: 0.6260822762747031....\n",
      "-----------------------------------\n",
      "Training loss: 0.5696291394589229....\n",
      "Validation loss: 0.6258511834838233....\n",
      "-----------------------------------\n",
      "Training loss: 0.5693320308325437....\n",
      "Validation loss: 0.625620424557912....\n",
      "-----------------------------------\n",
      "Training loss: 0.5690353472961187....\n",
      "Validation loss: 0.6253900577256902....\n",
      "-----------------------------------\n",
      "Training loss: 0.5687391166803408....\n",
      "Validation loss: 0.6251600673355726....\n",
      "-----------------------------------\n",
      "Training loss: 0.5684433266883888....\n",
      "Validation loss: 0.6249304561270501....\n",
      "-----------------------------------\n",
      "Training loss: 0.5681479683238776....\n",
      "Validation loss: 0.6247010233254436....\n",
      "-----------------------------------\n",
      "Training loss: 0.5678530522769183....\n",
      "Validation loss: 0.6244719551029498....\n",
      "-----------------------------------\n",
      "Training loss: 0.5675585980396345....\n",
      "Validation loss: 0.6242433030431933....\n",
      "-----------------------------------\n",
      "Training loss: 0.5672645858407268....\n",
      "Validation loss: 0.6240149954090347....\n",
      "-----------------------------------\n",
      "Training loss: 0.5669710263290534....\n",
      "Validation loss: 0.6237870535247293....\n",
      "-----------------------------------\n",
      "Training loss: 0.5666778814350979....\n",
      "Validation loss: 0.6235594998334846....\n",
      "-----------------------------------\n",
      "Training loss: 0.5663851802257769....\n",
      "Validation loss: 0.6233323949942249....\n",
      "-----------------------------------\n",
      "Training loss: 0.5660929177540837....\n",
      "Validation loss: 0.6231056276839124....\n",
      "-----------------------------------\n",
      "Training loss: 0.5658010846902904....\n",
      "Validation loss: 0.6228791239887993....\n",
      "-----------------------------------\n",
      "Training loss: 0.5655096541648863....\n",
      "Validation loss: 0.6226530785407883....\n",
      "-----------------------------------\n",
      "Training loss: 0.5652186075847285....\n",
      "Validation loss: 0.6224273856620421....\n",
      "-----------------------------------\n",
      "Training loss: 0.564927987012466....\n",
      "Validation loss: 0.6222020249633684....\n",
      "-----------------------------------\n",
      "Training loss: 0.5646378040670726....\n",
      "Validation loss: 0.621977131577226....\n",
      "-----------------------------------\n",
      "Training loss: 0.5643480969564945....\n",
      "Validation loss: 0.6217524863046601....\n",
      "-----------------------------------\n",
      "Training loss: 0.564058788929378....\n",
      "Validation loss: 0.6215281605805439....\n",
      "-----------------------------------\n",
      "Training loss: 0.5637698236336285....\n",
      "Validation loss: 0.6213042993969213....\n",
      "-----------------------------------\n",
      "Training loss: 0.5634812101449215....\n",
      "Validation loss: 0.6210807647754789....\n",
      "-----------------------------------\n",
      "Training loss: 0.5631930455040043....\n",
      "Validation loss: 0.6208577548236087....\n",
      "-----------------------------------\n",
      "Training loss: 0.5629052811043854....\n",
      "Validation loss: 0.6206350716206624....\n",
      "-----------------------------------\n",
      "Training loss: 0.562617846882171....\n",
      "Validation loss: 0.620412780394098....\n",
      "-----------------------------------\n",
      "Training loss: 0.5623307847060045....\n",
      "Validation loss: 0.6201908246721336....\n",
      "-----------------------------------\n",
      "Training loss: 0.5620441430391243....\n",
      "Validation loss: 0.619969313632071....\n",
      "-----------------------------------\n",
      "Training loss: 0.5617579568763825....\n",
      "Validation loss: 0.6197481599213007....\n",
      "-----------------------------------\n",
      "Training loss: 0.5614721997617185....\n",
      "Validation loss: 0.619527353187113....\n",
      "-----------------------------------\n",
      "Training loss: 0.5611868685622127....\n",
      "Validation loss: 0.6193069134738146....\n",
      "-----------------------------------\n",
      "Training loss: 0.5609019690677177....\n",
      "Validation loss: 0.6190867754963474....\n",
      "-----------------------------------\n",
      "Training loss: 0.5606174599299114....\n",
      "Validation loss: 0.618866992951039....\n",
      "-----------------------------------\n",
      "Training loss: 0.5603333541507606....\n",
      "Validation loss: 0.6186474548952772....\n",
      "-----------------------------------\n",
      "Training loss: 0.5600496259636527....\n",
      "Validation loss: 0.6184283903106012....\n",
      "-----------------------------------\n",
      "Training loss: 0.5597663099005326....\n",
      "Validation loss: 0.6182096454336168....\n",
      "-----------------------------------\n",
      "Training loss: 0.5594833609242045....\n",
      "Validation loss: 0.6179912228473071....\n",
      "-----------------------------------\n",
      "Training loss: 0.5592007762657598....\n",
      "Validation loss: 0.6177732489359019....\n",
      "-----------------------------------\n",
      "Training loss: 0.5589186222148385....\n",
      "Validation loss: 0.6175556267336814....\n",
      "-----------------------------------\n",
      "Training loss: 0.5586368827031384....\n",
      "Validation loss: 0.6173383986631927....\n",
      "-----------------------------------\n",
      "Training loss: 0.5583555496188839....\n",
      "Validation loss: 0.6171214739307238....\n",
      "-----------------------------------\n",
      "Training loss: 0.558074467942616....\n",
      "Validation loss: 0.616904780726533....\n",
      "-----------------------------------\n",
      "Training loss: 0.5577935915048526....\n",
      "Validation loss: 0.6166885847943747....\n",
      "-----------------------------------\n",
      "Training loss: 0.5575131465602822....\n",
      "Validation loss: 0.6164727740300301....\n",
      "-----------------------------------\n",
      "Training loss: 0.557233112958445....\n",
      "Validation loss: 0.6162572935167382....\n",
      "-----------------------------------\n",
      "Training loss: 0.5569534917117845....\n",
      "Validation loss: 0.616042086661755....\n",
      "-----------------------------------\n",
      "Training loss: 0.5566742518012991....\n",
      "Validation loss: 0.6158273198704673....\n",
      "-----------------------------------\n",
      "Training loss: 0.5563954147323088....\n",
      "Validation loss: 0.6156129424694597....\n",
      "-----------------------------------\n",
      "Training loss: 0.5561169536201562....\n",
      "Validation loss: 0.6153989086322054....\n",
      "-----------------------------------\n",
      "Training loss: 0.5558388411009707....\n",
      "Validation loss: 0.6151851359037488....\n",
      "-----------------------------------\n",
      "Training loss: 0.5555611192847276....\n",
      "Validation loss: 0.6149718051518435....\n",
      "-----------------------------------\n",
      "Training loss: 0.5552838087398722....\n",
      "Validation loss: 0.6147587735641616....\n",
      "-----------------------------------\n",
      "Training loss: 0.5550069090816061....\n",
      "Validation loss: 0.6145460191611501....\n",
      "-----------------------------------\n",
      "Training loss: 0.5547303923387318....\n",
      "Validation loss: 0.614333433545615....\n",
      "-----------------------------------\n",
      "Training loss: 0.5544542509424759....\n",
      "Validation loss: 0.6141212308667464....\n",
      "-----------------------------------\n",
      "Training loss: 0.5541784876487934....\n",
      "Validation loss: 0.61390936834154....\n",
      "-----------------------------------\n",
      "Training loss: 0.553903053992619....\n",
      "Validation loss: 0.6136978053729498....\n",
      "-----------------------------------\n",
      "Training loss: 0.5536279950910791....\n",
      "Validation loss: 0.6134865706637492....\n",
      "-----------------------------------\n",
      "Training loss: 0.5533533281826717....\n",
      "Validation loss: 0.6132757490003189....\n",
      "-----------------------------------\n",
      "Training loss: 0.5530790283710535....\n",
      "Validation loss: 0.6130652771775223....\n",
      "-----------------------------------\n",
      "Training loss: 0.5528050874291014....\n",
      "Validation loss: 0.6128551197103761....\n",
      "-----------------------------------\n",
      "Training loss: 0.55253155523835....\n",
      "Validation loss: 0.6126453571382451....\n",
      "-----------------------------------\n",
      "Training loss: 0.5522584344116096....\n",
      "Validation loss: 0.6124358501928725....\n",
      "-----------------------------------\n",
      "Training loss: 0.5519857189945337....\n",
      "Validation loss: 0.6122268108046257....\n",
      "-----------------------------------\n",
      "Training loss: 0.5517134043787111....\n",
      "Validation loss: 0.6120180575262242....\n",
      "-----------------------------------\n",
      "Training loss: 0.5514415270557288....\n",
      "Validation loss: 0.6118096539979417....\n",
      "-----------------------------------\n",
      "Training loss: 0.551170057526899....\n",
      "Validation loss: 0.6116015719870217....\n",
      "-----------------------------------\n",
      "Training loss: 0.5508989765935869....\n",
      "Validation loss: 0.6113937525143178....\n",
      "-----------------------------------\n",
      "Training loss: 0.5506282670715528....\n",
      "Validation loss: 0.6111864081595898....\n",
      "-----------------------------------\n",
      "Training loss: 0.5503579517929741....\n",
      "Validation loss: 0.610979460719054....\n",
      "-----------------------------------\n",
      "Training loss: 0.5500880066796551....\n",
      "Validation loss: 0.6107728493234281....\n",
      "-----------------------------------\n",
      "Training loss: 0.5498184089006467....\n",
      "Validation loss: 0.6105666114166688....\n",
      "-----------------------------------\n",
      "Training loss: 0.5495491998290132....\n",
      "Validation loss: 0.6103607146084307....\n",
      "-----------------------------------\n",
      "Training loss: 0.5492803394362816....\n",
      "Validation loss: 0.6101550277950835....\n",
      "-----------------------------------\n",
      "Training loss: 0.5490118346514458....\n",
      "Validation loss: 0.6099497381585683....\n",
      "-----------------------------------\n",
      "Training loss: 0.548743677497211....\n",
      "Validation loss: 0.6097448408329356....\n",
      "-----------------------------------\n",
      "Training loss: 0.548475920938083....\n",
      "Validation loss: 0.6095402272038357....\n",
      "-----------------------------------\n",
      "Training loss: 0.5482085230269236....\n",
      "Validation loss: 0.6093359332199427....\n",
      "-----------------------------------\n",
      "Training loss: 0.5479414976839412....\n",
      "Validation loss: 0.6091319569069865....\n",
      "-----------------------------------\n",
      "Training loss: 0.5476748611653034....\n",
      "Validation loss: 0.6089283309984773....\n",
      "-----------------------------------\n",
      "Training loss: 0.5474086206503812....\n",
      "Validation loss: 0.6087250277687322....\n",
      "-----------------------------------\n",
      "Training loss: 0.5471427787675736....\n",
      "Validation loss: 0.608521821595631....\n",
      "-----------------------------------\n",
      "Training loss: 0.5468772877366255....\n",
      "Validation loss: 0.6083190511472717....\n",
      "-----------------------------------\n",
      "Training loss: 0.5466121386575816....\n",
      "Validation loss: 0.6081165924755694....\n",
      "-----------------------------------\n",
      "Training loss: 0.5463473179225492....\n",
      "Validation loss: 0.607914442781181....\n",
      "-----------------------------------\n",
      "Training loss: 0.5460828549373252....\n",
      "Validation loss: 0.6077125748359945....\n",
      "-----------------------------------\n",
      "Training loss: 0.5458187807278575....\n",
      "Validation loss: 0.6075110022135431....\n",
      "-----------------------------------\n",
      "Training loss: 0.5455550297614484....\n",
      "Validation loss: 0.6073097564746912....\n",
      "-----------------------------------\n",
      "Training loss: 0.5452916406940301....\n",
      "Validation loss: 0.6071087864018925....\n",
      "-----------------------------------\n",
      "Training loss: 0.545028620378581....\n",
      "Validation loss: 0.606908154082418....\n",
      "-----------------------------------\n",
      "Training loss: 0.5447659425547258....\n",
      "Validation loss: 0.6067078327911907....\n",
      "-----------------------------------\n",
      "Training loss: 0.5445036101005243....\n",
      "Validation loss: 0.6065077031797366....\n",
      "-----------------------------------\n",
      "Training loss: 0.5442416381063072....\n",
      "Validation loss: 0.6063079789346907....\n",
      "-----------------------------------\n",
      "Training loss: 0.5439800027385944....\n",
      "Validation loss: 0.6061085923572992....\n",
      "-----------------------------------\n",
      "Training loss: 0.5437187499666674....\n",
      "Validation loss: 0.6059095746431722....\n",
      "-----------------------------------\n",
      "Training loss: 0.5434578410628781....\n",
      "Validation loss: 0.6057107976929544....\n",
      "-----------------------------------\n",
      "Training loss: 0.5431972977059644....\n",
      "Validation loss: 0.605512354821117....\n",
      "-----------------------------------\n",
      "Training loss: 0.542937126674571....\n",
      "Validation loss: 0.6053141917458136....\n",
      "-----------------------------------\n",
      "Training loss: 0.5426773255003126....\n",
      "Validation loss: 0.6051163201099935....\n",
      "-----------------------------------\n",
      "Training loss: 0.5424178768244167....\n",
      "Validation loss: 0.6049187739409363....\n",
      "-----------------------------------\n",
      "Training loss: 0.5421587958768176....\n",
      "Validation loss: 0.6047215466152214....\n",
      "-----------------------------------\n",
      "Training loss: 0.5419000891164392....\n",
      "Validation loss: 0.6045246260575116....\n",
      "-----------------------------------\n",
      "Training loss: 0.5416417247187106....\n",
      "Validation loss: 0.6043280673853062....\n",
      "-----------------------------------\n",
      "Training loss: 0.5413837179440603....\n",
      "Validation loss: 0.6041318074262636....\n",
      "-----------------------------------\n",
      "Training loss: 0.5411260776813994....\n",
      "Validation loss: 0.6039357018008523....\n",
      "-----------------------------------\n",
      "Training loss: 0.5408687653071637....\n",
      "Validation loss: 0.6037400797177147....\n",
      "-----------------------------------\n",
      "Training loss: 0.5406118112081086....\n",
      "Validation loss: 0.6035447830243456....\n",
      "-----------------------------------\n",
      "Training loss: 0.5403551946407641....\n",
      "Validation loss: 0.6033497523910485....\n",
      "-----------------------------------\n",
      "Training loss: 0.5400989248023144....\n",
      "Validation loss: 0.6031550557926459....\n",
      "-----------------------------------\n",
      "Training loss: 0.5398430162726272....\n",
      "Validation loss: 0.6029606505924578....\n",
      "-----------------------------------\n",
      "Training loss: 0.5395874633220081....\n",
      "Validation loss: 0.6027665476046726....\n",
      "-----------------------------------\n",
      "Training loss: 0.5393322739577805....\n",
      "Validation loss: 0.602572719272593....\n",
      "-----------------------------------\n",
      "Training loss: 0.5390774365314113....\n",
      "Validation loss: 0.6023792165628736....\n",
      "-----------------------------------\n",
      "Training loss: 0.5388229407238343....\n",
      "Validation loss: 0.6021859138061225....\n",
      "-----------------------------------\n",
      "Training loss: 0.5385687429704508....\n",
      "Validation loss: 0.6019929249397794....\n",
      "-----------------------------------\n",
      "Training loss: 0.5383148953099858....\n",
      "Validation loss: 0.6018002421922378....\n",
      "-----------------------------------\n",
      "Training loss: 0.538061378900419....\n",
      "Validation loss: 0.6016078742052978....\n",
      "-----------------------------------\n",
      "Training loss: 0.5378082041555209....\n",
      "Validation loss: 0.6014158027819583....\n",
      "-----------------------------------\n",
      "Training loss: 0.5375553895368674....\n",
      "Validation loss: 0.6012240638576907....\n",
      "-----------------------------------\n",
      "Training loss: 0.5373028780541779....\n",
      "Validation loss: 0.6010326530263751....\n",
      "-----------------------------------\n",
      "Training loss: 0.5370506845082582....\n",
      "Validation loss: 0.6008415149267496....\n",
      "-----------------------------------\n",
      "Training loss: 0.5367988380632478....\n",
      "Validation loss: 0.6006507666272207....\n",
      "-----------------------------------\n",
      "Training loss: 0.5365472986369613....\n",
      "Validation loss: 0.6004602560126087....\n",
      "-----------------------------------\n",
      "Training loss: 0.5362960410110504....\n",
      "Validation loss: 0.6002700119568803....\n",
      "-----------------------------------\n",
      "Training loss: 0.5360451162472003....\n",
      "Validation loss: 0.6000801218846309....\n",
      "-----------------------------------\n",
      "Training loss: 0.5357945021193734....\n",
      "Validation loss: 0.5998904780013266....\n",
      "-----------------------------------\n",
      "Training loss: 0.535544175455694....\n",
      "Validation loss: 0.5997009219135696....\n",
      "-----------------------------------\n",
      "Training loss: 0.5352941887226966....\n",
      "Validation loss: 0.5995118739711109....\n",
      "-----------------------------------\n",
      "Training loss: 0.5350445412974222....\n",
      "Validation loss: 0.5993231446372006....\n",
      "-----------------------------------\n",
      "Training loss: 0.5347952586757974....\n",
      "Validation loss: 0.5991347542830793....\n",
      "-----------------------------------\n",
      "Training loss: 0.5345463350265431....\n",
      "Validation loss: 0.5989465734290956....\n",
      "-----------------------------------\n",
      "Training loss: 0.5342977524080664....\n",
      "Validation loss: 0.5987587738099048....\n",
      "-----------------------------------\n",
      "Training loss: 0.5340495153267273....\n",
      "Validation loss: 0.598571288407353....\n",
      "-----------------------------------\n",
      "Training loss: 0.5338015887233521....\n",
      "Validation loss: 0.5983840249032805....\n",
      "-----------------------------------\n",
      "Training loss: 0.5335539793385492....\n",
      "Validation loss: 0.5981971386608477....\n",
      "-----------------------------------\n",
      "Training loss: 0.5832026120701145....\n",
      "Validation loss: 0.49821756963626146....\n",
      "-----------------------------------\n",
      "Training loss: 0.5829060106592572....\n",
      "Validation loss: 0.49804808803780154....\n",
      "-----------------------------------\n",
      "Training loss: 0.5826115979545649....\n",
      "Validation loss: 0.4978791313478999....\n",
      "-----------------------------------\n",
      "Training loss: 0.5823191913225437....\n",
      "Validation loss: 0.4977105125753482....\n",
      "-----------------------------------\n",
      "Training loss: 0.5820285973307042....\n",
      "Validation loss: 0.4975422644051634....\n",
      "-----------------------------------\n",
      "Training loss: 0.5817395854196175....\n",
      "Validation loss: 0.49737435555128645....\n",
      "-----------------------------------\n",
      "Training loss: 0.5814521480484472....\n",
      "Validation loss: 0.49720673173788316....\n",
      "-----------------------------------\n",
      "Training loss: 0.5811661522653563....\n",
      "Validation loss: 0.49703930408270586....\n",
      "-----------------------------------\n",
      "Training loss: 0.5808814941406645....\n",
      "Validation loss: 0.4968721422303891....\n",
      "-----------------------------------\n",
      "Training loss: 0.580598093735865....\n",
      "Validation loss: 0.4967052446283177....\n",
      "-----------------------------------\n",
      "Training loss: 0.5803158664508941....\n",
      "Validation loss: 0.49653855782491285....\n",
      "-----------------------------------\n",
      "Training loss: 0.5800347560582425....\n",
      "Validation loss: 0.49637201232114553....\n",
      "-----------------------------------\n",
      "Training loss: 0.5797547027311698....\n",
      "Validation loss: 0.49620561841889277....\n",
      "-----------------------------------\n",
      "Training loss: 0.5794755799403529....\n",
      "Validation loss: 0.4960393475699856....\n",
      "-----------------------------------\n",
      "Training loss: 0.5791973725332694....\n",
      "Validation loss: 0.49587316224196915....\n",
      "-----------------------------------\n",
      "Training loss: 0.5789200109102651....\n",
      "Validation loss: 0.4957071586583611....\n",
      "-----------------------------------\n",
      "Training loss: 0.5786434175805032....\n",
      "Validation loss: 0.4955412920341412....\n",
      "-----------------------------------\n",
      "Training loss: 0.5783676298601979....\n",
      "Validation loss: 0.4953755764232995....\n",
      "-----------------------------------\n",
      "Training loss: 0.5780926453516622....\n",
      "Validation loss: 0.4952098889562461....\n",
      "-----------------------------------\n",
      "Training loss: 0.5778184190774976....\n",
      "Validation loss: 0.4950442860449453....\n",
      "-----------------------------------\n",
      "Training loss: 0.5775449003388312....\n",
      "Validation loss: 0.4948788088005324....\n",
      "-----------------------------------\n",
      "Training loss: 0.5772720170112066....\n",
      "Validation loss: 0.4947134533859613....\n",
      "-----------------------------------\n",
      "Training loss: 0.5769997474605447....\n",
      "Validation loss: 0.4945481092845125....\n",
      "-----------------------------------\n",
      "Training loss: 0.5767281852922536....\n",
      "Validation loss: 0.4943828089104713....\n",
      "-----------------------------------\n",
      "Training loss: 0.5764572400756786....\n",
      "Validation loss: 0.49421762961524096....\n",
      "-----------------------------------\n",
      "Training loss: 0.5761868551810455....\n",
      "Validation loss: 0.49405265408372173....\n",
      "-----------------------------------\n",
      "Training loss: 0.5759170361271311....\n",
      "Validation loss: 0.4938877548117853....\n",
      "-----------------------------------\n",
      "Training loss: 0.5756477604138998....\n",
      "Validation loss: 0.4937228991163544....\n",
      "-----------------------------------\n",
      "Training loss: 0.5753790410923985....\n",
      "Validation loss: 0.49355817768967786....\n",
      "-----------------------------------\n",
      "Training loss: 0.575110860665354....\n",
      "Validation loss: 0.49339359996411186....\n",
      "-----------------------------------\n",
      "Training loss: 0.574843189389311....\n",
      "Validation loss: 0.49322911493651783....\n",
      "-----------------------------------\n",
      "Training loss: 0.5745759877827042....\n",
      "Validation loss: 0.4930647495221432....\n",
      "-----------------------------------\n",
      "Training loss: 0.5743092755573556....\n",
      "Validation loss: 0.49290050819463926....\n",
      "-----------------------------------\n",
      "Training loss: 0.5740430742737037....\n",
      "Validation loss: 0.49273643999093775....\n",
      "-----------------------------------\n",
      "Training loss: 0.5737772983406183....\n",
      "Validation loss: 0.4925725039313224....\n",
      "-----------------------------------\n",
      "Training loss: 0.5735119222117087....\n",
      "Validation loss: 0.49240871400472436....\n",
      "-----------------------------------\n",
      "Training loss: 0.5732469819329488....\n",
      "Validation loss: 0.4922450944577266....\n",
      "-----------------------------------\n",
      "Training loss: 0.5729825078690618....\n",
      "Validation loss: 0.4920816412089922....\n",
      "-----------------------------------\n",
      "Training loss: 0.5727184288894344....\n",
      "Validation loss: 0.49191831899158855....\n",
      "-----------------------------------\n",
      "Training loss: 0.5724547874077281....\n",
      "Validation loss: 0.4917551341487025....\n",
      "-----------------------------------\n",
      "Training loss: 0.5721916089546008....\n",
      "Validation loss: 0.4915920586639096....\n",
      "-----------------------------------\n",
      "Training loss: 0.5719289059343168....\n",
      "Validation loss: 0.49142921077141416....\n",
      "-----------------------------------\n",
      "Training loss: 0.5716666371766209....\n",
      "Validation loss: 0.4912665474408553....\n",
      "-----------------------------------\n",
      "Training loss: 0.571404786709813....\n",
      "Validation loss: 0.49110405300338633....\n",
      "-----------------------------------\n",
      "Training loss: 0.5711433471301781....\n",
      "Validation loss: 0.4909417493073675....\n",
      "-----------------------------------\n",
      "Training loss: 0.5708823843627383....\n",
      "Validation loss: 0.4907797413066721....\n",
      "-----------------------------------\n",
      "Training loss: 0.5706219382207178....\n",
      "Validation loss: 0.49061788402227025....\n",
      "-----------------------------------\n",
      "Training loss: 0.5703619297921692....\n",
      "Validation loss: 0.4904561999228496....\n",
      "-----------------------------------\n",
      "Training loss: 0.570102367068961....\n",
      "Validation loss: 0.4902947370843238....\n",
      "-----------------------------------\n",
      "Training loss: 0.5698432054577057....\n",
      "Validation loss: 0.490133454509695....\n",
      "-----------------------------------\n",
      "Training loss: 0.569584435619262....\n",
      "Validation loss: 0.48997236347254297....\n",
      "-----------------------------------\n",
      "Training loss: 0.5693260258074008....\n",
      "Validation loss: 0.4898114317720857....\n",
      "-----------------------------------\n",
      "Training loss: 0.5690680674002099....\n",
      "Validation loss: 0.48965074454009155....\n",
      "-----------------------------------\n",
      "Training loss: 0.5688105046955709....\n",
      "Validation loss: 0.4894902361575567....\n",
      "-----------------------------------\n",
      "Training loss: 0.568553308924745....\n",
      "Validation loss: 0.4893300062595654....\n",
      "-----------------------------------\n",
      "Training loss: 0.568296473686321....\n",
      "Validation loss: 0.48917000322824644....\n",
      "-----------------------------------\n",
      "Training loss: 0.5680400109860068....\n",
      "Validation loss: 0.4890102254607863....\n",
      "-----------------------------------\n",
      "Training loss: 0.5677838985141375....\n",
      "Validation loss: 0.48885071073379316....\n",
      "-----------------------------------\n",
      "Training loss: 0.5675280237197855....\n",
      "Validation loss: 0.4886914626815476....\n",
      "-----------------------------------\n",
      "Training loss: 0.5672724987932883....\n",
      "Validation loss: 0.4885324453804182....\n",
      "-----------------------------------\n",
      "Training loss: 0.5670173509595027....\n",
      "Validation loss: 0.48837366346889877....\n",
      "-----------------------------------\n",
      "Training loss: 0.566762554511502....\n",
      "Validation loss: 0.4882150254184296....\n",
      "-----------------------------------\n",
      "Training loss: 0.5665081187454967....\n",
      "Validation loss: 0.48805651963172153....\n",
      "-----------------------------------\n",
      "Training loss: 0.5662540410089144....\n",
      "Validation loss: 0.4878981915115855....\n",
      "-----------------------------------\n",
      "Training loss: 0.5660002986033661....\n",
      "Validation loss: 0.4877400227655874....\n",
      "-----------------------------------\n",
      "Training loss: 0.5657468401061339....\n",
      "Validation loss: 0.4875820203237938....\n",
      "-----------------------------------\n",
      "Training loss: 0.5654937419918618....\n",
      "Validation loss: 0.487424235959816....\n",
      "-----------------------------------\n",
      "Training loss: 0.5652410133922225....\n",
      "Validation loss: 0.487266627390484....\n",
      "-----------------------------------\n",
      "Training loss: 0.5649886454083671....\n",
      "Validation loss: 0.48710927745552063....\n",
      "-----------------------------------\n",
      "Training loss: 0.5647366305433636....\n",
      "Validation loss: 0.48695207597906215....\n",
      "-----------------------------------\n",
      "Training loss: 0.5644850162346552....\n",
      "Validation loss: 0.48679512082004656....\n",
      "-----------------------------------\n",
      "Training loss: 0.5642337851212464....\n",
      "Validation loss: 0.4866383477464678....\n",
      "-----------------------------------\n",
      "Training loss: 0.5639829800791867....\n",
      "Validation loss: 0.4864817828412473....\n",
      "-----------------------------------\n",
      "Training loss: 0.563732552146832....\n",
      "Validation loss: 0.48632546036285906....\n",
      "-----------------------------------\n",
      "Training loss: 0.5634824989896422....\n",
      "Validation loss: 0.4861692699883607....\n",
      "-----------------------------------\n",
      "Training loss: 0.5632327752473373....\n",
      "Validation loss: 0.48601326442742704....\n",
      "-----------------------------------\n",
      "Training loss: 0.5629834363510074....\n",
      "Validation loss: 0.4858575488658957....\n",
      "-----------------------------------\n",
      "Training loss: 0.5627344450575186....\n",
      "Validation loss: 0.4857020461910643....\n",
      "-----------------------------------\n",
      "Training loss: 0.5624857986455108....\n",
      "Validation loss: 0.48554677773311555....\n",
      "-----------------------------------\n",
      "Training loss: 0.5622374606333899....\n",
      "Validation loss: 0.48539157438035585....\n",
      "-----------------------------------\n",
      "Training loss: 0.5619894218520051....\n",
      "Validation loss: 0.4852366260186467....\n",
      "-----------------------------------\n",
      "Training loss: 0.5617417636864103....\n",
      "Validation loss: 0.4850819334205289....\n",
      "-----------------------------------\n",
      "Training loss: 0.5614944823689484....\n",
      "Validation loss: 0.4849274999360296....\n",
      "-----------------------------------\n",
      "Training loss: 0.5612476280406689....\n",
      "Validation loss: 0.48477332837743803....\n",
      "-----------------------------------\n",
      "Training loss: 0.5610011749911672....\n",
      "Validation loss: 0.48461941504199885....\n",
      "-----------------------------------\n",
      "Training loss: 0.560755074437987....\n",
      "Validation loss: 0.4844657016739071....\n",
      "-----------------------------------\n",
      "Training loss: 0.5605092656983064....\n",
      "Validation loss: 0.4843120190253991....\n",
      "-----------------------------------\n",
      "Training loss: 0.5602637116868334....\n",
      "Validation loss: 0.4841586057048627....\n",
      "-----------------------------------\n",
      "Training loss: 0.5600184779170827....\n",
      "Validation loss: 0.4840054141897304....\n",
      "-----------------------------------\n",
      "Training loss: 0.5597736181864481....\n",
      "Validation loss: 0.4838523995443162....\n",
      "-----------------------------------\n",
      "Training loss: 0.5595290831343691....\n",
      "Validation loss: 0.48369961577284737....\n",
      "-----------------------------------\n",
      "Training loss: 0.5592847646237504....\n",
      "Validation loss: 0.483547050671073....\n",
      "-----------------------------------\n",
      "Training loss: 0.559040766067681....\n",
      "Validation loss: 0.4833947114171904....\n",
      "-----------------------------------\n",
      "Training loss: 0.5587970502013957....\n",
      "Validation loss: 0.48324261708794775....\n",
      "-----------------------------------\n",
      "Training loss: 0.5585536164142249....\n",
      "Validation loss: 0.48309075611158864....\n",
      "-----------------------------------\n",
      "Training loss: 0.5583105150039118....\n",
      "Validation loss: 0.48293909656390543....\n",
      "-----------------------------------\n",
      "Training loss: 0.5580677287766866....\n",
      "Validation loss: 0.48278770793741704....\n",
      "-----------------------------------\n",
      "Training loss: 0.55782526919575....\n",
      "Validation loss: 0.48263654306810927....\n",
      "-----------------------------------\n",
      "Training loss: 0.5575831573656456....\n",
      "Validation loss: 0.4824855840829924....\n",
      "-----------------------------------\n",
      "Training loss: 0.5573413841531228....\n",
      "Validation loss: 0.48233490405469903....\n",
      "-----------------------------------\n",
      "Training loss: 0.5570999362350724....\n",
      "Validation loss: 0.482184342692623....\n",
      "-----------------------------------\n",
      "Training loss: 0.5568588865190197....\n",
      "Validation loss: 0.4820339900753819....\n",
      "-----------------------------------\n",
      "Training loss: 0.5566181781805883....\n",
      "Validation loss: 0.4818838861320787....\n",
      "-----------------------------------\n",
      "Training loss: 0.5563778009682828....\n",
      "Validation loss: 0.4817340333248049....\n",
      "-----------------------------------\n",
      "Training loss: 0.5561377551487412....\n",
      "Validation loss: 0.48158435692638546....\n",
      "-----------------------------------\n",
      "Training loss: 0.5558980024789716....\n",
      "Validation loss: 0.48143488828821707....\n",
      "-----------------------------------\n",
      "Training loss: 0.5556585737888025....\n",
      "Validation loss: 0.4812857560326725....\n",
      "-----------------------------------\n",
      "Training loss: 0.5554195039417177....\n",
      "Validation loss: 0.4811369119092214....\n",
      "-----------------------------------\n",
      "Training loss: 0.5551807356318945....\n",
      "Validation loss: 0.48098823080318603....\n",
      "-----------------------------------\n",
      "Training loss: 0.5549422925932074....\n",
      "Validation loss: 0.48083980047881975....\n",
      "-----------------------------------\n",
      "Training loss: 0.5547041974918341....\n",
      "Validation loss: 0.4806916124463288....\n",
      "-----------------------------------\n",
      "Training loss: 0.5544664129546325....\n",
      "Validation loss: 0.48054364446705033....\n",
      "-----------------------------------\n",
      "Training loss: 0.5542289148184039....\n",
      "Validation loss: 0.4803959495189498....\n",
      "-----------------------------------\n",
      "Training loss: 0.5539917791033413....\n",
      "Validation loss: 0.4802484777989802....\n",
      "-----------------------------------\n",
      "Training loss: 0.5537549707143965....\n",
      "Validation loss: 0.48010121486456186....\n",
      "-----------------------------------\n",
      "Training loss: 0.5535184909720151....\n",
      "Validation loss: 0.4799541950256165....\n",
      "-----------------------------------\n",
      "Training loss: 0.5532823282770044....\n",
      "Validation loss: 0.4798074141010976....\n",
      "-----------------------------------\n",
      "Training loss: 0.5530464856175158....\n",
      "Validation loss: 0.4796608483274626....\n",
      "-----------------------------------\n",
      "Training loss: 0.5528109614989591....\n",
      "Validation loss: 0.47951452986851334....\n",
      "-----------------------------------\n",
      "Training loss: 0.5525757925989305....\n",
      "Validation loss: 0.4793684054446296....\n",
      "-----------------------------------\n",
      "Training loss: 0.5523409343876574....\n",
      "Validation loss: 0.4792224541381412....\n",
      "-----------------------------------\n",
      "Training loss: 0.5521064297289667....\n",
      "Validation loss: 0.4790768253513331....\n",
      "-----------------------------------\n",
      "Training loss: 0.5518722893844726....\n",
      "Validation loss: 0.47893131398396116....\n",
      "-----------------------------------\n",
      "Training loss: 0.551638452911652....\n",
      "Validation loss: 0.47878606747941804....\n",
      "-----------------------------------\n",
      "Training loss: 0.5514049415139605....\n",
      "Validation loss: 0.4786410425414287....\n",
      "-----------------------------------\n",
      "Training loss: 0.551171761411441....\n",
      "Validation loss: 0.4784962110269713....\n",
      "-----------------------------------\n",
      "Training loss: 0.550938879099357....\n",
      "Validation loss: 0.4783516672507928....\n",
      "-----------------------------------\n",
      "Training loss: 0.5507062957570341....\n",
      "Validation loss: 0.4782073029696182....\n",
      "-----------------------------------\n",
      "Training loss: 0.5504739714943223....\n",
      "Validation loss: 0.4780631959262778....\n",
      "-----------------------------------\n",
      "Training loss: 0.5502419308182848....\n",
      "Validation loss: 0.477919285227481....\n",
      "-----------------------------------\n",
      "Training loss: 0.5500102225014432....\n",
      "Validation loss: 0.4777757154353962....\n",
      "-----------------------------------\n",
      "Training loss: 0.5497788768331112....\n",
      "Validation loss: 0.47763237961800237....\n",
      "-----------------------------------\n",
      "Training loss: 0.5495478075876646....\n",
      "Validation loss: 0.4774892164033552....\n",
      "-----------------------------------\n",
      "Training loss: 0.5493170303843095....\n",
      "Validation loss: 0.47734621791087056....\n",
      "-----------------------------------\n",
      "Training loss: 0.5490865347459307....\n",
      "Validation loss: 0.47720345680399867....\n",
      "-----------------------------------\n",
      "Training loss: 0.5488563198339403....\n",
      "Validation loss: 0.47706091825237884....\n",
      "-----------------------------------\n",
      "Training loss: 0.5486263979034101....\n",
      "Validation loss: 0.47691857071877536....\n",
      "-----------------------------------\n",
      "Training loss: 0.5483967687520888....\n",
      "Validation loss: 0.47677640484342665....\n",
      "-----------------------------------\n",
      "Training loss: 0.5481674457345147....\n",
      "Validation loss: 0.47663439285654197....\n",
      "-----------------------------------\n",
      "Training loss: 0.5479383947139242....\n",
      "Validation loss: 0.4764925610260431....\n",
      "-----------------------------------\n",
      "Training loss: 0.5477096382308553....\n",
      "Validation loss: 0.47635087545820726....\n",
      "-----------------------------------\n",
      "Training loss: 0.5474811573592144....\n",
      "Validation loss: 0.4762093681970048....\n",
      "-----------------------------------\n",
      "Training loss: 0.5472530339733046....\n",
      "Validation loss: 0.47606810102795366....\n",
      "-----------------------------------\n",
      "Training loss: 0.5470252319201274....\n",
      "Validation loss: 0.4759270142654196....\n",
      "-----------------------------------\n",
      "Training loss: 0.5467976795532955....\n",
      "Validation loss: 0.47578611928620484....\n",
      "-----------------------------------\n",
      "Training loss: 0.5465703745931332....\n",
      "Validation loss: 0.47564534834908284....\n",
      "-----------------------------------\n",
      "Training loss: 0.5463433841131128....\n",
      "Validation loss: 0.47550474571918505....\n",
      "-----------------------------------\n",
      "Training loss: 0.5461166347907164....\n",
      "Validation loss: 0.47536437983119567....\n",
      "-----------------------------------\n",
      "Training loss: 0.5458901230383506....\n",
      "Validation loss: 0.47522415285448183....\n",
      "-----------------------------------\n",
      "Training loss: 0.545663885478609....\n",
      "Validation loss: 0.47508414140702027....\n",
      "-----------------------------------\n",
      "Training loss: 0.545437919922584....\n",
      "Validation loss: 0.47494426724065936....\n",
      "-----------------------------------\n",
      "Training loss: 0.5452122463995529....\n",
      "Validation loss: 0.47480463580789894....\n",
      "-----------------------------------\n",
      "Training loss: 0.5449868709514112....\n",
      "Validation loss: 0.4746652021163907....\n",
      "-----------------------------------\n",
      "Training loss: 0.5447618708360316....\n",
      "Validation loss: 0.4745259771441318....\n",
      "-----------------------------------\n",
      "Training loss: 0.5445371770595959....\n",
      "Validation loss: 0.4743869660423764....\n",
      "-----------------------------------\n",
      "Training loss: 0.5443127698094855....\n",
      "Validation loss: 0.4742481417320471....\n",
      "-----------------------------------\n",
      "Training loss: 0.5440886524373765....\n",
      "Validation loss: 0.47410950928063106....\n",
      "-----------------------------------\n",
      "Training loss: 0.5438647957938045....\n",
      "Validation loss: 0.4739710464212136....\n",
      "-----------------------------------\n",
      "Training loss: 0.5436411575283674....\n",
      "Validation loss: 0.47383282448529374....\n",
      "-----------------------------------\n",
      "Training loss: 0.5434177097855138....\n",
      "Validation loss: 0.4736948413962106....\n",
      "-----------------------------------\n",
      "Training loss: 0.5431945229159474....\n",
      "Validation loss: 0.47355707450500417....\n",
      "-----------------------------------\n",
      "Training loss: 0.5429715792315556....\n",
      "Validation loss: 0.4734194477807069....\n",
      "-----------------------------------\n",
      "Training loss: 0.5427488180133948....\n",
      "Validation loss: 0.4732820665407711....\n",
      "-----------------------------------\n",
      "Training loss: 0.5425263227274472....\n",
      "Validation loss: 0.4731448545804812....\n",
      "-----------------------------------\n",
      "Training loss: 0.5423041139784982....\n",
      "Validation loss: 0.4730078374438777....\n",
      "-----------------------------------\n",
      "Training loss: 0.5420821926086603....\n",
      "Validation loss: 0.47287102668319014....\n",
      "-----------------------------------\n",
      "Training loss: 0.5418605570601754....\n",
      "Validation loss: 0.4727344531261823....\n",
      "-----------------------------------\n",
      "Training loss: 0.541639210540796....\n",
      "Validation loss: 0.4725981322074688....\n",
      "-----------------------------------\n",
      "Training loss: 0.5414181550388079....\n",
      "Validation loss: 0.4724619567988389....\n",
      "-----------------------------------\n",
      "Training loss: 0.5411973818245065....\n",
      "Validation loss: 0.47232600178450057....\n",
      "-----------------------------------\n",
      "Training loss: 0.5409768988491673....\n",
      "Validation loss: 0.47219023093993023....\n",
      "-----------------------------------\n",
      "Training loss: 0.5407566694917164....\n",
      "Validation loss: 0.47205462000616494....\n",
      "-----------------------------------\n",
      "Training loss: 0.5405366926936037....\n",
      "Validation loss: 0.4719192320821085....\n",
      "-----------------------------------\n",
      "Training loss: 0.540316980528706....\n",
      "Validation loss: 0.47178391897036115....\n",
      "-----------------------------------\n",
      "Training loss: 0.5400975184579481....\n",
      "Validation loss: 0.47164880488141736....\n",
      "-----------------------------------\n",
      "Training loss: 0.539878316789057....\n",
      "Validation loss: 0.4715138480590663....\n",
      "-----------------------------------\n",
      "Training loss: 0.5396593575576208....\n",
      "Validation loss: 0.4713789516654881....\n",
      "-----------------------------------\n",
      "Training loss: 0.539440610746715....\n",
      "Validation loss: 0.4712442907057329....\n",
      "-----------------------------------\n",
      "Training loss: 0.5392221736731554....\n",
      "Validation loss: 0.4711097611340876....\n",
      "-----------------------------------\n",
      "Training loss: 0.5390040007038498....\n",
      "Validation loss: 0.47097546855287764....\n",
      "-----------------------------------\n",
      "Training loss: 0.5387861337664599....\n",
      "Validation loss: 0.4708413573870708....\n",
      "-----------------------------------\n",
      "Training loss: 0.5385685420137298....\n",
      "Validation loss: 0.4707074899502098....\n",
      "-----------------------------------\n",
      "Training loss: 0.5383512767169139....\n",
      "Validation loss: 0.47057376220151376....\n",
      "-----------------------------------\n",
      "Training loss: 0.5381343027939332....\n",
      "Validation loss: 0.47044030015955607....\n",
      "-----------------------------------\n",
      "Training loss: 0.5379175460177963....\n",
      "Validation loss: 0.470307003884827....\n",
      "-----------------------------------\n",
      "Training loss: 0.537701077373984....\n",
      "Validation loss: 0.47017387510131997....\n",
      "-----------------------------------\n",
      "Training loss: 0.5374848790719662....\n",
      "Validation loss: 0.47004095704768284....\n",
      "-----------------------------------\n",
      "Training loss: 0.5372689393937281....\n",
      "Validation loss: 0.46990821261087945....\n",
      "-----------------------------------\n",
      "Training loss: 0.5370533198653895....\n",
      "Validation loss: 0.469775700488173....\n",
      "-----------------------------------\n",
      "Training loss: 0.5368380424008063....\n",
      "Validation loss: 0.46964335714012523....\n",
      "-----------------------------------\n",
      "Training loss: 0.5366230218933054....\n",
      "Validation loss: 0.4695112101938815....\n",
      "-----------------------------------\n",
      "Training loss: 0.5364082518699572....\n",
      "Validation loss: 0.46937928873794793....\n",
      "-----------------------------------\n",
      "Training loss: 0.5361937484550948....\n",
      "Validation loss: 0.469247553384653....\n",
      "-----------------------------------\n",
      "Training loss: 0.5359795062189759....\n",
      "Validation loss: 0.4691160289094069....\n",
      "-----------------------------------\n",
      "Training loss: 0.535765522061561....\n",
      "Validation loss: 0.46898465534631134....\n",
      "-----------------------------------\n",
      "Training loss: 0.5355517775569942....\n",
      "Validation loss: 0.468853528438979....\n",
      "-----------------------------------\n",
      "Training loss: 0.535338282445063....\n",
      "Validation loss: 0.46872262754568145....\n",
      "-----------------------------------\n",
      "Training loss: 0.5351250459619694....\n",
      "Validation loss: 0.46859192511533293....\n",
      "-----------------------------------\n",
      "Training loss: 0.5349120762269036....\n",
      "Validation loss: 0.4684613261060986....\n",
      "-----------------------------------\n",
      "Training loss: 0.534699438374991....\n",
      "Validation loss: 0.46833092270133014....\n",
      "-----------------------------------\n",
      "Training loss: 0.5344870356757615....\n",
      "Validation loss: 0.4682006973729411....\n",
      "-----------------------------------\n",
      "Training loss: 0.5342748902310036....\n",
      "Validation loss: 0.4680706374946854....\n",
      "-----------------------------------\n",
      "Training loss: 0.5340629892371922....\n",
      "Validation loss: 0.4679407531213716....\n",
      "-----------------------------------\n",
      "Training loss: 0.533851301246229....\n",
      "Validation loss: 0.46781105126006345....\n",
      "-----------------------------------\n",
      "Training loss: 0.5336398556243144....\n",
      "Validation loss: 0.46768153803027196....\n",
      "-----------------------------------\n",
      "Training loss: 0.5334286938081848....\n",
      "Validation loss: 0.4675522085571309....\n",
      "-----------------------------------\n",
      "Training loss: 0.5332177864473282....\n",
      "Validation loss: 0.46742304679031205....\n",
      "-----------------------------------\n",
      "Training loss: 0.5330071268465042....\n",
      "Validation loss: 0.46729409803595434....\n",
      "-----------------------------------\n",
      "Training loss: 0.532796676295579....\n",
      "Validation loss: 0.4671653755423335....\n",
      "-----------------------------------\n",
      "Training loss: 0.5325864797887152....\n",
      "Validation loss: 0.46703685355933255....\n",
      "-----------------------------------\n",
      "Training loss: 0.5323765639728445....\n",
      "Validation loss: 0.46690855509348683....\n",
      "-----------------------------------\n",
      "Training loss: 0.5321669696596164....\n",
      "Validation loss: 0.46678046336650897....\n",
      "-----------------------------------\n",
      "Training loss: 0.5319576598534045....\n",
      "Validation loss: 0.46665243218401625....\n",
      "-----------------------------------\n",
      "Training loss: 0.531748586780677....\n",
      "Validation loss: 0.46652464018163864....\n",
      "-----------------------------------\n",
      "Training loss: 0.5315397686914932....\n",
      "Validation loss: 0.46639701865248134....\n",
      "-----------------------------------\n",
      "Training loss: 0.5313311910238663....\n",
      "Validation loss: 0.4662696136451117....\n",
      "-----------------------------------\n",
      "Training loss: 0.5311228701593307....\n",
      "Validation loss: 0.4661423548005226....\n",
      "-----------------------------------\n",
      "Training loss: 0.5309148091194813....\n",
      "Validation loss: 0.46601526084092604....\n",
      "-----------------------------------\n",
      "Training loss: 0.5307069979667014....\n",
      "Validation loss: 0.46588830226558536....\n",
      "-----------------------------------\n",
      "Training loss: 0.5304994136616685....\n",
      "Validation loss: 0.4657615266504367....\n",
      "-----------------------------------\n",
      "Training loss: 0.5302920497146404....\n",
      "Validation loss: 0.4656349802334496....\n",
      "-----------------------------------\n",
      "Training loss: 0.5300849330318133....\n",
      "Validation loss: 0.46550851378120184....\n",
      "-----------------------------------\n",
      "Training loss: 0.5298780398830581....\n",
      "Validation loss: 0.4653822306849107....\n",
      "-----------------------------------\n",
      "Training loss: 0.5296713619371873....\n",
      "Validation loss: 0.4652561434024773....\n",
      "-----------------------------------\n",
      "Training loss: 0.5294649248372061....\n",
      "Validation loss: 0.4651302324392655....\n",
      "-----------------------------------\n",
      "Training loss: 0.529258683081629....\n",
      "Validation loss: 0.4650044349552011....\n",
      "-----------------------------------\n",
      "Training loss: 0.5290526820440937....\n",
      "Validation loss: 0.4648788451683538....\n",
      "-----------------------------------\n",
      "Training loss: 0.5288469279257907....\n",
      "Validation loss: 0.46475338501932945....\n",
      "-----------------------------------\n",
      "Training loss: 0.5286414167267031....\n",
      "Validation loss: 0.4646280419383963....\n",
      "-----------------------------------\n",
      "Training loss: 0.5284361679256478....\n",
      "Validation loss: 0.46450282274343196....\n",
      "-----------------------------------\n",
      "Training loss: 0.5282312468093583....\n",
      "Validation loss: 0.4643777791082371....\n",
      "-----------------------------------\n",
      "Training loss: 0.5280265261430106....\n",
      "Validation loss: 0.4642529863408096....\n",
      "-----------------------------------\n",
      "Training loss: 0.5278220222070862....\n",
      "Validation loss: 0.4641283351310427....\n",
      "-----------------------------------\n",
      "Training loss: 0.5276177734500548....\n",
      "Validation loss: 0.46400386481360656....\n",
      "-----------------------------------\n",
      "Training loss: 0.5274137608406411....\n",
      "Validation loss: 0.46387956910146805....\n",
      "-----------------------------------\n",
      "Training loss: 0.5272100023725252....\n",
      "Validation loss: 0.4637554338541428....\n",
      "-----------------------------------\n",
      "Training loss: 0.5270064719284092....\n",
      "Validation loss: 0.4636313823344539....\n",
      "-----------------------------------\n",
      "Training loss: 0.5268031900237194....\n",
      "Validation loss: 0.46350747693254674....\n",
      "-----------------------------------\n",
      "Training loss: 0.5266001387719836....\n",
      "Validation loss: 0.4633836895099449....\n",
      "-----------------------------------\n",
      "Training loss: 0.5263972820050381....\n",
      "Validation loss: 0.4632600785752709....\n",
      "-----------------------------------\n",
      "Training loss: 0.5261946441482319....\n",
      "Validation loss: 0.4631365232715981....\n",
      "-----------------------------------\n",
      "Training loss: 0.5259922194318474....\n",
      "Validation loss: 0.4630131772623405....\n",
      "-----------------------------------\n",
      "Training loss: 0.5257899874995451....\n",
      "Validation loss: 0.4628899480302156....\n",
      "-----------------------------------\n",
      "Training loss: 0.5255879730057074....\n",
      "Validation loss: 0.46276689829359374....\n",
      "-----------------------------------\n",
      "Training loss: 0.5253862196827299....\n",
      "Validation loss: 0.4626439741928371....\n",
      "-----------------------------------\n",
      "Training loss: 0.5251847115404695....\n",
      "Validation loss: 0.4625212411921935....\n",
      "-----------------------------------\n",
      "Training loss: 0.5249834412162834....\n",
      "Validation loss: 0.46239867832176523....\n",
      "-----------------------------------\n",
      "Training loss: 0.5247824035203632....\n",
      "Validation loss: 0.4622762167426479....\n",
      "-----------------------------------\n",
      "Training loss: 0.52458158290404....\n",
      "Validation loss: 0.4621538433234327....\n",
      "-----------------------------------\n",
      "Training loss: 0.5243809602027059....\n",
      "Validation loss: 0.46203162664084185....\n",
      "-----------------------------------\n",
      "Training loss: 0.5241805723618065....\n",
      "Validation loss: 0.46190960462485425....\n",
      "-----------------------------------\n",
      "Training loss: 0.5239804274720318....\n",
      "Validation loss: 0.4617877640439059....\n",
      "-----------------------------------\n",
      "Training loss: 0.5237805030889037....\n",
      "Validation loss: 0.46166608132379805....\n",
      "-----------------------------------\n",
      "Training loss: 0.5235808127200914....\n",
      "Validation loss: 0.4615445739435661....\n",
      "-----------------------------------\n",
      "Training loss: 0.5233813478709896....\n",
      "Validation loss: 0.46142327665660793....\n",
      "-----------------------------------\n",
      "Training loss: 0.5231821173727282....\n",
      "Validation loss: 0.46130214793477053....\n",
      "-----------------------------------\n",
      "Training loss: 0.522983137069865....\n",
      "Validation loss: 0.4611812363786992....\n",
      "-----------------------------------\n",
      "Training loss: 0.522784371945108....\n",
      "Validation loss: 0.4610604896249014....\n",
      "-----------------------------------\n",
      "Training loss: 0.522585864672001....\n",
      "Validation loss: 0.4609398882635267....\n",
      "-----------------------------------\n",
      "Training loss: 0.5223875928835043....\n",
      "Validation loss: 0.4608194716275104....\n",
      "-----------------------------------\n",
      "Training loss: 0.5221895325797705....\n",
      "Validation loss: 0.4606991528453005....\n",
      "-----------------------------------\n",
      "Training loss: 0.5219916654445297....\n",
      "Validation loss: 0.4605789507867039....\n",
      "-----------------------------------\n",
      "Training loss: 0.5217940251154235....\n",
      "Validation loss: 0.46045894557959016....\n",
      "-----------------------------------\n",
      "Training loss: 0.5215965951136324....\n",
      "Validation loss: 0.46033911792432347....\n",
      "-----------------------------------\n",
      "Training loss: 0.5213993493185772....\n",
      "Validation loss: 0.460219440355499....\n",
      "-----------------------------------\n",
      "Training loss: 0.5212023870825244....\n",
      "Validation loss: 0.46009992160080376....\n",
      "-----------------------------------\n",
      "Training loss: 0.5210056762920562....\n",
      "Validation loss: 0.4599805121328025....\n",
      "-----------------------------------\n",
      "Training loss: 0.5208091972125423....\n",
      "Validation loss: 0.45986131415195414....\n",
      "-----------------------------------\n",
      "Training loss: 0.5206129375158971....\n",
      "Validation loss: 0.4597421898534469....\n",
      "-----------------------------------\n",
      "Training loss: 0.5204169353147732....\n",
      "Validation loss: 0.4596232939379332....\n",
      "-----------------------------------\n",
      "Training loss: 0.5202211768533918....\n",
      "Validation loss: 0.45950453838321226....\n",
      "-----------------------------------\n",
      "Training loss: 0.5200256163082818....\n",
      "Validation loss: 0.45938595699301327....\n",
      "-----------------------------------\n",
      "Training loss: 0.5198303004249709....\n",
      "Validation loss: 0.45926754891769345....\n",
      "-----------------------------------\n",
      "Training loss: 0.5196352019334702....\n",
      "Validation loss: 0.45914929920823516....\n",
      "-----------------------------------\n",
      "Training loss: 0.5194403189477405....\n",
      "Validation loss: 0.4590311880775625....\n",
      "-----------------------------------\n",
      "Training loss: 0.5192456299541072....\n",
      "Validation loss: 0.458913261067894....\n",
      "-----------------------------------\n",
      "Training loss: 0.519051144502001....\n",
      "Validation loss: 0.4587954688069756....\n",
      "-----------------------------------\n",
      "Training loss: 0.5188568806478142....\n",
      "Validation loss: 0.45867783537872614....\n",
      "-----------------------------------\n",
      "Training loss: 0.5186628431389554....\n",
      "Validation loss: 0.45856036794060706....\n",
      "-----------------------------------\n",
      "Training loss: 0.5184690114386828....\n",
      "Validation loss: 0.45844293205392056....\n",
      "-----------------------------------\n",
      "Training loss: 0.5182753956913762....\n",
      "Validation loss: 0.45832565635000133....\n",
      "-----------------------------------\n",
      "Training loss: 0.5180819696740908....\n",
      "Validation loss: 0.45820854597868915....\n",
      "-----------------------------------\n",
      "Training loss: 0.5178887907162476....\n",
      "Validation loss: 0.4580916429421125....\n",
      "-----------------------------------\n",
      "Training loss: 0.517695829192023....\n",
      "Validation loss: 0.4579749243456762....\n",
      "-----------------------------------\n",
      "Training loss: 0.5175030762395044....\n",
      "Validation loss: 0.4578583378005738....\n",
      "-----------------------------------\n",
      "Training loss: 0.5173105621294466....\n",
      "Validation loss: 0.4577419670546415....\n",
      "-----------------------------------\n",
      "Training loss: 0.5171182581283639....\n",
      "Validation loss: 0.45762569427062877....\n",
      "-----------------------------------\n",
      "Training loss: 0.5169261840555311....\n",
      "Validation loss: 0.45750964599500826....\n",
      "-----------------------------------\n",
      "Training loss: 0.516734335508409....\n",
      "Validation loss: 0.4573937356991015....\n",
      "-----------------------------------\n",
      "Training loss: 0.5165426951436796....\n",
      "Validation loss: 0.45727794307788566....\n",
      "-----------------------------------\n",
      "Training loss: 0.5163512480360659....\n",
      "Validation loss: 0.45716232023161296....\n",
      "-----------------------------------\n",
      "Training loss: 0.5161599923431928....\n",
      "Validation loss: 0.4570468161274445....\n",
      "-----------------------------------\n",
      "Training loss: 0.5159689467976247....\n",
      "Validation loss: 0.4569314722771113....\n",
      "-----------------------------------\n",
      "Training loss: 0.5157781251083491....\n",
      "Validation loss: 0.45681630095423353....\n",
      "-----------------------------------\n",
      "Training loss: 0.5155875089379447....\n",
      "Validation loss: 0.4567012814570794....\n",
      "-----------------------------------\n",
      "Training loss: 0.5153971005562145....\n",
      "Validation loss: 0.45658645569441025....\n",
      "-----------------------------------\n",
      "Training loss: 0.5152068956286855....\n",
      "Validation loss: 0.4564717942137015....\n",
      "-----------------------------------\n",
      "Training loss: 0.5150168882987611....\n",
      "Validation loss: 0.45635730876671604....\n",
      "-----------------------------------\n",
      "Training loss: 0.5148271296141794....\n",
      "Validation loss: 0.4562429243682883....\n",
      "-----------------------------------\n",
      "Training loss: 0.514637575322308....\n",
      "Validation loss: 0.45612870047916587....\n",
      "-----------------------------------\n",
      "Training loss: 0.5144482365937338....\n",
      "Validation loss: 0.4560145989661043....\n",
      "-----------------------------------\n",
      "Training loss: 0.5142591166582797....\n",
      "Validation loss: 0.4559006077603172....\n",
      "-----------------------------------\n",
      "Training loss: 0.5140701989444549....\n",
      "Validation loss: 0.45578680875230093....\n",
      "-----------------------------------\n",
      "Training loss: 0.5138814699654413....\n",
      "Validation loss: 0.4556731169648385....\n",
      "-----------------------------------\n",
      "Training loss: 0.5136929313834482....\n",
      "Validation loss: 0.45555964452080666....\n",
      "-----------------------------------\n",
      "Training loss: 0.5135045904171975....\n",
      "Validation loss: 0.45544628961972705....\n",
      "-----------------------------------\n",
      "Training loss: 0.5133163839384531....\n",
      "Validation loss: 0.45533301656423025....\n",
      "-----------------------------------\n",
      "Training loss: 0.5131283695235861....\n",
      "Validation loss: 0.45521991549093915....\n",
      "-----------------------------------\n",
      "Training loss: 0.5129405699266315....\n",
      "Validation loss: 0.45510691525771974....\n",
      "-----------------------------------\n",
      "Training loss: 0.5127529825841839....\n",
      "Validation loss: 0.45499408667556307....\n",
      "-----------------------------------\n",
      "Training loss: 0.5125655992226882....\n",
      "Validation loss: 0.45488136275928864....\n",
      "-----------------------------------\n",
      "Training loss: 0.5123784419933467....\n",
      "Validation loss: 0.4547688902221689....\n",
      "-----------------------------------\n",
      "Training loss: 0.5121915145027768....\n",
      "Validation loss: 0.45465654642802006....\n",
      "-----------------------------------\n",
      "Training loss: 0.5120048309629371....\n",
      "Validation loss: 0.454544232847102....\n",
      "-----------------------------------\n",
      "Training loss: 0.5118184048325882....\n",
      "Validation loss: 0.45443207251702017....\n",
      "-----------------------------------\n",
      "Training loss: 0.5116321820904169....\n",
      "Validation loss: 0.45432006393874336....\n",
      "-----------------------------------\n",
      "Training loss: 0.5114461367147467....\n",
      "Validation loss: 0.4542082141046605....\n",
      "-----------------------------------\n",
      "Training loss: 0.5112602835882493....\n",
      "Validation loss: 0.4540964841914408....\n",
      "-----------------------------------\n",
      "Training loss: 0.5110746367832437....\n",
      "Validation loss: 0.45398493422672476....\n",
      "-----------------------------------\n",
      "Training loss: 0.5108892036728147....\n",
      "Validation loss: 0.4538735153855922....\n",
      "-----------------------------------\n",
      "Training loss: 0.510703959117677....\n",
      "Validation loss: 0.45376226843293443....\n",
      "-----------------------------------\n",
      "Training loss: 0.5105188983913733....\n",
      "Validation loss: 0.4536511164826623....\n",
      "-----------------------------------\n",
      "Training loss: 0.5103340501959284....\n",
      "Validation loss: 0.4535403560642852....\n",
      "-----------------------------------\n",
      "Training loss: 0.5101494387511296....\n",
      "Validation loss: 0.4534297209562715....\n",
      "-----------------------------------\n",
      "Training loss: 0.509965014930631....\n",
      "Validation loss: 0.4533192236850392....\n",
      "-----------------------------------\n",
      "Training loss: 0.509780771871952....\n",
      "Validation loss: 0.4532088512996969....\n",
      "-----------------------------------\n",
      "Training loss: 0.5095967164386234....\n",
      "Validation loss: 0.4530986211786762....\n",
      "-----------------------------------\n",
      "Training loss: 0.509412864073805....\n",
      "Validation loss: 0.45298848606123754....\n",
      "-----------------------------------\n",
      "Training loss: 0.5092291821168136....\n",
      "Validation loss: 0.4528785350722976....\n",
      "-----------------------------------\n",
      "Training loss: 0.5090456769439454....\n",
      "Validation loss: 0.45276872671970686....\n",
      "-----------------------------------\n",
      "Training loss: 0.5088623708731701....\n",
      "Validation loss: 0.4526590860062456....\n",
      "-----------------------------------\n",
      "Training loss: 0.5086792696747402....\n",
      "Validation loss: 0.4525494930015422....\n",
      "-----------------------------------\n",
      "Training loss: 0.5084963837432511....\n",
      "Validation loss: 0.4524400729486164....\n",
      "-----------------------------------\n",
      "Training loss: 0.5083137108374178....\n",
      "Validation loss: 0.4523307430243893....\n",
      "-----------------------------------\n",
      "Training loss: 0.5081312308411258....\n",
      "Validation loss: 0.45222159981970406....\n",
      "-----------------------------------\n",
      "Training loss: 0.5079489031500788....\n",
      "Validation loss: 0.4521125170122853....\n",
      "-----------------------------------\n",
      "Training loss: 0.5077667373352984....\n",
      "Validation loss: 0.45200359596140527....\n",
      "-----------------------------------\n",
      "Training loss: 0.5075847505129787....\n",
      "Validation loss: 0.4518948101923152....\n",
      "-----------------------------------\n",
      "Training loss: 0.5074029277575381....\n",
      "Validation loss: 0.4517859804611573....\n",
      "-----------------------------------\n",
      "Training loss: 0.5072213118487786....\n",
      "Validation loss: 0.45167726963890276....\n",
      "-----------------------------------\n",
      "Training loss: 0.5070398622341704....\n",
      "Validation loss: 0.4515688121372538....\n",
      "-----------------------------------\n",
      "Training loss: 0.5068586094936638....\n",
      "Validation loss: 0.4514604213263388....\n",
      "-----------------------------------\n",
      "Training loss: 0.5066775510807288....\n",
      "Validation loss: 0.4513522782809101....\n",
      "-----------------------------------\n",
      "Training loss: 0.5064966685880851....\n",
      "Validation loss: 0.45124423518038365....\n",
      "-----------------------------------\n",
      "Training loss: 0.5063159896501763....\n",
      "Validation loss: 0.45113634425299765....\n",
      "-----------------------------------\n",
      "Training loss: 0.5061355041737715....\n",
      "Validation loss: 0.4510286725659749....\n",
      "-----------------------------------\n",
      "Training loss: 0.5059552102779028....\n",
      "Validation loss: 0.4509211186888482....\n",
      "-----------------------------------\n",
      "Training loss: 0.5057751231573685....\n",
      "Validation loss: 0.45081371811073073....\n",
      "-----------------------------------\n",
      "Training loss: 0.5055952311049696....\n",
      "Validation loss: 0.45070645584361974....\n",
      "-----------------------------------\n",
      "Training loss: 0.5054155323867786....\n",
      "Validation loss: 0.4505993413388206....\n",
      "-----------------------------------\n",
      "Training loss: 0.505236011192984....\n",
      "Validation loss: 0.45049236454963026....\n",
      "-----------------------------------\n",
      "Training loss: 0.5050566788712542....\n",
      "Validation loss: 0.45038553155409683....\n",
      "-----------------------------------\n",
      "Training loss: 0.5048775379189127....\n",
      "Validation loss: 0.4502787993595473....\n",
      "-----------------------------------\n",
      "Training loss: 0.5046985783719213....\n",
      "Validation loss: 0.4501722443131821....\n",
      "-----------------------------------\n",
      "Training loss: 0.5045197741940558....\n",
      "Validation loss: 0.45006577671474074....\n",
      "-----------------------------------\n",
      "Training loss: 0.5043411088775744....\n",
      "Validation loss: 0.44995943724347975....\n",
      "-----------------------------------\n",
      "Training loss: 0.5041626097998785....\n",
      "Validation loss: 0.4498532676840653....\n",
      "-----------------------------------\n",
      "Training loss: 0.5039842989817598....\n",
      "Validation loss: 0.44974723179507076....\n",
      "-----------------------------------\n",
      "Training loss: 0.5038061762315044....\n",
      "Validation loss: 0.44964134375235876....\n",
      "-----------------------------------\n",
      "Training loss: 0.5036282440313742....\n",
      "Validation loss: 0.4495355615592069....\n",
      "-----------------------------------\n",
      "Training loss: 0.5034505033550624....\n",
      "Validation loss: 0.44943002894716066....\n",
      "-----------------------------------\n",
      "Training loss: 0.5032729437908613....\n",
      "Validation loss: 0.449324573309793....\n",
      "-----------------------------------\n",
      "Training loss: 0.5030955719847854....\n",
      "Validation loss: 0.449219327659915....\n",
      "-----------------------------------\n",
      "Training loss: 0.5029183807816626....\n",
      "Validation loss: 0.4491142099633489....\n",
      "-----------------------------------\n",
      "Training loss: 0.5027413886996398....\n",
      "Validation loss: 0.4490091556770271....\n",
      "-----------------------------------\n",
      "Training loss: 0.5025645625694773....\n",
      "Validation loss: 0.448904234055308....\n",
      "-----------------------------------\n",
      "Training loss: 0.5023879198425798....\n",
      "Validation loss: 0.4487994707298233....\n",
      "-----------------------------------\n",
      "Training loss: 0.5022114619779008....\n",
      "Validation loss: 0.4486948007771914....\n",
      "-----------------------------------\n",
      "Training loss: 0.5020351638122956....\n",
      "Validation loss: 0.4485903273520501....\n",
      "-----------------------------------\n",
      "Training loss: 0.5018590515826056....\n",
      "Validation loss: 0.4484859326568026....\n",
      "-----------------------------------\n",
      "Training loss: 0.5016831217864404....\n",
      "Validation loss: 0.4483817426674946....\n",
      "-----------------------------------\n",
      "Training loss: 0.5015073907454738....\n",
      "Validation loss: 0.4482775839678577....\n",
      "-----------------------------------\n",
      "Training loss: 0.5013318646782122....\n",
      "Validation loss: 0.44817365584008584....\n",
      "-----------------------------------\n",
      "Training loss: 0.5011565134618339....\n",
      "Validation loss: 0.4480698195016625....\n",
      "-----------------------------------\n",
      "Training loss: 0.5009813465727265....\n",
      "Validation loss: 0.4479660694675969....\n",
      "-----------------------------------\n",
      "Training loss: 0.5008063642776764....\n",
      "Validation loss: 0.4478625313352426....\n",
      "-----------------------------------\n",
      "Training loss: 0.5006315601347316....\n",
      "Validation loss: 0.44775908487934263....\n",
      "-----------------------------------\n",
      "Training loss: 0.5004569177205557....\n",
      "Validation loss: 0.4476557704090205....\n",
      "-----------------------------------\n",
      "Training loss: 0.500282445786947....\n",
      "Validation loss: 0.4475525916115747....\n",
      "-----------------------------------\n",
      "Training loss: 0.5001081482934301....\n",
      "Validation loss: 0.44744955961194716....\n",
      "-----------------------------------\n",
      "Training loss: 0.4999340305199378....\n",
      "Validation loss: 0.44734658699419916....\n",
      "-----------------------------------\n",
      "Training loss: 0.49976007865779504....\n",
      "Validation loss: 0.4472439036756775....\n",
      "-----------------------------------\n",
      "Training loss: 0.4995862885725331....\n",
      "Validation loss: 0.4471413014109378....\n",
      "-----------------------------------\n",
      "Training loss: 0.49941266342485763....\n",
      "Validation loss: 0.4470388689759803....\n",
      "-----------------------------------\n",
      "Training loss: 0.49923916416484715....\n",
      "Validation loss: 0.44693654988727516....\n",
      "-----------------------------------\n",
      "Training loss: 0.49906575091249056....\n",
      "Validation loss: 0.4468343340029526....\n",
      "-----------------------------------\n",
      "Training loss: 0.49889248189231244....\n",
      "Validation loss: 0.4467322585495488....\n",
      "-----------------------------------\n",
      "Training loss: 0.4987193738000995....\n",
      "Validation loss: 0.44663026499044584....\n",
      "-----------------------------------\n",
      "Training loss: 0.49854643570134005....\n",
      "Validation loss: 0.4465283392040065....\n",
      "-----------------------------------\n",
      "Training loss: 0.49837366821075024....\n",
      "Validation loss: 0.4464266389502925....\n",
      "-----------------------------------\n",
      "Training loss: 0.49820106555655364....\n",
      "Validation loss: 0.4463249954014775....\n",
      "-----------------------------------\n",
      "Training loss: 0.4980286325521729....\n",
      "Validation loss: 0.4462235142887405....\n",
      "-----------------------------------\n",
      "Training loss: 0.49785637357520485....\n",
      "Validation loss: 0.44612218883621196....\n",
      "-----------------------------------\n",
      "Training loss: 0.4976843020939139....\n",
      "Validation loss: 0.44602105974364326....\n",
      "-----------------------------------\n",
      "Training loss: 0.4975124042461487....\n",
      "Validation loss: 0.4459200023604106....\n",
      "-----------------------------------\n",
      "Training loss: 0.49734066975099483....\n",
      "Validation loss: 0.44581908817518495....\n",
      "-----------------------------------\n",
      "Training loss: 0.4971691049509197....\n",
      "Validation loss: 0.44571826390352604....\n",
      "-----------------------------------\n",
      "Training loss: 0.4969977184448147....\n",
      "Validation loss: 0.4456175722262152....\n",
      "-----------------------------------\n",
      "Training loss: 0.49682651521763965....\n",
      "Validation loss: 0.4455169516884998....\n",
      "-----------------------------------\n",
      "Training loss: 0.49665547761063294....\n",
      "Validation loss: 0.4454164802491453....\n",
      "-----------------------------------\n",
      "Training loss: 0.49648460697970664....\n",
      "Validation loss: 0.44531613382693386....\n",
      "-----------------------------------\n",
      "Training loss: 0.4963139057264441....\n",
      "Validation loss: 0.44521596102272465....\n",
      "-----------------------------------\n",
      "Training loss: 0.4961433505667377....\n",
      "Validation loss: 0.4451158521838006....\n",
      "-----------------------------------\n",
      "Training loss: 0.4959729524414679....\n",
      "Validation loss: 0.4450158830936441....\n",
      "-----------------------------------\n",
      "Training loss: 0.49580269914889824....\n",
      "Validation loss: 0.44491601088933885....\n",
      "-----------------------------------\n",
      "Training loss: 0.4956326181738127....\n",
      "Validation loss: 0.44481621664094967....\n",
      "-----------------------------------\n",
      "Training loss: 0.4954626906959718....\n",
      "Validation loss: 0.44471653973433484....\n",
      "-----------------------------------\n",
      "Training loss: 0.4952929529182113....\n",
      "Validation loss: 0.4446170353531628....\n",
      "-----------------------------------\n",
      "Training loss: 0.4951233530155322....\n",
      "Validation loss: 0.44451768157896526....\n",
      "-----------------------------------\n",
      "Training loss: 0.4949539227438727....\n",
      "Validation loss: 0.44441837386980515....\n",
      "-----------------------------------\n",
      "Training loss: 0.4947846673897768....\n",
      "Validation loss: 0.44431920047927487....\n",
      "-----------------------------------\n",
      "Training loss: 0.49461558844266645....\n",
      "Validation loss: 0.4442200680678069....\n",
      "-----------------------------------\n",
      "Training loss: 0.49444668169428946....\n",
      "Validation loss: 0.44412113597525854....\n",
      "-----------------------------------\n",
      "Training loss: 0.49427792494745176....\n",
      "Validation loss: 0.44402234643177885....\n",
      "-----------------------------------\n",
      "Training loss: 0.4941093182002185....\n",
      "Validation loss: 0.4439236699036003....\n",
      "-----------------------------------\n",
      "Training loss: 0.49394087745976806....\n",
      "Validation loss: 0.44382509461755737....\n",
      "-----------------------------------\n",
      "Training loss: 0.49377260917185967....\n",
      "Validation loss: 0.4437266554338192....\n",
      "-----------------------------------\n",
      "Training loss: 0.4936044841058964....\n",
      "Validation loss: 0.4436283431237161....\n",
      "-----------------------------------\n",
      "Training loss: 0.49343651709446773....\n",
      "Validation loss: 0.4435301326026304....\n",
      "-----------------------------------\n",
      "Training loss: 0.4932687098783672....\n",
      "Validation loss: 0.44343203635877354....\n",
      "-----------------------------------\n",
      "Training loss: 0.4931010466950903....\n",
      "Validation loss: 0.4433340734862367....\n",
      "-----------------------------------\n",
      "Training loss: 0.49293351048755485....\n",
      "Validation loss: 0.4432363072718041....\n",
      "-----------------------------------\n",
      "Training loss: 0.4927661254827781....\n",
      "Validation loss: 0.4431385994824477....\n",
      "-----------------------------------\n",
      "Training loss: 0.4925989067593815....\n",
      "Validation loss: 0.4430410267288999....\n",
      "-----------------------------------\n",
      "Training loss: 0.4924318543072409....\n",
      "Validation loss: 0.44294359829881735....\n",
      "-----------------------------------\n",
      "Training loss: 0.49226495497322964....\n",
      "Validation loss: 0.44284614334923644....\n",
      "-----------------------------------\n",
      "Training loss: 0.4920981249751169....\n",
      "Validation loss: 0.4427487267967474....\n",
      "-----------------------------------\n",
      "Training loss: 0.4919314397878999....\n",
      "Validation loss: 0.4426514841125844....\n",
      "-----------------------------------\n",
      "Training loss: 0.49176490809018786....\n",
      "Validation loss: 0.44255438181534995....\n",
      "-----------------------------------\n",
      "Training loss: 0.4915985361383476....\n",
      "Validation loss: 0.4424573967347542....\n",
      "-----------------------------------\n",
      "Training loss: 0.49143233925397867....\n",
      "Validation loss: 0.44236053721375257....\n",
      "-----------------------------------\n",
      "Training loss: 0.49126630153821815....\n",
      "Validation loss: 0.44226381923412644....\n",
      "-----------------------------------\n",
      "Training loss: 0.4911003911496405....\n",
      "Validation loss: 0.4421671897448148....\n",
      "-----------------------------------\n",
      "Training loss: 0.4909346318222351....\n",
      "Validation loss: 0.4420706535305532....\n",
      "-----------------------------------\n",
      "Training loss: 0.4907690207531959....\n",
      "Validation loss: 0.44197430230359114....\n",
      "-----------------------------------\n",
      "Training loss: 0.49060353161192416....\n",
      "Validation loss: 0.44187808420946545....\n",
      "-----------------------------------\n",
      "Training loss: 0.49043820109536523....\n",
      "Validation loss: 0.4417820165504569....\n",
      "-----------------------------------\n",
      "Training loss: 0.49027302658511873....\n",
      "Validation loss: 0.4416860226178929....\n",
      "-----------------------------------\n",
      "Training loss: 0.49010797123355954....\n",
      "Validation loss: 0.4415901683192752....\n",
      "-----------------------------------\n",
      "Training loss: 0.489943083619182....\n",
      "Validation loss: 0.44149446949285265....\n",
      "-----------------------------------\n",
      "Training loss: 0.4897783524329135....\n",
      "Validation loss: 0.44139891465112724....\n",
      "-----------------------------------\n",
      "Training loss: 0.4896137479029192....\n",
      "Validation loss: 0.44130350554240305....\n",
      "-----------------------------------\n",
      "Training loss: 0.48944930724509017....\n",
      "Validation loss: 0.44120813430353234....\n",
      "-----------------------------------\n",
      "Training loss: 0.4892850403669949....\n",
      "Validation loss: 0.4411129039047403....\n",
      "-----------------------------------\n",
      "Training loss: 0.48912096055466486....\n",
      "Validation loss: 0.44101773646721315....\n",
      "-----------------------------------\n",
      "Training loss: 0.48895704847833144....\n",
      "Validation loss: 0.4409227330718962....\n",
      "-----------------------------------\n",
      "Training loss: 0.4887932900724937....\n",
      "Validation loss: 0.44082781366459195....\n",
      "-----------------------------------\n",
      "Training loss: 0.48862968980604937....\n",
      "Validation loss: 0.4407330525482573....\n",
      "-----------------------------------\n",
      "Training loss: 0.4884662276061877....\n",
      "Validation loss: 0.4406383651069021....\n",
      "-----------------------------------\n",
      "Training loss: 0.48830291150614324....\n",
      "Validation loss: 0.44054380736671445....\n",
      "-----------------------------------\n",
      "Training loss: 0.4881397517699694....\n",
      "Validation loss: 0.4404493715288328....\n",
      "-----------------------------------\n",
      "Training loss: 0.4879767483895563....\n",
      "Validation loss: 0.4403550051685864....\n",
      "-----------------------------------\n",
      "Training loss: 0.4878138753888265....\n",
      "Validation loss: 0.4402608275430362....\n",
      "-----------------------------------\n",
      "Training loss: 0.4876511229121515....\n",
      "Validation loss: 0.4401666820519998....\n",
      "-----------------------------------\n",
      "Training loss: 0.4874885236989587....\n",
      "Validation loss: 0.440072665611239....\n",
      "-----------------------------------\n",
      "Training loss: 0.4873260952565211....\n",
      "Validation loss: 0.43997875889938376....\n",
      "-----------------------------------\n",
      "Training loss: 0.4871638135433899....\n",
      "Validation loss: 0.4398849748697898....\n",
      "-----------------------------------\n",
      "Training loss: 0.48700164358981146....\n",
      "Validation loss: 0.43979130486045076....\n",
      "-----------------------------------\n",
      "Training loss: 0.4868396310326615....\n",
      "Validation loss: 0.439697772699491....\n",
      "-----------------------------------\n",
      "Training loss: 0.4866777866279273....\n",
      "Validation loss: 0.4396043774944718....\n",
      "-----------------------------------\n",
      "Training loss: 0.486516106335476....\n",
      "Validation loss: 0.4395111147575668....\n",
      "-----------------------------------\n",
      "Training loss: 0.4863545863054619....\n",
      "Validation loss: 0.43941799531646686....\n",
      "-----------------------------------\n",
      "Training loss: 0.4861931785405597....\n",
      "Validation loss: 0.43932488928019897....\n",
      "-----------------------------------\n",
      "Training loss: 0.4860318250245887....\n",
      "Validation loss: 0.4392319865319697....\n",
      "-----------------------------------\n",
      "Training loss: 0.48587059448032677....\n",
      "Validation loss: 0.4391391617220968....\n",
      "-----------------------------------\n",
      "Training loss: 0.48570952946460244....\n",
      "Validation loss: 0.43904632509577723....\n",
      "-----------------------------------\n",
      "Training loss: 0.48554862514487507....\n",
      "Validation loss: 0.4389536105107256....\n",
      "-----------------------------------\n",
      "Training loss: 0.4853878734038849....\n",
      "Validation loss: 0.4388610261774793....\n",
      "-----------------------------------\n",
      "Training loss: 0.4852272690708418....\n",
      "Validation loss: 0.4387685595390981....\n",
      "-----------------------------------\n",
      "Training loss: 0.48506680179186773....\n",
      "Validation loss: 0.43867627027976974....\n",
      "-----------------------------------\n",
      "Training loss: 0.484906461046911....\n",
      "Validation loss: 0.4385840167149928....\n",
      "-----------------------------------\n",
      "Training loss: 0.4847462512384202....\n",
      "Validation loss: 0.4384919043348683....\n",
      "-----------------------------------\n",
      "Training loss: 0.48458618991393326....\n",
      "Validation loss: 0.4383998588468592....\n",
      "-----------------------------------\n",
      "Training loss: 0.4844262862235432....\n",
      "Validation loss: 0.43830789111010005....\n",
      "-----------------------------------\n",
      "Training loss: 0.4842665336887347....\n",
      "Validation loss: 0.43821607267100454....\n",
      "-----------------------------------\n",
      "Training loss: 0.4841069402230823....\n",
      "Validation loss: 0.4381243162733644....\n",
      "-----------------------------------\n",
      "Training loss: 0.48394749937967585....\n",
      "Validation loss: 0.4380326654552116....\n",
      "-----------------------------------\n",
      "Training loss: 0.48378824293753064....\n",
      "Validation loss: 0.4379411496769035....\n",
      "-----------------------------------\n",
      "Training loss: 0.4836291259518389....\n",
      "Validation loss: 0.4378497616511558....\n",
      "-----------------------------------\n",
      "Training loss: 0.48347016058948744....\n",
      "Validation loss: 0.4377585284493476....\n",
      "-----------------------------------\n",
      "Training loss: 0.4833113560713374....\n",
      "Validation loss: 0.43766735851451843....\n",
      "-----------------------------------\n",
      "Training loss: 0.483152722620667....\n",
      "Validation loss: 0.43757632649748174....\n",
      "-----------------------------------\n",
      "Training loss: 0.4829942459039064....\n",
      "Validation loss: 0.4374854222624765....\n",
      "-----------------------------------\n",
      "Training loss: 0.48283591288989514....\n",
      "Validation loss: 0.4373945562188086....\n",
      "-----------------------------------\n",
      "Training loss: 0.482677711393442....\n",
      "Validation loss: 0.4373037741534582....\n",
      "-----------------------------------\n",
      "Training loss: 0.48251968188614525....\n",
      "Validation loss: 0.4372130782189314....\n",
      "-----------------------------------\n",
      "Training loss: 0.48236180767955544....\n",
      "Validation loss: 0.43712253935785844....\n",
      "-----------------------------------\n",
      "Training loss: 0.4822040782039635....\n",
      "Validation loss: 0.43703214194606355....\n",
      "-----------------------------------\n",
      "Training loss: 0.48204648955051965....\n",
      "Validation loss: 0.4369418193862499....\n",
      "-----------------------------------\n",
      "Training loss: 0.48188907486361304....\n",
      "Validation loss: 0.4368516859616741....\n",
      "-----------------------------------\n",
      "Training loss: 0.48173181240863633....\n",
      "Validation loss: 0.4367616189655264....\n",
      "-----------------------------------\n",
      "Training loss: 0.48157469085797644....\n",
      "Validation loss: 0.4366716394938878....\n",
      "-----------------------------------\n",
      "Training loss: 0.48141767972686683....\n",
      "Validation loss: 0.4365817130295625....\n",
      "-----------------------------------\n",
      "Training loss: 0.4812608086995217....\n",
      "Validation loss: 0.4364919481992598....\n",
      "-----------------------------------\n",
      "Training loss: 0.48110407957885026....\n",
      "Validation loss: 0.43640238399660675....\n",
      "-----------------------------------\n",
      "Training loss: 0.48094751114342865....\n",
      "Validation loss: 0.4363127995974888....\n",
      "-----------------------------------\n",
      "Training loss: 0.4807911072318132....\n",
      "Validation loss: 0.4362234150163468....\n",
      "-----------------------------------\n",
      "Training loss: 0.48063486033262964....\n",
      "Validation loss: 0.4361341296190261....\n",
      "-----------------------------------\n",
      "Training loss: 0.4804787541359516....\n",
      "Validation loss: 0.43604492829174485....\n",
      "-----------------------------------\n",
      "Training loss: 0.4803227772473365....\n",
      "Validation loss: 0.43595588292318815....\n",
      "-----------------------------------\n",
      "Training loss: 0.48016695137065496....\n",
      "Validation loss: 0.4358668993461355....\n",
      "-----------------------------------\n",
      "Training loss: 0.4800112693005785....\n",
      "Validation loss: 0.43577806891599297....\n",
      "-----------------------------------\n",
      "Training loss: 0.47985573053216607....\n",
      "Validation loss: 0.43568925282480686....\n",
      "-----------------------------------\n",
      "Training loss: 0.4797003275200008....\n",
      "Validation loss: 0.4356006122986443....\n",
      "-----------------------------------\n",
      "Training loss: 0.47954506902452687....\n",
      "Validation loss: 0.43551211560116837....\n",
      "-----------------------------------\n",
      "Training loss: 0.47938996038013215....\n",
      "Validation loss: 0.43542368101886636....\n",
      "-----------------------------------\n",
      "Training loss: 0.4792350029620902....\n",
      "Validation loss: 0.43533535382548166....\n",
      "-----------------------------------\n",
      "Training loss: 0.4790801909455114....\n",
      "Validation loss: 0.43524713955732675....\n",
      "-----------------------------------\n",
      "Training loss: 0.47892550791889893....\n",
      "Validation loss: 0.4351590914867684....\n",
      "-----------------------------------\n",
      "Training loss: 0.47877097925918566....\n",
      "Validation loss: 0.43507108826379165....\n",
      "-----------------------------------\n",
      "Training loss: 0.47861660716633764....\n",
      "Validation loss: 0.4349832452363427....\n",
      "-----------------------------------\n",
      "Training loss: 0.47846240598806344....\n",
      "Validation loss: 0.4348954454347436....\n",
      "-----------------------------------\n",
      "Training loss: 0.4783083670948998....\n",
      "Validation loss: 0.43480770590040235....\n",
      "-----------------------------------\n",
      "Training loss: 0.4781544745015751....\n",
      "Validation loss: 0.43472012042296293....\n",
      "-----------------------------------\n",
      "Training loss: 0.47800074906089957....\n",
      "Validation loss: 0.4346326019037681....\n",
      "-----------------------------------\n",
      "Training loss: 0.47784720190775093....\n",
      "Validation loss: 0.43454520345579856....\n",
      "-----------------------------------\n",
      "Training loss: 0.4776938077390112....\n",
      "Validation loss: 0.4344579461724377....\n",
      "-----------------------------------\n",
      "Training loss: 0.4775405077732513....\n",
      "Validation loss: 0.43437082150334155....\n",
      "-----------------------------------\n",
      "Training loss: 0.47738734615681266....\n",
      "Validation loss: 0.4342836615039311....\n",
      "-----------------------------------\n",
      "Training loss: 0.4772343220799542....\n",
      "Validation loss: 0.4341967116771897....\n",
      "-----------------------------------\n",
      "Training loss: 0.4770814357982934....\n",
      "Validation loss: 0.4341098512518844....\n",
      "-----------------------------------\n",
      "Training loss: 0.47692868929977367....\n",
      "Validation loss: 0.43402305463803886....\n",
      "-----------------------------------\n",
      "Training loss: 0.4767760941295857....\n",
      "Validation loss: 0.43393640115226395....\n",
      "-----------------------------------\n",
      "Training loss: 0.47662365412885976....\n",
      "Validation loss: 0.4338498331649011....\n",
      "-----------------------------------\n",
      "Training loss: 0.476471376450844....\n",
      "Validation loss: 0.4337633619453654....\n",
      "-----------------------------------\n",
      "Training loss: 0.47631925826393856....\n",
      "Validation loss: 0.4336770526008696....\n",
      "-----------------------------------\n",
      "Training loss: 0.4761673143739237....\n",
      "Validation loss: 0.4335908575826101....\n",
      "-----------------------------------\n",
      "Training loss: 0.47601551142794923....\n",
      "Validation loss: 0.43350474796937516....\n",
      "-----------------------------------\n",
      "Training loss: 0.4758638375005593....\n",
      "Validation loss: 0.43341879325936294....\n",
      "-----------------------------------\n",
      "Training loss: 0.47571229786150915....\n",
      "Validation loss: 0.4333329328209945....\n",
      "-----------------------------------\n",
      "Training loss: 0.47556088534179014....\n",
      "Validation loss: 0.4332471699783252....\n",
      "-----------------------------------\n",
      "Training loss: 0.47540957051939553....\n",
      "Validation loss: 0.4331615006050089....\n",
      "-----------------------------------\n",
      "Training loss: 0.47525838974460444....\n",
      "Validation loss: 0.43307597889520577....\n",
      "-----------------------------------\n",
      "Training loss: 0.47510734072282557....\n",
      "Validation loss: 0.43299057866982404....\n",
      "-----------------------------------\n",
      "Training loss: 0.4749564221556834....\n",
      "Validation loss: 0.4329052683479218....\n",
      "-----------------------------------\n",
      "Training loss: 0.4748056563900944....\n",
      "Validation loss: 0.4328201015393542....\n",
      "-----------------------------------\n",
      "Training loss: 0.47465504715985496....\n",
      "Validation loss: 0.43273502522006035....\n",
      "-----------------------------------\n",
      "Training loss: 0.47450456855736123....\n",
      "Validation loss: 0.43265001593630437....\n",
      "-----------------------------------\n",
      "Training loss: 0.47435423180615305....\n",
      "Validation loss: 0.43256518954512885....\n",
      "-----------------------------------\n",
      "Training loss: 0.47420403913383097....\n",
      "Validation loss: 0.43248038491691043....\n",
      "-----------------------------------\n",
      "Training loss: 0.47405398608589233....\n",
      "Validation loss: 0.43239574994068686....\n",
      "-----------------------------------\n",
      "Training loss: 0.4739040704940258....\n",
      "Validation loss: 0.43231116861403185....\n",
      "-----------------------------------\n",
      "Training loss: 0.47375429573947414....\n",
      "Validation loss: 0.4322266282696219....\n",
      "-----------------------------------\n",
      "Training loss: 0.47360465584730266....\n",
      "Validation loss: 0.4321422518898401....\n",
      "-----------------------------------\n",
      "Training loss: 0.4734551401453782....\n",
      "Validation loss: 0.43205792107162627....\n",
      "-----------------------------------\n",
      "Training loss: 0.47330577651757766....\n",
      "Validation loss: 0.43197370684067415....\n",
      "-----------------------------------\n",
      "Training loss: 0.4731565385482346....\n",
      "Validation loss: 0.4318896139146228....\n",
      "-----------------------------------\n",
      "Training loss: 0.4730074267883088....\n",
      "Validation loss: 0.4318055523637839....\n",
      "-----------------------------------\n",
      "Training loss: 0.4728584531222725....\n",
      "Validation loss: 0.4317216857426927....\n",
      "-----------------------------------\n",
      "Training loss: 0.47270959445464267....\n",
      "Validation loss: 0.4316378515462366....\n",
      "-----------------------------------\n",
      "Training loss: 0.47256084054055125....\n",
      "Validation loss: 0.4315541494658724....\n",
      "-----------------------------------\n",
      "Training loss: 0.47241222567127233....\n",
      "Validation loss: 0.43147059180243413....\n",
      "-----------------------------------\n",
      "Training loss: 0.47226375003874477....\n",
      "Validation loss: 0.4313869938479165....\n",
      "-----------------------------------\n",
      "Training loss: 0.4721154043802158....\n",
      "Validation loss: 0.43130359635585835....\n",
      "-----------------------------------\n",
      "Training loss: 0.4719671809301092....\n",
      "Validation loss: 0.4312201930990236....\n",
      "-----------------------------------\n",
      "Training loss: 0.47181900162637935....\n",
      "Validation loss: 0.43113689733761607....\n",
      "-----------------------------------\n",
      "Training loss: 0.471670908458356....\n",
      "Validation loss: 0.4310536365407622....\n",
      "-----------------------------------\n",
      "Training loss: 0.4715229603965085....\n",
      "Validation loss: 0.4309705319311264....\n",
      "-----------------------------------\n",
      "Training loss: 0.471375127880547....\n",
      "Validation loss: 0.4308875441680563....\n",
      "-----------------------------------\n",
      "Training loss: 0.47122743184385774....\n",
      "Validation loss: 0.4308046555052209....\n",
      "-----------------------------------\n",
      "Training loss: 0.47107991504614394....\n",
      "Validation loss: 0.43072190935065024....\n",
      "-----------------------------------\n",
      "Training loss: 0.47093250047280616....\n",
      "Validation loss: 0.4306392785349535....\n",
      "-----------------------------------\n",
      "Training loss: 0.4707851867160848....\n",
      "Validation loss: 0.4305567785718724....\n",
      "-----------------------------------\n",
      "Training loss: 0.47063800413385276....\n",
      "Validation loss: 0.43047438684219946....\n",
      "-----------------------------------\n",
      "Training loss: 0.47049092784631497....\n",
      "Validation loss: 0.43039204453779445....\n",
      "-----------------------------------\n",
      "Training loss: 0.4703439846100597....\n",
      "Validation loss: 0.43030984747263884....\n",
      "-----------------------------------\n",
      "Training loss: 0.47019717838390446....\n",
      "Validation loss: 0.43022770504253005....\n",
      "-----------------------------------\n",
      "Training loss: 0.4700505267143142....\n",
      "Validation loss: 0.430145678377491....\n",
      "-----------------------------------\n",
      "Training loss: 0.46990397951966284....\n",
      "Validation loss: 0.4300637197508214....\n",
      "-----------------------------------\n",
      "Training loss: 0.46975756940168933....\n",
      "Validation loss: 0.4299819265074469....\n",
      "-----------------------------------\n",
      "Training loss: 0.46961127297903327....\n",
      "Validation loss: 0.42990021025236297....\n",
      "-----------------------------------\n",
      "Training loss: 0.46946508899852....\n",
      "Validation loss: 0.4298185986577754....\n",
      "-----------------------------------\n",
      "Training loss: 0.4693190083793638....\n",
      "Validation loss: 0.42973703024711074....\n",
      "-----------------------------------\n",
      "Training loss: 0.46917305715196395....\n",
      "Validation loss: 0.4296556326819062....\n",
      "-----------------------------------\n",
      "Training loss: 0.4690272356699085....\n",
      "Validation loss: 0.4295742871019559....\n",
      "-----------------------------------\n",
      "Training loss: 0.4688815460273216....\n",
      "Validation loss: 0.4294931137625916....\n",
      "-----------------------------------\n",
      "Training loss: 0.46873597506747106....\n",
      "Validation loss: 0.42941200179546196....\n",
      "-----------------------------------\n",
      "Training loss: 0.46859055345736783....\n",
      "Validation loss: 0.42933109244296325....\n",
      "-----------------------------------\n",
      "Training loss: 0.4684452656424476....\n",
      "Validation loss: 0.42925022284539166....\n",
      "-----------------------------------\n",
      "Training loss: 0.4683000926165677....\n",
      "Validation loss: 0.42916948814346817....\n",
      "-----------------------------------\n",
      "Training loss: 0.46815505592130185....\n",
      "Validation loss: 0.42908882606504933....\n",
      "-----------------------------------\n",
      "Training loss: 0.4680101556217361....\n",
      "Validation loss: 0.429008275740927....\n",
      "-----------------------------------\n",
      "Training loss: 0.46786538060152955....\n",
      "Validation loss: 0.42892775979292114....\n",
      "-----------------------------------\n",
      "Training loss: 0.46772075898172977....\n",
      "Validation loss: 0.42884742854268637....\n",
      "-----------------------------------\n",
      "Training loss: 0.46757627559618586....\n",
      "Validation loss: 0.42876712417015705....\n",
      "-----------------------------------\n",
      "Training loss: 0.4674319207280331....\n",
      "Validation loss: 0.428686912522526....\n",
      "-----------------------------------\n",
      "Training loss: 0.4672876553127372....\n",
      "Validation loss: 0.4286067997470974....\n",
      "-----------------------------------\n",
      "Training loss: 0.467143519761278....\n",
      "Validation loss: 0.42852673921596945....\n",
      "-----------------------------------\n",
      "Training loss: 0.46699949160164156....\n",
      "Validation loss: 0.4284467871995641....\n",
      "-----------------------------------\n",
      "Training loss: 0.46685558682541944....\n",
      "Validation loss: 0.4283669023630539....\n",
      "-----------------------------------\n",
      "Training loss: 0.46671178490011594....\n",
      "Validation loss: 0.4282871872647944....\n",
      "-----------------------------------\n",
      "Training loss: 0.46656811363602013....\n",
      "Validation loss: 0.4282074831069947....\n",
      "-----------------------------------\n",
      "Training loss: 0.46642457374811835....\n",
      "Validation loss: 0.42812795004009574....\n",
      "-----------------------------------\n",
      "Training loss: 0.46628117140364905....\n",
      "Validation loss: 0.42804849376168874....\n",
      "-----------------------------------\n",
      "Training loss: 0.46613789594486904....\n",
      "Validation loss: 0.4279691795563018....\n",
      "-----------------------------------\n",
      "Training loss: 0.4659947381790464....\n",
      "Validation loss: 0.4278899228005076....\n",
      "-----------------------------------\n",
      "Training loss: 0.4658517180534246....\n",
      "Validation loss: 0.42781081552109856....\n",
      "-----------------------------------\n",
      "Training loss: 0.46570881818785315....\n",
      "Validation loss: 0.4277317605499453....\n",
      "-----------------------------------\n",
      "Training loss: 0.4655660611185134....\n",
      "Validation loss: 0.427652806758716....\n",
      "-----------------------------------\n",
      "Training loss: 0.4654234315257888....\n",
      "Validation loss: 0.4275739995859728....\n",
      "-----------------------------------\n",
      "Training loss: 0.465280894389142....\n",
      "Validation loss: 0.4274951900629544....\n",
      "-----------------------------------\n",
      "Training loss: 0.46513850613725316....\n",
      "Validation loss: 0.4274164743064147....\n",
      "-----------------------------------\n",
      "Training loss: 0.46499623293293985....\n",
      "Validation loss: 0.42733780952240435....\n",
      "-----------------------------------\n",
      "Training loss: 0.4648540754965651....\n",
      "Validation loss: 0.42725926251104296....\n",
      "-----------------------------------\n",
      "Training loss: 0.46471202453519694....\n",
      "Validation loss: 0.42718083470426066....\n",
      "-----------------------------------\n",
      "Training loss: 0.46457006231523945....\n",
      "Validation loss: 0.4271023960093266....\n",
      "-----------------------------------\n",
      "Training loss: 0.4644282212871011....\n",
      "Validation loss: 0.42702409878577224....\n",
      "-----------------------------------\n",
      "Training loss: 0.46428655797277535....\n",
      "Validation loss: 0.42694577890223506....\n",
      "-----------------------------------\n",
      "Training loss: 0.4641450375130449....\n",
      "Validation loss: 0.4268675084831361....\n",
      "-----------------------------------\n",
      "Training loss: 0.46400366186541175....\n",
      "Validation loss: 0.42678938116873655....\n",
      "-----------------------------------\n",
      "Training loss: 0.4638624057312575....\n",
      "Validation loss: 0.42671131877343904....\n",
      "-----------------------------------\n",
      "Training loss: 0.4637212753105637....\n",
      "Validation loss: 0.4266333779194741....\n",
      "-----------------------------------\n",
      "Training loss: 0.46358027425707143....\n",
      "Validation loss: 0.42655547705977753....\n",
      "-----------------------------------\n",
      "Training loss: 0.4634393865592857....\n",
      "Validation loss: 0.4264777383420058....\n",
      "-----------------------------------\n",
      "Training loss: 0.4632986058293316....\n",
      "Validation loss: 0.4264000517188122....\n",
      "-----------------------------------\n",
      "Training loss: 0.463157952025069....\n",
      "Validation loss: 0.42632250345753636....\n",
      "-----------------------------------\n",
      "Training loss: 0.4630174183982145....\n",
      "Validation loss: 0.4262449630342743....\n",
      "-----------------------------------\n",
      "Training loss: 0.4628770085786283....\n",
      "Validation loss: 0.4261675715230679....\n",
      "-----------------------------------\n",
      "Training loss: 0.4627367192591009....\n",
      "Validation loss: 0.4260902676549374....\n",
      "-----------------------------------\n",
      "Training loss: 0.4625965481899458....\n",
      "Validation loss: 0.4260130383736623....\n",
      "-----------------------------------\n",
      "Training loss: 0.46245650505281855....\n",
      "Validation loss: 0.42593580232538436....\n",
      "-----------------------------------\n",
      "Training loss: 0.46231662752249625....\n",
      "Validation loss: 0.42585872573268313....\n",
      "-----------------------------------\n",
      "Training loss: 0.46217687428546006....\n",
      "Validation loss: 0.4257817430615816....\n",
      "-----------------------------------\n",
      "Training loss: 0.46203722793663665....\n",
      "Validation loss: 0.42570488786435934....\n",
      "-----------------------------------\n",
      "Training loss: 0.461897698620452....\n",
      "Validation loss: 0.42562805797204106....\n",
      "-----------------------------------\n",
      "Training loss: 0.46175825747186877....\n",
      "Validation loss: 0.42555140708900324....\n",
      "-----------------------------------\n",
      "Training loss: 0.4616189358836617....\n",
      "Validation loss: 0.4254748771367731....\n",
      "-----------------------------------\n",
      "Training loss: 0.46147972367006784....\n",
      "Validation loss: 0.42539839074258895....\n",
      "-----------------------------------\n",
      "Training loss: 0.46134061895108947....\n",
      "Validation loss: 0.4253220489025373....\n",
      "-----------------------------------\n",
      "Training loss: 0.4612016291961193....\n",
      "Validation loss: 0.4252457762357808....\n",
      "-----------------------------------\n",
      "Training loss: 0.4610627461944702....\n",
      "Validation loss: 0.42516960389534986....\n",
      "-----------------------------------\n",
      "Training loss: 0.46092400084288....\n",
      "Validation loss: 0.42509343911975017....\n",
      "-----------------------------------\n",
      "Training loss: 0.46078534767737134....\n",
      "Validation loss: 0.42501743742279363....\n",
      "-----------------------------------\n",
      "Training loss: 0.4606468347353822....\n",
      "Validation loss: 0.42494141664174784....\n",
      "-----------------------------------\n",
      "Training loss: 0.4605084317638198....\n",
      "Validation loss: 0.4248655108678186....\n",
      "-----------------------------------\n",
      "Training loss: 0.4603701458570222....\n",
      "Validation loss: 0.4247896456815401....\n",
      "-----------------------------------\n",
      "Training loss: 0.46023198538100785....\n",
      "Validation loss: 0.42471397295672925....\n",
      "-----------------------------------\n",
      "Training loss: 0.46009395936817055....\n",
      "Validation loss: 0.42463829637417566....\n",
      "-----------------------------------\n",
      "Training loss: 0.45995605023238695....\n",
      "Validation loss: 0.42456276396290715....\n",
      "-----------------------------------\n",
      "Training loss: 0.459818251656493....\n",
      "Validation loss: 0.4244872826869923....\n",
      "-----------------------------------\n",
      "Training loss: 0.45968057704580806....\n",
      "Validation loss: 0.4244119042916091....\n",
      "-----------------------------------\n",
      "Training loss: 0.45954301963985383....\n",
      "Validation loss: 0.4243366101445544....\n",
      "-----------------------------------\n",
      "Training loss: 0.4594055727184623....\n",
      "Validation loss: 0.4242614052818562....\n",
      "-----------------------------------\n",
      "Training loss: 0.4592682494801496....\n",
      "Validation loss: 0.42418628554567567....\n",
      "-----------------------------------\n",
      "Training loss: 0.4591310554537219....\n",
      "Validation loss: 0.4241112397739645....\n",
      "-----------------------------------\n",
      "Training loss: 0.4589939836895142....\n",
      "Validation loss: 0.42403629328410924....\n",
      "-----------------------------------\n",
      "Training loss: 0.45885704647650166....\n",
      "Validation loss: 0.42396148334549194....\n",
      "-----------------------------------\n",
      "Training loss: 0.45872022404141866....\n",
      "Validation loss: 0.42388669477913976....\n",
      "-----------------------------------\n",
      "Training loss: 0.45858351512437096....\n",
      "Validation loss: 0.4238119898724515....\n",
      "-----------------------------------\n",
      "Training loss: 0.4584469327334856....\n",
      "Validation loss: 0.4237373194909324....\n",
      "-----------------------------------\n",
      "Training loss: 0.45831044168616064....\n",
      "Validation loss: 0.42366285891790156....\n",
      "-----------------------------------\n",
      "Training loss: 0.45817403138092644....\n",
      "Validation loss: 0.4235883858891147....\n",
      "-----------------------------------\n",
      "Training loss: 0.45803775378942657....\n",
      "Validation loss: 0.4235141029892844....\n",
      "-----------------------------------\n",
      "Training loss: 0.45790156334367593....\n",
      "Validation loss: 0.42343985386099703....\n",
      "-----------------------------------\n",
      "Training loss: 0.45776549451036064....\n",
      "Validation loss: 0.42336567295841526....\n",
      "-----------------------------------\n",
      "Training loss: 0.45762952214570624....\n",
      "Validation loss: 0.4232916183877799....\n",
      "-----------------------------------\n",
      "Training loss: 0.45749363801843446....\n",
      "Validation loss: 0.42321764596676004....\n",
      "-----------------------------------\n",
      "Training loss: 0.4573578750189939....\n",
      "Validation loss: 0.4231437230441073....\n",
      "-----------------------------------\n",
      "Training loss: 0.457222205878323....\n",
      "Validation loss: 0.42306990374666453....\n",
      "-----------------------------------\n",
      "Training loss: 0.457086653786463....\n",
      "Validation loss: 0.42299613886978604....\n",
      "-----------------------------------\n",
      "Training loss: 0.45695120503714853....\n",
      "Validation loss: 0.4229224779434123....\n",
      "-----------------------------------\n",
      "Training loss: 0.4568158828823754....\n",
      "Validation loss: 0.42284887064669474....\n",
      "-----------------------------------\n",
      "Training loss: 0.4566806926934053....\n",
      "Validation loss: 0.4227753613075679....\n",
      "-----------------------------------\n",
      "Training loss: 0.45654563670258697....\n",
      "Validation loss: 0.4227018997190284....\n",
      "-----------------------------------\n",
      "Training loss: 0.4564106832131304....\n",
      "Validation loss: 0.4226285696444062....\n",
      "-----------------------------------\n",
      "Training loss: 0.45627584149414135....\n",
      "Validation loss: 0.42255525223392165....\n",
      "-----------------------------------\n",
      "Training loss: 0.4561411102965799....\n",
      "Validation loss: 0.422482102038546....\n",
      "-----------------------------------\n",
      "Training loss: 0.456006470955516....\n",
      "Validation loss: 0.4224089722399958....\n",
      "-----------------------------------\n",
      "Training loss: 0.4558719391713098....\n",
      "Validation loss: 0.42233593712049144....\n",
      "-----------------------------------\n",
      "Training loss: 0.45573749471353536....\n",
      "Validation loss: 0.42226290766428404....\n",
      "-----------------------------------\n",
      "Training loss: 0.45560311738085235....\n",
      "Validation loss: 0.4221898482791792....\n",
      "-----------------------------------\n",
      "Training loss: 0.45546879364053966....\n",
      "Validation loss: 0.4221169181848952....\n",
      "-----------------------------------\n",
      "Training loss: 0.45533455092765507....\n",
      "Validation loss: 0.4220440669905265....\n",
      "-----------------------------------\n",
      "Training loss: 0.4552004160485268....\n",
      "Validation loss: 0.42197131699593865....\n",
      "-----------------------------------\n",
      "Training loss: 0.45506637661168314....\n",
      "Validation loss: 0.4218986426193162....\n",
      "-----------------------------------\n",
      "Training loss: 0.45493245982199143....\n",
      "Validation loss: 0.4218260263470384....\n",
      "-----------------------------------\n",
      "Training loss: 0.4547986438137951....\n",
      "Validation loss: 0.42175355473753917....\n",
      "-----------------------------------\n",
      "Training loss: 0.45466492385548807....\n",
      "Validation loss: 0.42168111688295923....\n",
      "-----------------------------------\n",
      "Training loss: 0.4545313234898494....\n",
      "Validation loss: 0.4216088124353377....\n",
      "-----------------------------------\n",
      "Training loss: 0.4543978430864633....\n",
      "Validation loss: 0.4215365408316605....\n",
      "-----------------------------------\n",
      "Training loss: 0.45426449735512237....\n",
      "Validation loss: 0.4214643543924838....\n",
      "-----------------------------------\n",
      "Training loss: 0.45413124061724935....\n",
      "Validation loss: 0.4213922649136406....\n",
      "-----------------------------------\n",
      "Training loss: 0.45399809187133705....\n",
      "Validation loss: 0.4213202218501991....\n",
      "-----------------------------------\n",
      "Training loss: 0.4538650581898826....\n",
      "Validation loss: 0.42124827284834526....\n",
      "-----------------------------------\n",
      "Training loss: 0.45373212044935746....\n",
      "Validation loss: 0.42117636752717874....\n",
      "-----------------------------------\n",
      "Training loss: 0.45359932125930713....\n",
      "Validation loss: 0.421104511719174....\n",
      "-----------------------------------\n",
      "Training loss: 0.45346662508918173....\n",
      "Validation loss: 0.4210327917276567....\n",
      "-----------------------------------\n",
      "Training loss: 0.4533340360195792....\n",
      "Validation loss: 0.4209611488745771....\n",
      "-----------------------------------\n",
      "Training loss: 0.45320157889910195....\n",
      "Validation loss: 0.4208895208581942....\n",
      "-----------------------------------\n",
      "Training loss: 0.45306923261608895....\n",
      "Validation loss: 0.42081805060838795....\n",
      "-----------------------------------\n",
      "Training loss: 0.4529370060840001....\n",
      "Validation loss: 0.42074658395348846....\n",
      "-----------------------------------\n",
      "Training loss: 0.4528048934246257....\n",
      "Validation loss: 0.4206752408235642....\n",
      "-----------------------------------\n",
      "Training loss: 0.4526728844666934....\n",
      "Validation loss: 0.42060396831550706....\n",
      "-----------------------------------\n",
      "Training loss: 0.45254097234631957....\n",
      "Validation loss: 0.420532815796145....\n",
      "-----------------------------------\n",
      "Training loss: 0.45240918592819446....\n",
      "Validation loss: 0.4204617387695573....\n",
      "-----------------------------------\n",
      "Training loss: 0.45227752518732506....\n",
      "Validation loss: 0.42039066223675126....\n",
      "-----------------------------------\n",
      "Training loss: 0.45214600392528714....\n",
      "Validation loss: 0.4203197593074782....\n",
      "-----------------------------------\n",
      "Training loss: 0.45201459158718515....\n",
      "Validation loss: 0.42024885743256307....\n",
      "-----------------------------------\n",
      "Training loss: 0.45188328779023446....\n",
      "Validation loss: 0.42017811644849534....\n",
      "-----------------------------------\n",
      "Training loss: 0.451752101405311....\n",
      "Validation loss: 0.42010739340131886....\n",
      "-----------------------------------\n",
      "Training loss: 0.4516210078644246....\n",
      "Validation loss: 0.4200368342655668....\n",
      "-----------------------------------\n",
      "Training loss: 0.45149001858626603....\n",
      "Validation loss: 0.4199662463604644....\n",
      "-----------------------------------\n",
      "Training loss: 0.4513591409879564....\n",
      "Validation loss: 0.41989583463794283....\n",
      "-----------------------------------\n",
      "Training loss: 0.4512283427594632....\n",
      "Validation loss: 0.4198254357606763....\n",
      "-----------------------------------\n",
      "Training loss: 0.4510976500340459....\n",
      "Validation loss: 0.4197551682292733....\n",
      "-----------------------------------\n",
      "Training loss: 0.45096704860156966....\n",
      "Validation loss: 0.41968500532167596....\n",
      "-----------------------------------\n",
      "Training loss: 0.4508365586787676....\n",
      "Validation loss: 0.4196148523991093....\n",
      "-----------------------------------\n",
      "Training loss: 0.45070616675923486....\n",
      "Validation loss: 0.4195448688050344....\n",
      "-----------------------------------\n",
      "Training loss: 0.45057586806359634....\n",
      "Validation loss: 0.4194748961244233....\n",
      "-----------------------------------\n",
      "Training loss: 0.4504456933532101....\n",
      "Validation loss: 0.4194049892930244....\n",
      "-----------------------------------\n",
      "Training loss: 0.4503156254389832....\n",
      "Validation loss: 0.41933508695019445....\n",
      "-----------------------------------\n",
      "Training loss: 0.4501856901579582....\n",
      "Validation loss: 0.4192653644083396....\n",
      "-----------------------------------\n",
      "Training loss: 0.4500558794555783....\n",
      "Validation loss: 0.41919565838937684....\n",
      "-----------------------------------\n",
      "Training loss: 0.44992616777792005....\n",
      "Validation loss: 0.41912610153214597....\n",
      "-----------------------------------\n",
      "Training loss: 0.449796563654307....\n",
      "Validation loss: 0.4190565531740847....\n",
      "-----------------------------------\n",
      "Training loss: 0.44966707200312517....\n",
      "Validation loss: 0.41898715386463903....\n",
      "-----------------------------------\n",
      "Training loss: 0.44953767695173624....\n",
      "Validation loss: 0.4189178069268081....\n",
      "-----------------------------------\n",
      "Training loss: 0.449408377192344....\n",
      "Validation loss: 0.4188485731098004....\n",
      "-----------------------------------\n",
      "Training loss: 0.44927918412743206....\n",
      "Validation loss: 0.41877934115787857....\n",
      "-----------------------------------\n",
      "Training loss: 0.4491500901848445....\n",
      "Validation loss: 0.4187102381789396....\n",
      "-----------------------------------\n",
      "Training loss: 0.449021051730816....\n",
      "Validation loss: 0.4186412043039776....\n",
      "-----------------------------------\n",
      "Training loss: 0.44889213219632373....\n",
      "Validation loss: 0.4185722138127534....\n",
      "-----------------------------------\n",
      "Training loss: 0.44876331089934984....\n",
      "Validation loss: 0.4185033176300508....\n",
      "-----------------------------------\n",
      "Training loss: 0.44863458302144604....\n",
      "Validation loss: 0.41843452284629573....\n",
      "-----------------------------------\n",
      "Training loss: 0.4485059468999899....\n",
      "Validation loss: 0.4183657726176331....\n",
      "-----------------------------------\n",
      "Training loss: 0.44837743843142425....\n",
      "Validation loss: 0.41829710345972443....\n",
      "-----------------------------------\n",
      "Training loss: 0.44824901810113005....\n",
      "Validation loss: 0.418228524671531....\n",
      "-----------------------------------\n",
      "Training loss: 0.4481206770613288....\n",
      "Validation loss: 0.41815997668363586....\n",
      "-----------------------------------\n",
      "Training loss: 0.4479924135124571....\n",
      "Validation loss: 0.4180914581381496....\n",
      "-----------------------------------\n",
      "Training loss: 0.44786423326940655....\n",
      "Validation loss: 0.41802304727631606....\n",
      "-----------------------------------\n",
      "Training loss: 0.4477361199534077....\n",
      "Validation loss: 0.4179546883890029....\n",
      "-----------------------------------\n",
      "Training loss: 0.44760810642413557....\n",
      "Validation loss: 0.41788637954346197....\n",
      "-----------------------------------\n",
      "Training loss: 0.4474801894286658....\n",
      "Validation loss: 0.4178181296100997....\n",
      "-----------------------------------\n",
      "Training loss: 0.44735235375288845....\n",
      "Validation loss: 0.41774999493170445....\n",
      "-----------------------------------\n",
      "Training loss: 0.44722462048413547....\n",
      "Validation loss: 0.41768189790013843....\n",
      "-----------------------------------\n",
      "Training loss: 0.4470970246749247....\n",
      "Validation loss: 0.4176139046866237....\n",
      "-----------------------------------\n",
      "Training loss: 0.44696958425668903....\n",
      "Validation loss: 0.4175460269623135....\n",
      "-----------------------------------\n",
      "Training loss: 0.44684223442405235....\n",
      "Validation loss: 0.4174782570096612....\n",
      "-----------------------------------\n",
      "Training loss: 0.4467149746252544....\n",
      "Validation loss: 0.41741053949676266....\n",
      "-----------------------------------\n",
      "Training loss: 0.4465878165406968....\n",
      "Validation loss: 0.4173428180572955....\n",
      "-----------------------------------\n",
      "Training loss: 0.44646075600022167....\n",
      "Validation loss: 0.41727525498394097....\n",
      "-----------------------------------\n",
      "Training loss: 0.44633378606180146....\n",
      "Validation loss: 0.4172077200765012....\n",
      "-----------------------------------\n",
      "Training loss: 0.44620692515097105....\n",
      "Validation loss: 0.41714026043011626....\n",
      "-----------------------------------\n",
      "Training loss: 0.4460801621426241....\n",
      "Validation loss: 0.4170728594870195....\n",
      "-----------------------------------\n",
      "Training loss: 0.445953475443764....\n",
      "Validation loss: 0.41700544990476857....\n",
      "-----------------------------------\n",
      "Training loss: 0.4458268959265593....\n",
      "Validation loss: 0.41693816829643365....\n",
      "-----------------------------------\n",
      "Training loss: 0.44570041587755954....\n",
      "Validation loss: 0.41687095690290543....\n",
      "-----------------------------------\n",
      "Training loss: 0.44557403074569746....\n",
      "Validation loss: 0.4168038230780571....\n",
      "-----------------------------------\n",
      "Training loss: 0.4454477471729609....\n",
      "Validation loss: 0.41673672731258354....\n",
      "-----------------------------------\n",
      "Training loss: 0.44532157731694966....\n",
      "Validation loss: 0.4166697182724286....\n",
      "-----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.89      0.84       499\n",
      "           2       0.84      0.83      0.84       500\n",
      "           3       0.84      0.82      0.83       500\n",
      "           4       0.86      0.85      0.86       500\n",
      "           5       0.81      0.80      0.81       500\n",
      "           6       0.87      0.87      0.87       500\n",
      "           7       0.85      0.84      0.85       500\n",
      "           8       0.79      0.73      0.76       500\n",
      "           9       0.80      0.82      0.81       500\n",
      "          10       0.88      0.90      0.89       500\n",
      "\n",
      "    accuracy                           0.84      4999\n",
      "   macro avg       0.84      0.84      0.83      4999\n",
      "weighted avg       0.84      0.84      0.83      4999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distortion 0.2\n",
    "xtr1= distortion(0.2,X_train_arr,X_train_arr.shape[0])\n",
    "xtest1=distortion(0.2,X_test_arr, X_test_arr.shape[0])\n",
    "C1 = NN(0.01)\n",
    "C1.add_layer(50,256,'relu')\n",
    "C1.add_layer(10,50,'softmax')\n",
    "t_loss,v_loss= train_and_validate(C1,xtr1,Y_train_arr,n=3)\n",
    "report1 = test(C1,xtest1)\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "941147b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.5880221476479988....\n",
      "Validation loss: 2.5909792554724462....\n",
      "-----------------------------------\n",
      "Training loss: 2.574783695951101....\n",
      "Validation loss: 2.579001004614071....\n",
      "-----------------------------------\n",
      "Training loss: 2.562541083867856....\n",
      "Validation loss: 2.5679524982031645....\n",
      "-----------------------------------\n",
      "Training loss: 2.551233105532015....\n",
      "Validation loss: 2.557684220361112....\n",
      "-----------------------------------\n",
      "Training loss: 2.540708443831499....\n",
      "Validation loss: 2.548143060347563....\n",
      "-----------------------------------\n",
      "Training loss: 2.5309122716151116....\n",
      "Validation loss: 2.539267840999017....\n",
      "-----------------------------------\n",
      "Training loss: 2.5217948116129087....\n",
      "Validation loss: 2.530981560619638....\n",
      "-----------------------------------\n",
      "Training loss: 2.5132666415436202....\n",
      "Validation loss: 2.5232092937423762....\n",
      "-----------------------------------\n",
      "Training loss: 2.5052662511794934....\n",
      "Validation loss: 2.515912882975651....\n",
      "-----------------------------------\n",
      "Training loss: 2.497735646923007....\n",
      "Validation loss: 2.5090389394312047....\n",
      "-----------------------------------\n",
      "Training loss: 2.4906513675287094....\n",
      "Validation loss: 2.502552848652774....\n",
      "-----------------------------------\n",
      "Training loss: 2.483971285887454....\n",
      "Validation loss: 2.496456051634833....\n",
      "-----------------------------------\n",
      "Training loss: 2.4776758655168543....\n",
      "Validation loss: 2.4907276431205347....\n",
      "-----------------------------------\n",
      "Training loss: 2.4717304859344145....\n",
      "Validation loss: 2.485306046607683....\n",
      "-----------------------------------\n",
      "Training loss: 2.4660999504767953....\n",
      "Validation loss: 2.480145321831334....\n",
      "-----------------------------------\n",
      "Training loss: 2.4607315215391954....\n",
      "Validation loss: 2.475239660022684....\n",
      "-----------------------------------\n",
      "Training loss: 2.4556319962283615....\n",
      "Validation loss: 2.4705631268090467....\n",
      "-----------------------------------\n",
      "Training loss: 2.450780952470467....\n",
      "Validation loss: 2.4660902404626786....\n",
      "-----------------------------------\n",
      "Training loss: 2.446125318980754....\n",
      "Validation loss: 2.461807583615905....\n",
      "-----------------------------------\n",
      "Training loss: 2.441666089309052....\n",
      "Validation loss: 2.4577175660154937....\n",
      "-----------------------------------\n",
      "Training loss: 2.4374117210540542....\n",
      "Validation loss: 2.4538047413247885....\n",
      "-----------------------------------\n",
      "Training loss: 2.4333386516520528....\n",
      "Validation loss: 2.450045039902716....\n",
      "-----------------------------------\n",
      "Training loss: 2.4294253079357992....\n",
      "Validation loss: 2.4464276091815....\n",
      "-----------------------------------\n",
      "Training loss: 2.4256625570276844....\n",
      "Validation loss: 2.442943143825527....\n",
      "-----------------------------------\n",
      "Training loss: 2.4220406280942917....\n",
      "Validation loss: 2.4395817697558004....\n",
      "-----------------------------------\n",
      "Training loss: 2.418538766173272....\n",
      "Validation loss: 2.436347829779734....\n",
      "-----------------------------------\n",
      "Training loss: 2.4151542510860193....\n",
      "Validation loss: 2.433228032505729....\n",
      "-----------------------------------\n",
      "Training loss: 2.4118876604409953....\n",
      "Validation loss: 2.430204087543676....\n",
      "-----------------------------------\n",
      "Training loss: 2.4087193300651033....\n",
      "Validation loss: 2.427274675538216....\n",
      "-----------------------------------\n",
      "Training loss: 2.4056502347548254....\n",
      "Validation loss: 2.4244342765110614....\n",
      "-----------------------------------\n",
      "Training loss: 2.402671959046484....\n",
      "Validation loss: 2.421680812286355....\n",
      "-----------------------------------\n",
      "Training loss: 2.3997785405602845....\n",
      "Validation loss: 2.41900864881141....\n",
      "-----------------------------------\n",
      "Training loss: 2.396966246650116....\n",
      "Validation loss: 2.4164075749539577....\n",
      "-----------------------------------\n",
      "Training loss: 2.394231189723316....\n",
      "Validation loss: 2.4138756858240704....\n",
      "-----------------------------------\n",
      "Training loss: 2.391574497467599....\n",
      "Validation loss: 2.41141092111929....\n",
      "-----------------------------------\n",
      "Training loss: 2.3889783983628075....\n",
      "Validation loss: 2.4090001481507293....\n",
      "-----------------------------------\n",
      "Training loss: 2.386440682601681....\n",
      "Validation loss: 2.4066537654713036....\n",
      "-----------------------------------\n",
      "Training loss: 2.383976746099301....\n",
      "Validation loss: 2.404363760908738....\n",
      "-----------------------------------\n",
      "Training loss: 2.3815720032522933....\n",
      "Validation loss: 2.402129448393027....\n",
      "-----------------------------------\n",
      "Training loss: 2.3792232629704184....\n",
      "Validation loss: 2.399942495121974....\n",
      "-----------------------------------\n",
      "Training loss: 2.376920942939425....\n",
      "Validation loss: 2.3977966069695897....\n",
      "-----------------------------------\n",
      "Training loss: 2.3746623333530708....\n",
      "Validation loss: 2.395695844519244....\n",
      "-----------------------------------\n",
      "Training loss: 2.3724497929183337....\n",
      "Validation loss: 2.3936302087974517....\n",
      "-----------------------------------\n",
      "Training loss: 2.3702762425433406....\n",
      "Validation loss: 2.3915993514048544....\n",
      "-----------------------------------\n",
      "Training loss: 2.3681458592813884....\n",
      "Validation loss: 2.3896100675894574....\n",
      "-----------------------------------\n",
      "Training loss: 2.3660588613963456....\n",
      "Validation loss: 2.387655803466767....\n",
      "-----------------------------------\n",
      "Training loss: 2.3640073590105017....\n",
      "Validation loss: 2.385733960404713....\n",
      "-----------------------------------\n",
      "Training loss: 2.361985401711557....\n",
      "Validation loss: 2.383841401690836....\n",
      "-----------------------------------\n",
      "Training loss: 2.359998305455287....\n",
      "Validation loss: 2.381981867333568....\n",
      "-----------------------------------\n",
      "Training loss: 2.3580452433611976....\n",
      "Validation loss: 2.380152323880367....\n",
      "-----------------------------------\n",
      "Training loss: 2.356121182763436....\n",
      "Validation loss: 2.378353317908689....\n",
      "-----------------------------------\n",
      "Training loss: 2.3542296496655406....\n",
      "Validation loss: 2.3765848128501257....\n",
      "-----------------------------------\n",
      "Training loss: 2.3523672043818666....\n",
      "Validation loss: 2.3748399918629577....\n",
      "-----------------------------------\n",
      "Training loss: 2.3505321177185117....\n",
      "Validation loss: 2.3731164839446084....\n",
      "-----------------------------------\n",
      "Training loss: 2.348722410740721....\n",
      "Validation loss: 2.3714178473291216....\n",
      "-----------------------------------\n",
      "Training loss: 2.3469364236441463....\n",
      "Validation loss: 2.3697397461225966....\n",
      "-----------------------------------\n",
      "Training loss: 2.3451740322069905....\n",
      "Validation loss: 2.3680860586074783....\n",
      "-----------------------------------\n",
      "Training loss: 2.3434359480547187....\n",
      "Validation loss: 2.3664535240144007....\n",
      "-----------------------------------\n",
      "Training loss: 2.3417184702488525....\n",
      "Validation loss: 2.364838950680278....\n",
      "-----------------------------------\n",
      "Training loss: 2.3400189858355023....\n",
      "Validation loss: 2.363242453719637....\n",
      "-----------------------------------\n",
      "Training loss: 2.338337150558127....\n",
      "Validation loss: 2.361665199560029....\n",
      "-----------------------------------\n",
      "Training loss: 2.3366734894150074....\n",
      "Validation loss: 2.3601003602855677....\n",
      "-----------------------------------\n",
      "Training loss: 2.3350229639856344....\n",
      "Validation loss: 2.3585506732073154....\n",
      "-----------------------------------\n",
      "Training loss: 2.3333881504603924....\n",
      "Validation loss: 2.357016080248645....\n",
      "-----------------------------------\n",
      "Training loss: 2.331767143682812....\n",
      "Validation loss: 2.3554960337436888....\n",
      "-----------------------------------\n",
      "Training loss: 2.3301600701489704....\n",
      "Validation loss: 2.3539923227813975....\n",
      "-----------------------------------\n",
      "Training loss: 2.3285701013373856....\n",
      "Validation loss: 2.352505111775594....\n",
      "-----------------------------------\n",
      "Training loss: 2.3269990462169594....\n",
      "Validation loss: 2.351032687847881....\n",
      "-----------------------------------\n",
      "Training loss: 2.3254439014469606....\n",
      "Validation loss: 2.349571508538124....\n",
      "-----------------------------------\n",
      "Training loss: 2.3238997128908436....\n",
      "Validation loss: 2.348125573930372....\n",
      "-----------------------------------\n",
      "Training loss: 2.322368480570268....\n",
      "Validation loss: 2.346695317910513....\n",
      "-----------------------------------\n",
      "Training loss: 2.3208518522033392....\n",
      "Validation loss: 2.3452802947534135....\n",
      "-----------------------------------\n",
      "Training loss: 2.319347737114191....\n",
      "Validation loss: 2.343874106496011....\n",
      "-----------------------------------\n",
      "Training loss: 2.317852964083681....\n",
      "Validation loss: 2.3424795358060955....\n",
      "-----------------------------------\n",
      "Training loss: 2.3163693985712253....\n",
      "Validation loss: 2.34109677230026....\n",
      "-----------------------------------\n",
      "Training loss: 2.314896255800204....\n",
      "Validation loss: 2.339725469016993....\n",
      "-----------------------------------\n",
      "Training loss: 2.313433604625015....\n",
      "Validation loss: 2.3383612388204913....\n",
      "-----------------------------------\n",
      "Training loss: 2.311980584429541....\n",
      "Validation loss: 2.3370068064466905....\n",
      "-----------------------------------\n",
      "Training loss: 2.3105384409946956....\n",
      "Validation loss: 2.3356618627797285....\n",
      "-----------------------------------\n",
      "Training loss: 2.309104930695264....\n",
      "Validation loss: 2.3343237427352417....\n",
      "-----------------------------------\n",
      "Training loss: 2.3076780810690845....\n",
      "Validation loss: 2.332993098629895....\n",
      "-----------------------------------\n",
      "Training loss: 2.3062573038227767....\n",
      "Validation loss: 2.331670121788427....\n",
      "-----------------------------------\n",
      "Training loss: 2.3048468735573544....\n",
      "Validation loss: 2.3303559253682797....\n",
      "-----------------------------------\n",
      "Training loss: 2.3034459509542824....\n",
      "Validation loss: 2.329049239243119....\n",
      "-----------------------------------\n",
      "Training loss: 2.302054315343439....\n",
      "Validation loss: 2.3277486681957034....\n",
      "-----------------------------------\n",
      "Training loss: 2.300670089693363....\n",
      "Validation loss: 2.3264522443686895....\n",
      "-----------------------------------\n",
      "Training loss: 2.2992908488309145....\n",
      "Validation loss: 2.325164029792719....\n",
      "-----------------------------------\n",
      "Training loss: 2.2979208787740113....\n",
      "Validation loss: 2.3238811824261076....\n",
      "-----------------------------------\n",
      "Training loss: 2.296557319049858....\n",
      "Validation loss: 2.3226045689911183....\n",
      "-----------------------------------\n",
      "Training loss: 2.295199996814174....\n",
      "Validation loss: 2.3213341891127337....\n",
      "-----------------------------------\n",
      "Training loss: 2.2938499090364775....\n",
      "Validation loss: 2.320070266077356....\n",
      "-----------------------------------\n",
      "Training loss: 2.2925064037393645....\n",
      "Validation loss: 2.3188147298532225....\n",
      "-----------------------------------\n",
      "Training loss: 2.291171671705811....\n",
      "Validation loss: 2.317563819158766....\n",
      "-----------------------------------\n",
      "Training loss: 2.2898400122403513....\n",
      "Validation loss: 2.3163181471322214....\n",
      "-----------------------------------\n",
      "Training loss: 2.28851558674014....\n",
      "Validation loss: 2.3150764496616603....\n",
      "-----------------------------------\n",
      "Training loss: 2.2871972948672306....\n",
      "Validation loss: 2.3138401904020847....\n",
      "-----------------------------------\n",
      "Training loss: 2.2858847258676205....\n",
      "Validation loss: 2.312609549397579....\n",
      "-----------------------------------\n",
      "Training loss: 2.2845770187740624....\n",
      "Validation loss: 2.311384735649318....\n",
      "-----------------------------------\n",
      "Training loss: 2.2832752432456154....\n",
      "Validation loss: 2.31016460219031....\n",
      "-----------------------------------\n",
      "Training loss: 2.281977806247863....\n",
      "Validation loss: 2.3089481539187897....\n",
      "-----------------------------------\n",
      "Training loss: 2.2806833798381305....\n",
      "Validation loss: 2.3077378865130944....\n",
      "-----------------------------------\n",
      "Training loss: 2.2793953518091787....\n",
      "Validation loss: 2.3065333973959206....\n",
      "-----------------------------------\n",
      "Training loss: 2.2781144729934546....\n",
      "Validation loss: 2.305332532828571....\n",
      "-----------------------------------\n",
      "Training loss: 2.276839321138839....\n",
      "Validation loss: 2.304135853181984....\n",
      "-----------------------------------\n",
      "Training loss: 2.2755679007154512....\n",
      "Validation loss: 2.3029444933264....\n",
      "-----------------------------------\n",
      "Training loss: 2.274302568021613....\n",
      "Validation loss: 2.301757715934387....\n",
      "-----------------------------------\n",
      "Training loss: 2.2730423665437356....\n",
      "Validation loss: 2.3005761092738055....\n",
      "-----------------------------------\n",
      "Training loss: 2.27178662822026....\n",
      "Validation loss: 2.299401100603472....\n",
      "-----------------------------------\n",
      "Training loss: 2.270536212297936....\n",
      "Validation loss: 2.298227527722137....\n",
      "-----------------------------------\n",
      "Training loss: 2.2692874673408676....\n",
      "Validation loss: 2.2970571202295633....\n",
      "-----------------------------------\n",
      "Training loss: 2.268041551779551....\n",
      "Validation loss: 2.2958901367970084....\n",
      "-----------------------------------\n",
      "Training loss: 2.2667997544075678....\n",
      "Validation loss: 2.2947279665707367....\n",
      "-----------------------------------\n",
      "Training loss: 2.2655628134662513....\n",
      "Validation loss: 2.293568992453412....\n",
      "-----------------------------------\n",
      "Training loss: 2.2643312218014744....\n",
      "Validation loss: 2.292414400328768....\n",
      "-----------------------------------\n",
      "Training loss: 2.2631045983349285....\n",
      "Validation loss: 2.2912622222053556....\n",
      "-----------------------------------\n",
      "Training loss: 2.2618799261768325....\n",
      "Validation loss: 2.290113733323031....\n",
      "-----------------------------------\n",
      "Training loss: 2.2606586535554576....\n",
      "Validation loss: 2.2889697956077306....\n",
      "-----------------------------------\n",
      "Training loss: 2.259443985213155....\n",
      "Validation loss: 2.2878316411994084....\n",
      "-----------------------------------\n",
      "Training loss: 2.258236257815384....\n",
      "Validation loss: 2.2866972851626595....\n",
      "-----------------------------------\n",
      "Training loss: 2.257031444931913....\n",
      "Validation loss: 2.2855674221049522....\n",
      "-----------------------------------\n",
      "Training loss: 2.255829291172432....\n",
      "Validation loss: 2.284438581495202....\n",
      "-----------------------------------\n",
      "Training loss: 2.2546292287008294....\n",
      "Validation loss: 2.283313917167069....\n",
      "-----------------------------------\n",
      "Training loss: 2.253431151949197....\n",
      "Validation loss: 2.2821922554156457....\n",
      "-----------------------------------\n",
      "Training loss: 2.25223611760347....\n",
      "Validation loss: 2.2810724209779107....\n",
      "-----------------------------------\n",
      "Training loss: 2.25104396388309....\n",
      "Validation loss: 2.279955926971651....\n",
      "-----------------------------------\n",
      "Training loss: 2.2498543191724965....\n",
      "Validation loss: 2.2788427894792505....\n",
      "-----------------------------------\n",
      "Training loss: 2.248666731889434....\n",
      "Validation loss: 2.2777338067475026....\n",
      "-----------------------------------\n",
      "Training loss: 2.247482286854411....\n",
      "Validation loss: 2.2766276758082094....\n",
      "-----------------------------------\n",
      "Training loss: 2.246300784797695....\n",
      "Validation loss: 2.2755262778325753....\n",
      "-----------------------------------\n",
      "Training loss: 2.2451243447239047....\n",
      "Validation loss: 2.274428593750552....\n",
      "-----------------------------------\n",
      "Training loss: 2.2439515385096396....\n",
      "Validation loss: 2.273336295145632....\n",
      "-----------------------------------\n",
      "Training loss: 2.242782638859028....\n",
      "Validation loss: 2.2722479216697367....\n",
      "-----------------------------------\n",
      "Training loss: 2.2416159067827457....\n",
      "Validation loss: 2.271162179126783....\n",
      "-----------------------------------\n",
      "Training loss: 2.2404509696843373....\n",
      "Validation loss: 2.2700795688530224....\n",
      "-----------------------------------\n",
      "Training loss: 2.239288233842849....\n",
      "Validation loss: 2.2689980310044766....\n",
      "-----------------------------------\n",
      "Training loss: 2.238127971627262....\n",
      "Validation loss: 2.2679189894353375....\n",
      "-----------------------------------\n",
      "Training loss: 2.2369705149518246....\n",
      "Validation loss: 2.2668419066165306....\n",
      "-----------------------------------\n",
      "Training loss: 2.2358137876730044....\n",
      "Validation loss: 2.2657662608603752....\n",
      "-----------------------------------\n",
      "Training loss: 2.234658473023344....\n",
      "Validation loss: 2.2646909036568013....\n",
      "-----------------------------------\n",
      "Training loss: 2.233505101563628....\n",
      "Validation loss: 2.263618143572796....\n",
      "-----------------------------------\n",
      "Training loss: 2.232354378705997....\n",
      "Validation loss: 2.2625466004442405....\n",
      "-----------------------------------\n",
      "Training loss: 2.23120552657273....\n",
      "Validation loss: 2.2614777626458125....\n",
      "-----------------------------------\n",
      "Training loss: 2.2300588929717295....\n",
      "Validation loss: 2.2604094414558236....\n",
      "-----------------------------------\n",
      "Training loss: 2.228911810886291....\n",
      "Validation loss: 2.2593421863767427....\n",
      "-----------------------------------\n",
      "Training loss: 2.227766758657866....\n",
      "Validation loss: 2.258277441686404....\n",
      "-----------------------------------\n",
      "Training loss: 2.2266261629126296....\n",
      "Validation loss: 2.257214904190279....\n",
      "-----------------------------------\n",
      "Training loss: 2.225489086964473....\n",
      "Validation loss: 2.256154174089892....\n",
      "-----------------------------------\n",
      "Training loss: 2.224353295334372....\n",
      "Validation loss: 2.25509522995666....\n",
      "-----------------------------------\n",
      "Training loss: 2.223218588166611....\n",
      "Validation loss: 2.254039013604698....\n",
      "-----------------------------------\n",
      "Training loss: 2.222085564764049....\n",
      "Validation loss: 2.2529850546871084....\n",
      "-----------------------------------\n",
      "Training loss: 2.220953644518131....\n",
      "Validation loss: 2.251934004345935....\n",
      "-----------------------------------\n",
      "Training loss: 2.219823947925981....\n",
      "Validation loss: 2.2508846247053906....\n",
      "-----------------------------------\n",
      "Training loss: 2.21869738566175....\n",
      "Validation loss: 2.249836827586227....\n",
      "-----------------------------------\n",
      "Training loss: 2.217572783884245....\n",
      "Validation loss: 2.248791957770754....\n",
      "-----------------------------------\n",
      "Training loss: 2.21645180102515....\n",
      "Validation loss: 2.24775144255852....\n",
      "-----------------------------------\n",
      "Training loss: 2.215334753559477....\n",
      "Validation loss: 2.246711068558566....\n",
      "-----------------------------------\n",
      "Training loss: 2.214217666765836....\n",
      "Validation loss: 2.24567321294185....\n",
      "-----------------------------------\n",
      "Training loss: 2.2131014421997253....\n",
      "Validation loss: 2.2446355166301672....\n",
      "-----------------------------------\n",
      "Training loss: 2.211985470537286....\n",
      "Validation loss: 2.2435993947246082....\n",
      "-----------------------------------\n",
      "Training loss: 2.2108710427780998....\n",
      "Validation loss: 2.2425643470816228....\n",
      "-----------------------------------\n",
      "Training loss: 2.2097587038364637....\n",
      "Validation loss: 2.241529945735375....\n",
      "-----------------------------------\n",
      "Training loss: 2.208648192979146....\n",
      "Validation loss: 2.2404969463504587....\n",
      "-----------------------------------\n",
      "Training loss: 2.2075398440100016....\n",
      "Validation loss: 2.239466721915401....\n",
      "-----------------------------------\n",
      "Training loss: 2.206434376650919....\n",
      "Validation loss: 2.238438495788331....\n",
      "-----------------------------------\n",
      "Training loss: 2.2053299530458172....\n",
      "Validation loss: 2.23741276050531....\n",
      "-----------------------------------\n",
      "Training loss: 2.2042272663168716....\n",
      "Validation loss: 2.236388437336043....\n",
      "-----------------------------------\n",
      "Training loss: 2.203125456486245....\n",
      "Validation loss: 2.2353659458161284....\n",
      "-----------------------------------\n",
      "Training loss: 2.202025215944267....\n",
      "Validation loss: 2.2343449123756254....\n",
      "-----------------------------------\n",
      "Training loss: 2.2009254678844066....\n",
      "Validation loss: 2.2333246596888157....\n",
      "-----------------------------------\n",
      "Training loss: 2.1998261372407057....\n",
      "Validation loss: 2.232305022294034....\n",
      "-----------------------------------\n",
      "Training loss: 2.198728259735768....\n",
      "Validation loss: 2.2312873520141716....\n",
      "-----------------------------------\n",
      "Training loss: 2.19763161373262....\n",
      "Validation loss: 2.230272025092305....\n",
      "-----------------------------------\n",
      "Training loss: 2.196536961233803....\n",
      "Validation loss: 2.2292573303480285....\n",
      "-----------------------------------\n",
      "Training loss: 2.195441239509693....\n",
      "Validation loss: 2.2282433964758352....\n",
      "-----------------------------------\n",
      "Training loss: 2.194345690951139....\n",
      "Validation loss: 2.2272311291426057....\n",
      "-----------------------------------\n",
      "Training loss: 2.193251283348532....\n",
      "Validation loss: 2.2262193279276272....\n",
      "-----------------------------------\n",
      "Training loss: 2.1921565057886117....\n",
      "Validation loss: 2.2252077140125976....\n",
      "-----------------------------------\n",
      "Training loss: 2.191062528328475....\n",
      "Validation loss: 2.2241977537404782....\n",
      "-----------------------------------\n",
      "Training loss: 2.1899693158244142....\n",
      "Validation loss: 2.223188611394432....\n",
      "-----------------------------------\n",
      "Training loss: 2.1888766298727176....\n",
      "Validation loss: 2.2221811936863327....\n",
      "-----------------------------------\n",
      "Training loss: 2.187786096268561....\n",
      "Validation loss: 2.2211755831038174....\n",
      "-----------------------------------\n",
      "Training loss: 2.1866967066349856....\n",
      "Validation loss: 2.2201693556724096....\n",
      "-----------------------------------\n",
      "Training loss: 2.1856079237784303....\n",
      "Validation loss: 2.219163747393753....\n",
      "-----------------------------------\n",
      "Training loss: 2.1845208117695054....\n",
      "Validation loss: 2.218158062285952....\n",
      "-----------------------------------\n",
      "Training loss: 2.183433542114604....\n",
      "Validation loss: 2.217152867576446....\n",
      "-----------------------------------\n",
      "Training loss: 2.1823471197752453....\n",
      "Validation loss: 2.216146460531658....\n",
      "-----------------------------------\n",
      "Training loss: 2.1812608421064437....\n",
      "Validation loss: 2.2151406597784606....\n",
      "-----------------------------------\n",
      "Training loss: 2.180176509678989....\n",
      "Validation loss: 2.214136409291492....\n",
      "-----------------------------------\n",
      "Training loss: 2.1790934286393786....\n",
      "Validation loss: 2.213133762509569....\n",
      "-----------------------------------\n",
      "Training loss: 2.178011103449003....\n",
      "Validation loss: 2.212133030784309....\n",
      "-----------------------------------\n",
      "Training loss: 2.1769298645175312....\n",
      "Validation loss: 2.2111343556049627....\n",
      "-----------------------------------\n",
      "Training loss: 2.1758494492266354....\n",
      "Validation loss: 2.2101368488108073....\n",
      "-----------------------------------\n",
      "Training loss: 2.174770252832385....\n",
      "Validation loss: 2.209138529276649....\n",
      "-----------------------------------\n",
      "Training loss: 2.1736916107864253....\n",
      "Validation loss: 2.2081411027991016....\n",
      "-----------------------------------\n",
      "Training loss: 2.172614303786252....\n",
      "Validation loss: 2.2071441005988013....\n",
      "-----------------------------------\n",
      "Training loss: 2.171536019126304....\n",
      "Validation loss: 2.206147773743629....\n",
      "-----------------------------------\n",
      "Training loss: 2.1704572490790137....\n",
      "Validation loss: 2.205152250104712....\n",
      "-----------------------------------\n",
      "Training loss: 2.1693797454819554....\n",
      "Validation loss: 2.204157204241813....\n",
      "-----------------------------------\n",
      "Training loss: 2.16830228196064....\n",
      "Validation loss: 2.2031626746861965....\n",
      "-----------------------------------\n",
      "Training loss: 2.1672259761241865....\n",
      "Validation loss: 2.202169498479469....\n",
      "-----------------------------------\n",
      "Training loss: 2.1661509916743946....\n",
      "Validation loss: 2.201176881005547....\n",
      "-----------------------------------\n",
      "Training loss: 2.1650749236987568....\n",
      "Validation loss: 2.2001838345700024....\n",
      "-----------------------------------\n",
      "Training loss: 2.163998761612515....\n",
      "Validation loss: 2.19919190356787....\n",
      "-----------------------------------\n",
      "Training loss: 2.1629238502608183....\n",
      "Validation loss: 2.198199937861581....\n",
      "-----------------------------------\n",
      "Training loss: 2.1618492467073285....\n",
      "Validation loss: 2.1972098524443697....\n",
      "-----------------------------------\n",
      "Training loss: 2.160776704313218....\n",
      "Validation loss: 2.19622212911365....\n",
      "-----------------------------------\n",
      "Training loss: 2.159704915588941....\n",
      "Validation loss: 2.1952349716940165....\n",
      "-----------------------------------\n",
      "Training loss: 2.158633609751026....\n",
      "Validation loss: 2.1942488737564654....\n",
      "-----------------------------------\n",
      "Training loss: 2.157564002201566....\n",
      "Validation loss: 2.19326354331447....\n",
      "-----------------------------------\n",
      "Training loss: 2.156494761756663....\n",
      "Validation loss: 2.1922780839400557....\n",
      "-----------------------------------\n",
      "Training loss: 2.155426065060409....\n",
      "Validation loss: 2.191292319602592....\n",
      "-----------------------------------\n",
      "Training loss: 2.1543581939814316....\n",
      "Validation loss: 2.1903074460954897....\n",
      "-----------------------------------\n",
      "Training loss: 2.1532912481842486....\n",
      "Validation loss: 2.189322934831956....\n",
      "-----------------------------------\n",
      "Training loss: 2.1522257710092374....\n",
      "Validation loss: 2.188339043355355....\n",
      "-----------------------------------\n",
      "Training loss: 2.1511607463316462....\n",
      "Validation loss: 2.187355013096975....\n",
      "-----------------------------------\n",
      "Training loss: 2.150095697437747....\n",
      "Validation loss: 2.186372710079715....\n",
      "-----------------------------------\n",
      "Training loss: 2.149030756706569....\n",
      "Validation loss: 2.1853902538022925....\n",
      "-----------------------------------\n",
      "Training loss: 2.147966706770776....\n",
      "Validation loss: 2.184406507854129....\n",
      "-----------------------------------\n",
      "Training loss: 2.1469027026056238....\n",
      "Validation loss: 2.183423715145873....\n",
      "-----------------------------------\n",
      "Training loss: 2.145839357313679....\n",
      "Validation loss: 2.182442213522762....\n",
      "-----------------------------------\n",
      "Training loss: 2.1447771606496633....\n",
      "Validation loss: 2.1814597934355806....\n",
      "-----------------------------------\n",
      "Training loss: 2.1437160286434356....\n",
      "Validation loss: 2.1804771184676577....\n",
      "-----------------------------------\n",
      "Training loss: 2.142655268659058....\n",
      "Validation loss: 2.1794929987053555....\n",
      "-----------------------------------\n",
      "Training loss: 2.1415941999987127....\n",
      "Validation loss: 2.1785101194153857....\n",
      "-----------------------------------\n",
      "Training loss: 2.1405325285285186....\n",
      "Validation loss: 2.1775265889076554....\n",
      "-----------------------------------\n",
      "Training loss: 2.139470388496527....\n",
      "Validation loss: 2.1765436631490855....\n",
      "-----------------------------------\n",
      "Training loss: 2.1384085332197174....\n",
      "Validation loss: 2.1755623293838373....\n",
      "-----------------------------------\n",
      "Training loss: 2.137347331250136....\n",
      "Validation loss: 2.174581224794305....\n",
      "-----------------------------------\n",
      "Training loss: 2.136286489836236....\n",
      "Validation loss: 2.1736003413991756....\n",
      "-----------------------------------\n",
      "Training loss: 2.135226059321198....\n",
      "Validation loss: 2.1726183007453046....\n",
      "-----------------------------------\n",
      "Training loss: 2.1341641668733065....\n",
      "Validation loss: 2.171636711946403....\n",
      "-----------------------------------\n",
      "Training loss: 2.1331015300168508....\n",
      "Validation loss: 2.17065471769493....\n",
      "-----------------------------------\n",
      "Training loss: 2.1320376692283185....\n",
      "Validation loss: 2.1696726366594845....\n",
      "-----------------------------------\n",
      "Training loss: 2.1309726580304282....\n",
      "Validation loss: 2.1686920418972395....\n",
      "-----------------------------------\n",
      "Training loss: 2.1299086069910187....\n",
      "Validation loss: 2.1677119460346503....\n",
      "-----------------------------------\n",
      "Training loss: 2.128844929508493....\n",
      "Validation loss: 2.166731487121945....\n",
      "-----------------------------------\n",
      "Training loss: 2.127781031837384....\n",
      "Validation loss: 2.1657516957751874....\n",
      "-----------------------------------\n",
      "Training loss: 2.1267175418193998....\n",
      "Validation loss: 2.164771770696438....\n",
      "-----------------------------------\n",
      "Training loss: 2.1256542814296475....\n",
      "Validation loss: 2.1637930741756053....\n",
      "-----------------------------------\n",
      "Training loss: 2.1245902730119637....\n",
      "Validation loss: 2.1628144645301424....\n",
      "-----------------------------------\n",
      "Training loss: 2.123525710240039....\n",
      "Validation loss: 2.1618351068300954....\n",
      "-----------------------------------\n",
      "Training loss: 2.1224595914485014....\n",
      "Validation loss: 2.160855014228444....\n",
      "-----------------------------------\n",
      "Training loss: 2.1213924362238847....\n",
      "Validation loss: 2.159875862791196....\n",
      "-----------------------------------\n",
      "Training loss: 2.1203271532817505....\n",
      "Validation loss: 2.1588977133907057....\n",
      "-----------------------------------\n",
      "Training loss: 2.119262258069958....\n",
      "Validation loss: 2.15792029872188....\n",
      "-----------------------------------\n",
      "Training loss: 2.1181969374738605....\n",
      "Validation loss: 2.1569446816764573....\n",
      "-----------------------------------\n",
      "Training loss: 2.1171326524691145....\n",
      "Validation loss: 2.1559694491988286....\n",
      "-----------------------------------\n",
      "Training loss: 2.1160679681904364....\n",
      "Validation loss: 2.15499387148231....\n",
      "-----------------------------------\n",
      "Training loss: 2.1150021564634724....\n",
      "Validation loss: 2.154019404059186....\n",
      "-----------------------------------\n",
      "Training loss: 2.113936579910668....\n",
      "Validation loss: 2.1530452728084675....\n",
      "-----------------------------------\n",
      "Training loss: 2.1128699237072803....\n",
      "Validation loss: 2.152070816210942....\n",
      "-----------------------------------\n",
      "Training loss: 2.1118023928572196....\n",
      "Validation loss: 2.1510963633899483....\n",
      "-----------------------------------\n",
      "Training loss: 2.110734033598346....\n",
      "Validation loss: 2.1501213530954337....\n",
      "-----------------------------------\n",
      "Training loss: 2.1096662195402565....\n",
      "Validation loss: 2.149148118486011....\n",
      "-----------------------------------\n",
      "Training loss: 2.1085990725372126....\n",
      "Validation loss: 2.148175724179505....\n",
      "-----------------------------------\n",
      "Training loss: 2.107530633099074....\n",
      "Validation loss: 2.1472023075485094....\n",
      "-----------------------------------\n",
      "Training loss: 2.106461622384668....\n",
      "Validation loss: 2.146229204155838....\n",
      "-----------------------------------\n",
      "Training loss: 2.1053932776396103....\n",
      "Validation loss: 2.145256779779066....\n",
      "-----------------------------------\n",
      "Training loss: 2.1043260050733275....\n",
      "Validation loss: 2.1442842610879334....\n",
      "-----------------------------------\n",
      "Training loss: 2.10325878026411....\n",
      "Validation loss: 2.1433123241861205....\n",
      "-----------------------------------\n",
      "Training loss: 2.1021917633927596....\n",
      "Validation loss: 2.1423406177083817....\n",
      "-----------------------------------\n",
      "Training loss: 2.101125434918151....\n",
      "Validation loss: 2.141368020804535....\n",
      "-----------------------------------\n",
      "Training loss: 2.100059606403818....\n",
      "Validation loss: 2.1403951650391573....\n",
      "-----------------------------------\n",
      "Training loss: 2.0989937407327726....\n",
      "Validation loss: 2.139422112525268....\n",
      "-----------------------------------\n",
      "Training loss: 2.097928089965591....\n",
      "Validation loss: 2.138450392524825....\n",
      "-----------------------------------\n",
      "Training loss: 2.096864268980901....\n",
      "Validation loss: 2.137479846356272....\n",
      "-----------------------------------\n",
      "Training loss: 2.0958018105455443....\n",
      "Validation loss: 2.1365098640392772....\n",
      "-----------------------------------\n",
      "Training loss: 2.0947402655730185....\n",
      "Validation loss: 2.1355396309641446....\n",
      "-----------------------------------\n",
      "Training loss: 2.093678907397784....\n",
      "Validation loss: 2.1345690515002174....\n",
      "-----------------------------------\n",
      "Training loss: 2.0926176639405596....\n",
      "Validation loss: 2.1335988845664815....\n",
      "-----------------------------------\n",
      "Training loss: 2.0915565753363774....\n",
      "Validation loss: 2.1326283919399662....\n",
      "-----------------------------------\n",
      "Training loss: 2.0904962111718293....\n",
      "Validation loss: 2.13165937119968....\n",
      "-----------------------------------\n",
      "Training loss: 2.0894370885245337....\n",
      "Validation loss: 2.130691222366627....\n",
      "-----------------------------------\n",
      "Training loss: 2.088378758316779....\n",
      "Validation loss: 2.1297228102083943....\n",
      "-----------------------------------\n",
      "Training loss: 2.087320304243087....\n",
      "Validation loss: 2.1287538637610233....\n",
      "-----------------------------------\n",
      "Training loss: 2.086262825155219....\n",
      "Validation loss: 2.1277860281610232....\n",
      "-----------------------------------\n",
      "Training loss: 2.0852052096708995....\n",
      "Validation loss: 2.1268172716976705....\n",
      "-----------------------------------\n",
      "Training loss: 2.084146528996465....\n",
      "Validation loss: 2.125848968321695....\n",
      "-----------------------------------\n",
      "Training loss: 2.0830874464111435....\n",
      "Validation loss: 2.124881019270033....\n",
      "-----------------------------------\n",
      "Training loss: 2.082028847564553....\n",
      "Validation loss: 2.123913485893713....\n",
      "-----------------------------------\n",
      "Training loss: 2.0809707461629223....\n",
      "Validation loss: 2.1229461396828637....\n",
      "-----------------------------------\n",
      "Training loss: 2.079912108619129....\n",
      "Validation loss: 2.1219799841580973....\n",
      "-----------------------------------\n",
      "Training loss: 2.0788528266436703....\n",
      "Validation loss: 2.1210140111571563....\n",
      "-----------------------------------\n",
      "Training loss: 2.0777920101967915....\n",
      "Validation loss: 2.120048983824887....\n",
      "-----------------------------------\n",
      "Training loss: 2.076731373592544....\n",
      "Validation loss: 2.119085072315735....\n",
      "-----------------------------------\n",
      "Training loss: 2.0756718624229022....\n",
      "Validation loss: 2.1181213515193433....\n",
      "-----------------------------------\n",
      "Training loss: 2.074612848844226....\n",
      "Validation loss: 2.1171584782243547....\n",
      "-----------------------------------\n",
      "Training loss: 2.073553514187791....\n",
      "Validation loss: 2.1161963202054657....\n",
      "-----------------------------------\n",
      "Training loss: 2.072493964568336....\n",
      "Validation loss: 2.1152341857106145....\n",
      "-----------------------------------\n",
      "Training loss: 2.0714352605272066....\n",
      "Validation loss: 2.1142718271230714....\n",
      "-----------------------------------\n",
      "Training loss: 2.070377379057221....\n",
      "Validation loss: 2.113309720180112....\n",
      "-----------------------------------\n",
      "Training loss: 2.0693184891978005....\n",
      "Validation loss: 2.1123479311927853....\n",
      "-----------------------------------\n",
      "Training loss: 2.0682590363763143....\n",
      "Validation loss: 2.1113864905253514....\n",
      "-----------------------------------\n",
      "Training loss: 2.0672007760336797....\n",
      "Validation loss: 2.110425280097194....\n",
      "-----------------------------------\n",
      "Training loss: 2.0661423120809252....\n",
      "Validation loss: 2.1094625638343563....\n",
      "-----------------------------------\n",
      "Training loss: 2.0650838558293283....\n",
      "Validation loss: 2.108499428117705....\n",
      "-----------------------------------\n",
      "Training loss: 2.0640252027406136....\n",
      "Validation loss: 2.1075360767516713....\n",
      "-----------------------------------\n",
      "Training loss: 2.062966220041626....\n",
      "Validation loss: 2.1065722709066086....\n",
      "-----------------------------------\n",
      "Training loss: 2.06190713357832....\n",
      "Validation loss: 2.1056086073929636....\n",
      "-----------------------------------\n",
      "Training loss: 2.060848662120556....\n",
      "Validation loss: 2.1046457095026168....\n",
      "-----------------------------------\n",
      "Training loss: 2.059791396249776....\n",
      "Validation loss: 2.1036827253307075....\n",
      "-----------------------------------\n",
      "Training loss: 2.0587346081381233....\n",
      "Validation loss: 2.102719382590748....\n",
      "-----------------------------------\n",
      "Training loss: 2.0576778839544057....\n",
      "Validation loss: 2.101756797982311....\n",
      "-----------------------------------\n",
      "Training loss: 2.0566220529754444....\n",
      "Validation loss: 2.100793319116453....\n",
      "-----------------------------------\n",
      "Training loss: 2.0555659392825762....\n",
      "Validation loss: 2.0998306256042376....\n",
      "-----------------------------------\n",
      "Training loss: 2.054509613474861....\n",
      "Validation loss: 2.0988677551192287....\n",
      "-----------------------------------\n",
      "Training loss: 2.053453245269565....\n",
      "Validation loss: 2.097905415665755....\n",
      "-----------------------------------\n",
      "Training loss: 2.0523976148078433....\n",
      "Validation loss: 2.0969432534003225....\n",
      "-----------------------------------\n",
      "Training loss: 2.0513419460460125....\n",
      "Validation loss: 2.0959797539721614....\n",
      "-----------------------------------\n",
      "Training loss: 2.0502847298453224....\n",
      "Validation loss: 2.09501524566674....\n",
      "-----------------------------------\n",
      "Training loss: 2.049227263887713....\n",
      "Validation loss: 2.094050909859301....\n",
      "-----------------------------------\n",
      "Training loss: 2.048171049281042....\n",
      "Validation loss: 2.09308727011642....\n",
      "-----------------------------------\n",
      "Training loss: 2.047114795934882....\n",
      "Validation loss: 2.0921235095988524....\n",
      "-----------------------------------\n",
      "Training loss: 2.0460591244993553....\n",
      "Validation loss: 2.0911608010675047....\n",
      "-----------------------------------\n",
      "Training loss: 2.0450035374183884....\n",
      "Validation loss: 2.0901981504315317....\n",
      "-----------------------------------\n",
      "Training loss: 2.0439468651889814....\n",
      "Validation loss: 2.089236054791354....\n",
      "-----------------------------------\n",
      "Training loss: 2.0428902389173977....\n",
      "Validation loss: 2.088273918171077....\n",
      "-----------------------------------\n",
      "Training loss: 2.0418337743652537....\n",
      "Validation loss: 2.0873111106185998....\n",
      "-----------------------------------\n",
      "Training loss: 2.040777584396564....\n",
      "Validation loss: 2.0863488313360365....\n",
      "-----------------------------------\n",
      "Training loss: 2.0397213571831974....\n",
      "Validation loss: 2.0853857282347352....\n",
      "-----------------------------------\n",
      "Training loss: 2.038664282452295....\n",
      "Validation loss: 2.084422418561617....\n",
      "-----------------------------------\n",
      "Training loss: 2.037607049526139....\n",
      "Validation loss: 2.0834583450341526....\n",
      "-----------------------------------\n",
      "Training loss: 2.0365503875659603....\n",
      "Validation loss: 2.0824938805555893....\n",
      "-----------------------------------\n",
      "Training loss: 2.0354940993303132....\n",
      "Validation loss: 2.081529220962638....\n",
      "-----------------------------------\n",
      "Training loss: 2.034437614193704....\n",
      "Validation loss: 2.0805634185244863....\n",
      "-----------------------------------\n",
      "Training loss: 2.033381283462661....\n",
      "Validation loss: 2.0795982706741136....\n",
      "-----------------------------------\n",
      "Training loss: 2.03232539614792....\n",
      "Validation loss: 2.078632531104563....\n",
      "-----------------------------------\n",
      "Training loss: 2.031268754591481....\n",
      "Validation loss: 2.0776663776513606....\n",
      "-----------------------------------\n",
      "Training loss: 2.0302111244163474....\n",
      "Validation loss: 2.07669956035271....\n",
      "-----------------------------------\n",
      "Training loss: 2.0291523531587754....\n",
      "Validation loss: 2.0757335163850095....\n",
      "-----------------------------------\n",
      "Training loss: 2.0280935528225257....\n",
      "Validation loss: 2.0747671502963465....\n",
      "-----------------------------------\n",
      "Training loss: 2.027034037917917....\n",
      "Validation loss: 2.0738019667162453....\n",
      "-----------------------------------\n",
      "Training loss: 2.0259753565969363....\n",
      "Validation loss: 2.0728376917076683....\n",
      "-----------------------------------\n",
      "Training loss: 2.0249163332025426....\n",
      "Validation loss: 2.071872045887037....\n",
      "-----------------------------------\n",
      "Training loss: 2.0238567207761546....\n",
      "Validation loss: 2.070905387095347....\n",
      "-----------------------------------\n",
      "Training loss: 2.022796516714341....\n",
      "Validation loss: 2.0699384367422753....\n",
      "-----------------------------------\n",
      "Training loss: 2.0217360331770338....\n",
      "Validation loss: 2.0689716758156775....\n",
      "-----------------------------------\n",
      "Training loss: 2.020675616733426....\n",
      "Validation loss: 2.068004633526027....\n",
      "-----------------------------------\n",
      "Training loss: 2.0196148891436527....\n",
      "Validation loss: 2.0670375698353958....\n",
      "-----------------------------------\n",
      "Training loss: 2.0185543472974476....\n",
      "Validation loss: 2.0660704576802984....\n",
      "-----------------------------------\n",
      "Training loss: 2.017494045516491....\n",
      "Validation loss: 2.065102852223194....\n",
      "-----------------------------------\n",
      "Training loss: 2.016433520375973....\n",
      "Validation loss: 2.064135542426752....\n",
      "-----------------------------------\n",
      "Training loss: 2.0153732284363466....\n",
      "Validation loss: 2.0631684503464647....\n",
      "-----------------------------------\n",
      "Training loss: 2.014313714113114....\n",
      "Validation loss: 2.062201554028287....\n",
      "-----------------------------------\n",
      "Training loss: 2.0132544795146323....\n",
      "Validation loss: 2.0612347914657105....\n",
      "-----------------------------------\n",
      "Training loss: 2.012195944775453....\n",
      "Validation loss: 2.0602676848671417....\n",
      "-----------------------------------\n",
      "Training loss: 2.011136589429279....\n",
      "Validation loss: 2.059300187404526....\n",
      "-----------------------------------\n",
      "Training loss: 2.0100772321886953....\n",
      "Validation loss: 2.058334552931949....\n",
      "-----------------------------------\n",
      "Training loss: 2.009018670663919....\n",
      "Validation loss: 2.0573684551862383....\n",
      "-----------------------------------\n",
      "Training loss: 2.007959691921733....\n",
      "Validation loss: 2.056400289496004....\n",
      "-----------------------------------\n",
      "Training loss: 2.0068996794868506....\n",
      "Validation loss: 2.0554320425712778....\n",
      "-----------------------------------\n",
      "Training loss: 2.0058401334841984....\n",
      "Validation loss: 2.0544640717625033....\n",
      "-----------------------------------\n",
      "Training loss: 2.0047812244581036....\n",
      "Validation loss: 2.0534947036651983....\n",
      "-----------------------------------\n",
      "Training loss: 2.003721714224417....\n",
      "Validation loss: 2.0525255644432505....\n",
      "-----------------------------------\n",
      "Training loss: 2.002663018002739....\n",
      "Validation loss: 2.0515562654692516....\n",
      "-----------------------------------\n",
      "Training loss: 2.001603333745241....\n",
      "Validation loss: 2.0505862291288435....\n",
      "-----------------------------------\n",
      "Training loss: 2.000543166317635....\n",
      "Validation loss: 2.0496167127115212....\n",
      "-----------------------------------\n",
      "Training loss: 1.9994841970637633....\n",
      "Validation loss: 2.0486474792058815....\n",
      "-----------------------------------\n",
      "Training loss: 1.998424748460385....\n",
      "Validation loss: 2.0476784271156636....\n",
      "-----------------------------------\n",
      "Training loss: 1.9973656690465513....\n",
      "Validation loss: 2.04670901478422....\n",
      "-----------------------------------\n",
      "Training loss: 1.996305342467202....\n",
      "Validation loss: 2.045737726893723....\n",
      "-----------------------------------\n",
      "Training loss: 1.995242082320755....\n",
      "Validation loss: 2.0447671806497882....\n",
      "-----------------------------------\n",
      "Training loss: 1.994178609016856....\n",
      "Validation loss: 2.0437973294551144....\n",
      "-----------------------------------\n",
      "Training loss: 1.9931148710523168....\n",
      "Validation loss: 2.042825405280233....\n",
      "-----------------------------------\n",
      "Training loss: 1.9920495318105915....\n",
      "Validation loss: 2.0418541924382416....\n",
      "-----------------------------------\n",
      "Training loss: 1.9909846880595174....\n",
      "Validation loss: 2.0408825725743287....\n",
      "-----------------------------------\n",
      "Training loss: 1.9899203526276616....\n",
      "Validation loss: 2.039909789972783....\n",
      "-----------------------------------\n",
      "Training loss: 1.9888549786679797....\n",
      "Validation loss: 2.0389359232702393....\n",
      "-----------------------------------\n",
      "Training loss: 1.9877889637306074....\n",
      "Validation loss: 2.037962903238368....\n",
      "-----------------------------------\n",
      "Training loss: 1.9867235177835703....\n",
      "Validation loss: 2.0369883762658136....\n",
      "-----------------------------------\n",
      "Training loss: 1.9856574597760772....\n",
      "Validation loss: 2.0360141065409976....\n",
      "-----------------------------------\n",
      "Training loss: 1.984591726771663....\n",
      "Validation loss: 2.0350399491158453....\n",
      "-----------------------------------\n",
      "Training loss: 1.9835247189474186....\n",
      "Validation loss: 2.0340662711343565....\n",
      "-----------------------------------\n",
      "Training loss: 1.9824578808275426....\n",
      "Validation loss: 2.033091700050193....\n",
      "-----------------------------------\n",
      "Training loss: 1.981391336803463....\n",
      "Validation loss: 2.032117306340034....\n",
      "-----------------------------------\n",
      "Training loss: 1.9803266938680635....\n",
      "Validation loss: 2.0311425375878813....\n",
      "-----------------------------------\n",
      "Training loss: 1.97926262399572....\n",
      "Validation loss: 2.0301672745306916....\n",
      "-----------------------------------\n",
      "Training loss: 1.9781982029267058....\n",
      "Validation loss: 2.0291924807666626....\n",
      "-----------------------------------\n",
      "Training loss: 1.9771333619316687....\n",
      "Validation loss: 2.028217939750699....\n",
      "-----------------------------------\n",
      "Training loss: 1.9760677996678773....\n",
      "Validation loss: 2.027243758540696....\n",
      "-----------------------------------\n",
      "Training loss: 1.975002119875747....\n",
      "Validation loss: 2.0262710517959697....\n",
      "-----------------------------------\n",
      "Training loss: 1.973936760965476....\n",
      "Validation loss: 2.0252969939027143....\n",
      "-----------------------------------\n",
      "Training loss: 1.9728695165112218....\n",
      "Validation loss: 2.02432335526785....\n",
      "-----------------------------------\n",
      "Training loss: 1.9718011056523355....\n",
      "Validation loss: 2.0233493200017842....\n",
      "-----------------------------------\n",
      "Training loss: 1.9707320062330147....\n",
      "Validation loss: 2.02237503422481....\n",
      "-----------------------------------\n",
      "Training loss: 1.9696629088715076....\n",
      "Validation loss: 2.0214009169779006....\n",
      "-----------------------------------\n",
      "Training loss: 1.968593781755209....\n",
      "Validation loss: 2.020426706099451....\n",
      "-----------------------------------\n",
      "Training loss: 1.9675253931721022....\n",
      "Validation loss: 2.0194521783041415....\n",
      "-----------------------------------\n",
      "Training loss: 1.9664564983139314....\n",
      "Validation loss: 2.018478103954388....\n",
      "-----------------------------------\n",
      "Training loss: 1.96538874579811....\n",
      "Validation loss: 2.017504016525344....\n",
      "-----------------------------------\n",
      "Training loss: 1.9643203545506862....\n",
      "Validation loss: 2.0165302731638715....\n",
      "-----------------------------------\n",
      "Training loss: 1.9632515359559068....\n",
      "Validation loss: 2.0155571046933574....\n",
      "-----------------------------------\n",
      "Training loss: 1.9621820939841383....\n",
      "Validation loss: 2.0145843201084435....\n",
      "-----------------------------------\n",
      "Training loss: 1.9611128365725856....\n",
      "Validation loss: 2.013611330768845....\n",
      "-----------------------------------\n",
      "Training loss: 1.9600438279937158....\n",
      "Validation loss: 2.0126392151869776....\n",
      "-----------------------------------\n",
      "Training loss: 1.9589742060714739....\n",
      "Validation loss: 2.011667072241714....\n",
      "-----------------------------------\n",
      "Training loss: 1.9579038874689276....\n",
      "Validation loss: 2.010695098607781....\n",
      "-----------------------------------\n",
      "Training loss: 1.9568351087705078....\n",
      "Validation loss: 2.0097239217964993....\n",
      "-----------------------------------\n",
      "Training loss: 1.9557672051259423....\n",
      "Validation loss: 2.008754581122068....\n",
      "-----------------------------------\n",
      "Training loss: 1.95470062655077....\n",
      "Validation loss: 2.007785221672859....\n",
      "-----------------------------------\n",
      "Training loss: 1.9536342214093518....\n",
      "Validation loss: 2.0068174648381634....\n",
      "-----------------------------------\n",
      "Training loss: 1.9525694580360167....\n",
      "Validation loss: 2.0058504774451604....\n",
      "-----------------------------------\n",
      "Training loss: 1.951504756259934....\n",
      "Validation loss: 2.0048838589011524....\n",
      "-----------------------------------\n",
      "Training loss: 1.9504400810906568....\n",
      "Validation loss: 2.003917193031904....\n",
      "-----------------------------------\n",
      "Training loss: 1.9493747398766328....\n",
      "Validation loss: 2.002950457508527....\n",
      "-----------------------------------\n",
      "Training loss: 1.948307219864567....\n",
      "Validation loss: 2.001984462502968....\n",
      "-----------------------------------\n",
      "Training loss: 1.947239641826149....\n",
      "Validation loss: 2.0010189149389737....\n",
      "-----------------------------------\n",
      "Training loss: 1.9461713089433765....\n",
      "Validation loss: 2.000053605288104....\n",
      "-----------------------------------\n",
      "Training loss: 1.9451028801063512....\n",
      "Validation loss: 1.9990880914453397....\n",
      "-----------------------------------\n",
      "Training loss: 1.9440344615886562....\n",
      "Validation loss: 1.9981225690260827....\n",
      "-----------------------------------\n",
      "Training loss: 1.9429656643228503....\n",
      "Validation loss: 1.9971567278283568....\n",
      "-----------------------------------\n",
      "Training loss: 1.9418972937294112....\n",
      "Validation loss: 1.996190628608663....\n",
      "-----------------------------------\n",
      "Training loss: 1.9408282678610815....\n",
      "Validation loss: 1.9952249709042635....\n",
      "-----------------------------------\n",
      "Training loss: 1.939760089702325....\n",
      "Validation loss: 1.9942597374601687....\n",
      "-----------------------------------\n",
      "Training loss: 1.93869083023805....\n",
      "Validation loss: 1.9932935527553042....\n",
      "-----------------------------------\n",
      "Training loss: 1.9376206285130375....\n",
      "Validation loss: 1.9923269899467464....\n",
      "-----------------------------------\n",
      "Training loss: 1.9365503125301418....\n",
      "Validation loss: 1.9913609616431438....\n",
      "-----------------------------------\n",
      "Training loss: 1.9354805564134716....\n",
      "Validation loss: 1.990395994318596....\n",
      "-----------------------------------\n",
      "Training loss: 1.9344119552473165....\n",
      "Validation loss: 1.9894310634753343....\n",
      "-----------------------------------\n",
      "Training loss: 1.9333435022805778....\n",
      "Validation loss: 1.988465519422934....\n",
      "-----------------------------------\n",
      "Training loss: 1.9322746541498765....\n",
      "Validation loss: 1.9874991532925939....\n",
      "-----------------------------------\n",
      "Training loss: 1.931204922222982....\n",
      "Validation loss: 1.9865323791575482....\n",
      "-----------------------------------\n",
      "Training loss: 1.9301358128816934....\n",
      "Validation loss: 1.9855657440759569....\n",
      "-----------------------------------\n",
      "Training loss: 1.9290680297383842....\n",
      "Validation loss: 1.9845991227206197....\n",
      "-----------------------------------\n",
      "Training loss: 1.928000070907224....\n",
      "Validation loss: 1.983631650631967....\n",
      "-----------------------------------\n",
      "Training loss: 1.9269316467099984....\n",
      "Validation loss: 1.9826641468549737....\n",
      "-----------------------------------\n",
      "Training loss: 1.925862961866754....\n",
      "Validation loss: 1.981695404206472....\n",
      "-----------------------------------\n",
      "Training loss: 1.924792911116543....\n",
      "Validation loss: 1.9807264381680676....\n",
      "-----------------------------------\n",
      "Training loss: 1.9237221580055026....\n",
      "Validation loss: 1.9797567866665895....\n",
      "-----------------------------------\n",
      "Training loss: 1.9226499731107054....\n",
      "Validation loss: 1.978786854141257....\n",
      "-----------------------------------\n",
      "Training loss: 1.9215768209224757....\n",
      "Validation loss: 1.9778164226095565....\n",
      "-----------------------------------\n",
      "Training loss: 1.920504438863147....\n",
      "Validation loss: 1.9768480545219491....\n",
      "-----------------------------------\n",
      "Training loss: 1.9194329911801566....\n",
      "Validation loss: 1.9758799492828467....\n",
      "-----------------------------------\n",
      "Training loss: 1.918361344031763....\n",
      "Validation loss: 1.9749131245070803....\n",
      "-----------------------------------\n",
      "Training loss: 1.917290072967701....\n",
      "Validation loss: 1.9739462779949601....\n",
      "-----------------------------------\n",
      "Training loss: 1.916219894970802....\n",
      "Validation loss: 1.9729794557385205....\n",
      "-----------------------------------\n",
      "Training loss: 1.91514998526349....\n",
      "Validation loss: 1.9720128025714234....\n",
      "-----------------------------------\n",
      "Training loss: 1.9140802334146017....\n",
      "Validation loss: 1.9710464970223787....\n",
      "-----------------------------------\n",
      "Training loss: 1.9130111298764074....\n",
      "Validation loss: 1.9700797402742842....\n",
      "-----------------------------------\n",
      "Training loss: 1.91194181176661....\n",
      "Validation loss: 1.969112337947623....\n",
      "-----------------------------------\n",
      "Training loss: 1.9108729562974243....\n",
      "Validation loss: 1.968144404431388....\n",
      "-----------------------------------\n",
      "Training loss: 1.909805145579186....\n",
      "Validation loss: 1.967177345733443....\n",
      "-----------------------------------\n",
      "Training loss: 1.9087368863795249....\n",
      "Validation loss: 1.9662103628900427....\n",
      "-----------------------------------\n",
      "Training loss: 1.907668250647448....\n",
      "Validation loss: 1.9652432638895916....\n",
      "-----------------------------------\n",
      "Training loss: 1.9066002288545412....\n",
      "Validation loss: 1.9642742766246144....\n",
      "-----------------------------------\n",
      "Training loss: 1.905533113613403....\n",
      "Validation loss: 1.963307034696527....\n",
      "-----------------------------------\n",
      "Training loss: 1.9044675820537877....\n",
      "Validation loss: 1.9623403206102963....\n",
      "-----------------------------------\n",
      "Training loss: 1.903401585222007....\n",
      "Validation loss: 1.961373648142707....\n",
      "-----------------------------------\n",
      "Training loss: 1.902334931420641....\n",
      "Validation loss: 1.9604084161752795....\n",
      "-----------------------------------\n",
      "Training loss: 1.9012688564418456....\n",
      "Validation loss: 1.9594441560975329....\n",
      "-----------------------------------\n",
      "Training loss: 1.9002044428103604....\n",
      "Validation loss: 1.9584814493779823....\n",
      "-----------------------------------\n",
      "Training loss: 1.8991415068996897....\n",
      "Validation loss: 1.957518627871359....\n",
      "-----------------------------------\n",
      "Training loss: 1.8980785772008735....\n",
      "Validation loss: 1.9565560338597547....\n",
      "-----------------------------------\n",
      "Training loss: 1.8970157662024962....\n",
      "Validation loss: 1.9555926024347197....\n",
      "-----------------------------------\n",
      "Training loss: 1.8959521038181941....\n",
      "Validation loss: 1.954629412706452....\n",
      "-----------------------------------\n",
      "Training loss: 1.8948878011121941....\n",
      "Validation loss: 1.953665828546937....\n",
      "-----------------------------------\n",
      "Training loss: 1.8938229840557796....\n",
      "Validation loss: 1.9527016910111066....\n",
      "-----------------------------------\n",
      "Training loss: 1.892757858513618....\n",
      "Validation loss: 1.951736994703182....\n",
      "-----------------------------------\n",
      "Training loss: 1.8916924842294027....\n",
      "Validation loss: 1.9507714723385934....\n",
      "-----------------------------------\n",
      "Training loss: 1.890626202525674....\n",
      "Validation loss: 1.9498058876329376....\n",
      "-----------------------------------\n",
      "Training loss: 1.8895598365704587....\n",
      "Validation loss: 1.9488408786294487....\n",
      "-----------------------------------\n",
      "Training loss: 1.8884938220499252....\n",
      "Validation loss: 1.947876249624377....\n",
      "-----------------------------------\n",
      "Training loss: 1.887427782470572....\n",
      "Validation loss: 1.9469107475867564....\n",
      "-----------------------------------\n",
      "Training loss: 1.8863605522276978....\n",
      "Validation loss: 1.9459459902098952....\n",
      "-----------------------------------\n",
      "Training loss: 1.8852938195114601....\n",
      "Validation loss: 1.9449817019806237....\n",
      "-----------------------------------\n",
      "Training loss: 1.8842287205188377....\n",
      "Validation loss: 1.9440191267907039....\n",
      "-----------------------------------\n",
      "Training loss: 1.8831645219518898....\n",
      "Validation loss: 1.9430568844017964....\n",
      "-----------------------------------\n",
      "Training loss: 1.882100839066238....\n",
      "Validation loss: 1.9420955897337717....\n",
      "-----------------------------------\n",
      "Training loss: 1.8810377266224463....\n",
      "Validation loss: 1.94113333087702....\n",
      "-----------------------------------\n",
      "Training loss: 1.8799745370620033....\n",
      "Validation loss: 1.9401706141696045....\n",
      "-----------------------------------\n",
      "Training loss: 1.878911559021054....\n",
      "Validation loss: 1.9392078974188072....\n",
      "-----------------------------------\n",
      "Training loss: 1.8778489471478323....\n",
      "Validation loss: 1.9382447675301349....\n",
      "-----------------------------------\n",
      "Training loss: 1.8767862195435843....\n",
      "Validation loss: 1.937281293284606....\n",
      "-----------------------------------\n",
      "Training loss: 1.8757238278656372....\n",
      "Validation loss: 1.9363192952825738....\n",
      "-----------------------------------\n",
      "Training loss: 1.8746619111762104....\n",
      "Validation loss: 1.9353580908510053....\n",
      "-----------------------------------\n",
      "Training loss: 1.8736003248145403....\n",
      "Validation loss: 1.9343978202939998....\n",
      "-----------------------------------\n",
      "Training loss: 1.8725387034322942....\n",
      "Validation loss: 1.9334391964680095....\n",
      "-----------------------------------\n",
      "Training loss: 1.8714782986959415....\n",
      "Validation loss: 1.9324815829677446....\n",
      "-----------------------------------\n",
      "Training loss: 1.8704190878896574....\n",
      "Validation loss: 1.9315249044238263....\n",
      "-----------------------------------\n",
      "Training loss: 1.869360420766301....\n",
      "Validation loss: 1.930568869324827....\n",
      "-----------------------------------\n",
      "Training loss: 1.8683028860485853....\n",
      "Validation loss: 1.929612825953206....\n",
      "-----------------------------------\n",
      "Training loss: 1.867246238676552....\n",
      "Validation loss: 1.9286570671603491....\n",
      "-----------------------------------\n",
      "Training loss: 1.8661903032140517....\n",
      "Validation loss: 1.927702684017904....\n",
      "-----------------------------------\n",
      "Training loss: 1.865134862416345....\n",
      "Validation loss: 1.926749427501494....\n",
      "-----------------------------------\n",
      "Training loss: 1.864080388813586....\n",
      "Validation loss: 1.9257964744097649....\n",
      "-----------------------------------\n",
      "Training loss: 1.8630264137683528....\n",
      "Validation loss: 1.9248444434789413....\n",
      "-----------------------------------\n",
      "Training loss: 1.861973987068819....\n",
      "Validation loss: 1.923893122506674....\n",
      "-----------------------------------\n",
      "Training loss: 1.8609230770837892....\n",
      "Validation loss: 1.922941464040099....\n",
      "-----------------------------------\n",
      "Training loss: 1.8598732937961393....\n",
      "Validation loss: 1.9219898723890947....\n",
      "-----------------------------------\n",
      "Training loss: 1.8588233585749048....\n",
      "Validation loss: 1.921038333656367....\n",
      "-----------------------------------\n",
      "Training loss: 1.8577739722983915....\n",
      "Validation loss: 1.9200858094105167....\n",
      "-----------------------------------\n",
      "Training loss: 1.8567250019398367....\n",
      "Validation loss: 1.9191332915358457....\n",
      "-----------------------------------\n",
      "Training loss: 1.8556763950519335....\n",
      "Validation loss: 1.9181809634682312....\n",
      "-----------------------------------\n",
      "Training loss: 1.8546280448573214....\n",
      "Validation loss: 1.9172291372027221....\n",
      "-----------------------------------\n",
      "Training loss: 1.8535804416067798....\n",
      "Validation loss: 1.9162775327123451....\n",
      "-----------------------------------\n",
      "Training loss: 1.8525333904208092....\n",
      "Validation loss: 1.9153267239005263....\n",
      "-----------------------------------\n",
      "Training loss: 1.8514862427374073....\n",
      "Validation loss: 1.914376491276415....\n",
      "-----------------------------------\n",
      "Training loss: 1.8504394685375491....\n",
      "Validation loss: 1.9134274462509366....\n",
      "-----------------------------------\n",
      "Training loss: 1.849393227012354....\n",
      "Validation loss: 1.9124802102991012....\n",
      "-----------------------------------\n",
      "Training loss: 1.848348636293893....\n",
      "Validation loss: 1.911533670217766....\n",
      "-----------------------------------\n",
      "Training loss: 1.847304232230972....\n",
      "Validation loss: 1.9105870349730545....\n",
      "-----------------------------------\n",
      "Training loss: 1.8462610658858767....\n",
      "Validation loss: 1.9096410552971248....\n",
      "-----------------------------------\n",
      "Training loss: 1.8452183612490434....\n",
      "Validation loss: 1.9086953067769323....\n",
      "-----------------------------------\n",
      "Training loss: 1.8441758319029047....\n",
      "Validation loss: 1.9077498829519302....\n",
      "-----------------------------------\n",
      "Training loss: 1.843133262011901....\n",
      "Validation loss: 1.9068048805390523....\n",
      "-----------------------------------\n",
      "Training loss: 1.8420915689389743....\n",
      "Validation loss: 1.9058598876753041....\n",
      "-----------------------------------\n",
      "Training loss: 1.8410502734963046....\n",
      "Validation loss: 1.9049156649487626....\n",
      "-----------------------------------\n",
      "Training loss: 1.8400098826636477....\n",
      "Validation loss: 1.9039725425333536....\n",
      "-----------------------------------\n",
      "Training loss: 1.838970548892104....\n",
      "Validation loss: 1.9030306219442683....\n",
      "-----------------------------------\n",
      "Training loss: 1.8379313422986947....\n",
      "Validation loss: 1.902089330954475....\n",
      "-----------------------------------\n",
      "Training loss: 1.8368920470281265....\n",
      "Validation loss: 1.9011480235089508....\n",
      "-----------------------------------\n",
      "Training loss: 1.8358532052948344....\n",
      "Validation loss: 1.9002074250933072....\n",
      "-----------------------------------\n",
      "Training loss: 1.8348152456213078....\n",
      "Validation loss: 1.8992667770005036....\n",
      "-----------------------------------\n",
      "Training loss: 1.8337774719883555....\n",
      "Validation loss: 1.8983260871838357....\n",
      "-----------------------------------\n",
      "Training loss: 1.8327405515304624....\n",
      "Validation loss: 1.8973851478461663....\n",
      "-----------------------------------\n",
      "Training loss: 1.8317034918634358....\n",
      "Validation loss: 1.8964437124569415....\n",
      "-----------------------------------\n",
      "Training loss: 1.83066507051703....\n",
      "Validation loss: 1.8955030637333254....\n",
      "-----------------------------------\n",
      "Training loss: 1.829627312333598....\n",
      "Validation loss: 1.89456212090109....\n",
      "-----------------------------------\n",
      "Training loss: 1.8285894184413598....\n",
      "Validation loss: 1.893621538525316....\n",
      "-----------------------------------\n",
      "Training loss: 1.827551114507676....\n",
      "Validation loss: 1.8926817127294377....\n",
      "-----------------------------------\n",
      "Training loss: 1.8265136863870508....\n",
      "Validation loss: 1.8917422475227703....\n",
      "-----------------------------------\n",
      "Training loss: 1.8254767350791574....\n",
      "Validation loss: 1.8908033010062668....\n",
      "-----------------------------------\n",
      "Training loss: 1.8244400482010938....\n",
      "Validation loss: 1.8898646158747425....\n",
      "-----------------------------------\n",
      "Training loss: 1.8234029237056228....\n",
      "Validation loss: 1.8889250234950732....\n",
      "-----------------------------------\n",
      "Training loss: 1.8223653298873728....\n",
      "Validation loss: 1.8879856983686751....\n",
      "-----------------------------------\n",
      "Training loss: 1.8213284545955706....\n",
      "Validation loss: 1.8870464949897146....\n",
      "-----------------------------------\n",
      "Training loss: 1.82029234672356....\n",
      "Validation loss: 1.8861066710906313....\n",
      "-----------------------------------\n",
      "Training loss: 1.8192563681697789....\n",
      "Validation loss: 1.8851668829540449....\n",
      "-----------------------------------\n",
      "Training loss: 1.8182195576516107....\n",
      "Validation loss: 1.8842262472059035....\n",
      "-----------------------------------\n",
      "Training loss: 1.817182055663573....\n",
      "Validation loss: 1.883286210591145....\n",
      "-----------------------------------\n",
      "Training loss: 1.8161451940938427....\n",
      "Validation loss: 1.8823467763162502....\n",
      "-----------------------------------\n",
      "Training loss: 1.815108496794622....\n",
      "Validation loss: 1.8814081072520126....\n",
      "-----------------------------------\n",
      "Training loss: 1.8140717093505199....\n",
      "Validation loss: 1.8804696252093063....\n",
      "-----------------------------------\n",
      "Training loss: 1.813035133644625....\n",
      "Validation loss: 1.8795317279309467....\n",
      "-----------------------------------\n",
      "Training loss: 1.811998710517541....\n",
      "Validation loss: 1.8785945684788798....\n",
      "-----------------------------------\n",
      "Training loss: 1.8109622496327342....\n",
      "Validation loss: 1.8776568597409513....\n",
      "-----------------------------------\n",
      "Training loss: 1.8099266957596412....\n",
      "Validation loss: 1.8767205588916742....\n",
      "-----------------------------------\n",
      "Training loss: 1.8088932542284744....\n",
      "Validation loss: 1.8757855067010314....\n",
      "-----------------------------------\n",
      "Training loss: 1.80786027915249....\n",
      "Validation loss: 1.874851260601267....\n",
      "-----------------------------------\n",
      "Training loss: 1.8068273931689285....\n",
      "Validation loss: 1.8739174973093413....\n",
      "-----------------------------------\n",
      "Training loss: 1.8057949535125097....\n",
      "Validation loss: 1.8729832790673335....\n",
      "-----------------------------------\n",
      "Training loss: 1.804762414239471....\n",
      "Validation loss: 1.872050051720166....\n",
      "-----------------------------------\n",
      "Training loss: 1.803730520132427....\n",
      "Validation loss: 1.8711182635768264....\n",
      "-----------------------------------\n",
      "Training loss: 1.802699562356081....\n",
      "Validation loss: 1.8701871051582624....\n",
      "-----------------------------------\n",
      "Training loss: 1.801667790870881....\n",
      "Validation loss: 1.8692559473004573....\n",
      "-----------------------------------\n",
      "Training loss: 1.8006359555039495....\n",
      "Validation loss: 1.868324662208708....\n",
      "-----------------------------------\n",
      "Training loss: 1.7996039467350218....\n",
      "Validation loss: 1.8673938452279955....\n",
      "-----------------------------------\n",
      "Training loss: 1.7985728078748915....\n",
      "Validation loss: 1.866464199818829....\n",
      "-----------------------------------\n",
      "Training loss: 1.7975420287598023....\n",
      "Validation loss: 1.865535013330232....\n",
      "-----------------------------------\n",
      "Training loss: 1.7965110272809983....\n",
      "Validation loss: 1.8646066355860287....\n",
      "-----------------------------------\n",
      "Training loss: 1.7954810857921584....\n",
      "Validation loss: 1.8636792400971587....\n",
      "-----------------------------------\n",
      "Training loss: 1.7944521951885448....\n",
      "Validation loss: 1.8627525231359623....\n",
      "-----------------------------------\n",
      "Training loss: 1.7934230493571208....\n",
      "Validation loss: 1.8618261244315073....\n",
      "-----------------------------------\n",
      "Training loss: 1.7923944928272149....\n",
      "Validation loss: 1.8609003076063166....\n",
      "-----------------------------------\n",
      "Training loss: 1.791366091137765....\n",
      "Validation loss: 1.8599752161683036....\n",
      "-----------------------------------\n",
      "Training loss: 1.7903378160498393....\n",
      "Validation loss: 1.859049900232477....\n",
      "-----------------------------------\n",
      "Training loss: 1.7893090879359497....\n",
      "Validation loss: 1.8581257675960376....\n",
      "-----------------------------------\n",
      "Training loss: 1.788280495849042....\n",
      "Validation loss: 1.8572026292232005....\n",
      "-----------------------------------\n",
      "Training loss: 1.7872515971942884....\n",
      "Validation loss: 1.8562804000373898....\n",
      "-----------------------------------\n",
      "Training loss: 1.7862228575302788....\n",
      "Validation loss: 1.8553588925562576....\n",
      "-----------------------------------\n",
      "Training loss: 1.7851951368438153....\n",
      "Validation loss: 1.8544370851332215....\n",
      "-----------------------------------\n",
      "Training loss: 1.7841686096091915....\n",
      "Validation loss: 1.8535150737221147....\n",
      "-----------------------------------\n",
      "Training loss: 1.7831426071420562....\n",
      "Validation loss: 1.8525936737185966....\n",
      "-----------------------------------\n",
      "Training loss: 1.7821169661377119....\n",
      "Validation loss: 1.8516731506578474....\n",
      "-----------------------------------\n",
      "Training loss: 1.7810919144551085....\n",
      "Validation loss: 1.8507530124461662....\n",
      "-----------------------------------\n",
      "Training loss: 1.7800669966948366....\n",
      "Validation loss: 1.8498330882248797....\n",
      "-----------------------------------\n",
      "Training loss: 1.779042903132899....\n",
      "Validation loss: 1.8489135444695939....\n",
      "-----------------------------------\n",
      "Training loss: 1.778020404015122....\n",
      "Validation loss: 1.8479950949613846....\n",
      "-----------------------------------\n",
      "Training loss: 1.7769989957034962....\n",
      "Validation loss: 1.8470782045031484....\n",
      "-----------------------------------\n",
      "Training loss: 1.7759785883702797....\n",
      "Validation loss: 1.8461609320827455....\n",
      "-----------------------------------\n",
      "Training loss: 1.7749583181800688....\n",
      "Validation loss: 1.8452437979397782....\n",
      "-----------------------------------\n",
      "Training loss: 1.7739386422372598....\n",
      "Validation loss: 1.8443274270693724....\n",
      "-----------------------------------\n",
      "Training loss: 1.7729197319427121....\n",
      "Validation loss: 1.8434113923317585....\n",
      "-----------------------------------\n",
      "Training loss: 1.7719007482261442....\n",
      "Validation loss: 1.8424954994643068....\n",
      "-----------------------------------\n",
      "Training loss: 1.770882304036453....\n",
      "Validation loss: 1.8415796451171065....\n",
      "-----------------------------------\n",
      "Training loss: 1.7698653168183045....\n",
      "Validation loss: 1.8406637063221312....\n",
      "-----------------------------------\n",
      "Training loss: 1.768848348770565....\n",
      "Validation loss: 1.8397486630334834....\n",
      "-----------------------------------\n",
      "Training loss: 1.7678319226111414....\n",
      "Validation loss: 1.8388344465183812....\n",
      "-----------------------------------\n",
      "Training loss: 1.7668163697390722....\n",
      "Validation loss: 1.8379211742819659....\n",
      "-----------------------------------\n",
      "Training loss: 1.7658014360145258....\n",
      "Validation loss: 1.8370078989714202....\n",
      "-----------------------------------\n",
      "Training loss: 1.7647860706081953....\n",
      "Validation loss: 1.836094993587472....\n",
      "-----------------------------------\n",
      "Training loss: 1.763770949199866....\n",
      "Validation loss: 1.835182709684389....\n",
      "-----------------------------------\n",
      "Training loss: 1.7627560552491672....\n",
      "Validation loss: 1.834270866919092....\n",
      "-----------------------------------\n",
      "Training loss: 1.761742189625529....\n",
      "Validation loss: 1.8333599213094427....\n",
      "-----------------------------------\n",
      "Training loss: 1.7607299501745923....\n",
      "Validation loss: 1.8324493364808896....\n",
      "-----------------------------------\n",
      "Training loss: 1.7597192544852873....\n",
      "Validation loss: 1.8315396947724627....\n",
      "-----------------------------------\n",
      "Training loss: 1.7587103645860498....\n",
      "Validation loss: 1.830629622570729....\n",
      "-----------------------------------\n",
      "Training loss: 1.7577011749662264....\n",
      "Validation loss: 1.829718952271925....\n",
      "-----------------------------------\n",
      "Training loss: 1.756692550664745....\n",
      "Validation loss: 1.8288095562776914....\n",
      "-----------------------------------\n",
      "Training loss: 1.7556852545158075....\n",
      "Validation loss: 1.8279008247102826....\n",
      "-----------------------------------\n",
      "Training loss: 1.7546785271582461....\n",
      "Validation loss: 1.8269919628335758....\n",
      "-----------------------------------\n",
      "Training loss: 1.7536717848454195....\n",
      "Validation loss: 1.826083127447566....\n",
      "-----------------------------------\n",
      "Training loss: 1.7526645147817752....\n",
      "Validation loss: 1.8251751029778152....\n",
      "-----------------------------------\n",
      "Training loss: 1.7516576097640257....\n",
      "Validation loss: 1.8242670708573292....\n",
      "-----------------------------------\n",
      "Training loss: 1.7506513754231843....\n",
      "Validation loss: 1.8233590738165326....\n",
      "-----------------------------------\n",
      "Training loss: 1.7496450707854536....\n",
      "Validation loss: 1.8224517676991578....\n",
      "-----------------------------------\n",
      "Training loss: 1.7486393668103017....\n",
      "Validation loss: 1.8215460822108693....\n",
      "-----------------------------------\n",
      "Training loss: 1.7476353462648202....\n",
      "Validation loss: 1.8206410592112452....\n",
      "-----------------------------------\n",
      "Training loss: 1.7466322059554213....\n",
      "Validation loss: 1.8197369111608321....\n",
      "-----------------------------------\n",
      "Training loss: 1.7456297964281018....\n",
      "Validation loss: 1.818834626926725....\n",
      "-----------------------------------\n",
      "Training loss: 1.7446279509476526....\n",
      "Validation loss: 1.817933996354431....\n",
      "-----------------------------------\n",
      "Training loss: 1.7436270614078189....\n",
      "Validation loss: 1.8170341479445447....\n",
      "-----------------------------------\n",
      "Training loss: 1.7426264930186393....\n",
      "Validation loss: 1.8161347786715212....\n",
      "-----------------------------------\n",
      "Training loss: 1.741626971187329....\n",
      "Validation loss: 1.815235754418735....\n",
      "-----------------------------------\n",
      "Training loss: 1.7406288281845264....\n",
      "Validation loss: 1.8143370487743904....\n",
      "-----------------------------------\n",
      "Training loss: 1.739632030954606....\n",
      "Validation loss: 1.813439115092615....\n",
      "-----------------------------------\n",
      "Training loss: 1.7386358317139403....\n",
      "Validation loss: 1.8125414029693359....\n",
      "-----------------------------------\n",
      "Training loss: 1.737639381874701....\n",
      "Validation loss: 1.8116439616168436....\n",
      "-----------------------------------\n",
      "Training loss: 1.7366435994073506....\n",
      "Validation loss: 1.8107463468790184....\n",
      "-----------------------------------\n",
      "Training loss: 1.7356479775569846....\n",
      "Validation loss: 1.8098507428758217....\n",
      "-----------------------------------\n",
      "Training loss: 1.7346533153465353....\n",
      "Validation loss: 1.8089555093817111....\n",
      "-----------------------------------\n",
      "Training loss: 1.7336593313796147....\n",
      "Validation loss: 1.8080609141661834....\n",
      "-----------------------------------\n",
      "Training loss: 1.7326658676014308....\n",
      "Validation loss: 1.8071673559920407....\n",
      "-----------------------------------\n",
      "Training loss: 1.7316736182946166....\n",
      "Validation loss: 1.8062739830611059....\n",
      "-----------------------------------\n",
      "Training loss: 1.7306821586579566....\n",
      "Validation loss: 1.8053814540415019....\n",
      "-----------------------------------\n",
      "Training loss: 1.729691720572772....\n",
      "Validation loss: 1.8044894868486183....\n",
      "-----------------------------------\n",
      "Training loss: 1.7287024837747982....\n",
      "Validation loss: 1.8035988378069587....\n",
      "-----------------------------------\n",
      "Training loss: 1.727714398691317....\n",
      "Validation loss: 1.802707869939276....\n",
      "-----------------------------------\n",
      "Training loss: 1.7267265035545556....\n",
      "Validation loss: 1.8018176669906623....\n",
      "-----------------------------------\n",
      "Training loss: 1.7257403420990756....\n",
      "Validation loss: 1.8009278305428615....\n",
      "-----------------------------------\n",
      "Training loss: 1.724754665625245....\n",
      "Validation loss: 1.8000388219482353....\n",
      "-----------------------------------\n",
      "Training loss: 1.7237699532372706....\n",
      "Validation loss: 1.7991521140378732....\n",
      "-----------------------------------\n",
      "Training loss: 1.7227866549739548....\n",
      "Validation loss: 1.7982658010214745....\n",
      "-----------------------------------\n",
      "Training loss: 1.7218032410516995....\n",
      "Validation loss: 1.797379403594132....\n",
      "-----------------------------------\n",
      "Training loss: 1.720820153029965....\n",
      "Validation loss: 1.7964947369217563....\n",
      "-----------------------------------\n",
      "Training loss: 1.719838031084212....\n",
      "Validation loss: 1.795611033510199....\n",
      "-----------------------------------\n",
      "Training loss: 1.718855806299816....\n",
      "Validation loss: 1.7947279026045566....\n",
      "-----------------------------------\n",
      "Training loss: 1.717873138399176....\n",
      "Validation loss: 1.793845358154387....\n",
      "-----------------------------------\n",
      "Training loss: 1.7168913694426453....\n",
      "Validation loss: 1.7929626698404781....\n",
      "-----------------------------------\n",
      "Training loss: 1.7159099524520498....\n",
      "Validation loss: 1.7920798340475772....\n",
      "-----------------------------------\n",
      "Training loss: 1.7149278916920765....\n",
      "Validation loss: 1.791197827454077....\n",
      "-----------------------------------\n",
      "Training loss: 1.7139469881881728....\n",
      "Validation loss: 1.7903174412949774....\n",
      "-----------------------------------\n",
      "Training loss: 1.7129674207456842....\n",
      "Validation loss: 1.7894375907168378....\n",
      "-----------------------------------\n",
      "Training loss: 1.71198890498151....\n",
      "Validation loss: 1.7885587035636428....\n",
      "-----------------------------------\n",
      "Training loss: 1.7110114699144883....\n",
      "Validation loss: 1.787680546612946....\n",
      "-----------------------------------\n",
      "Training loss: 1.7100346669813198....\n",
      "Validation loss: 1.786803673545373....\n",
      "-----------------------------------\n",
      "Training loss: 1.7090591650095412....\n",
      "Validation loss: 1.7859271794936153....\n",
      "-----------------------------------\n",
      "Training loss: 1.708083339518438....\n",
      "Validation loss: 1.7850507562700424....\n",
      "-----------------------------------\n",
      "Training loss: 1.7071087058481713....\n",
      "Validation loss: 1.7841751857569734....\n",
      "-----------------------------------\n",
      "Training loss: 1.7061348067091788....\n",
      "Validation loss: 1.7832998778797666....\n",
      "-----------------------------------\n",
      "Training loss: 1.7051615298314144....\n",
      "Validation loss: 1.7824263343161095....\n",
      "-----------------------------------\n",
      "Training loss: 1.7041895942699987....\n",
      "Validation loss: 1.7815531430859477....\n",
      "-----------------------------------\n",
      "Training loss: 1.7032177872157515....\n",
      "Validation loss: 1.7806812214956513....\n",
      "-----------------------------------\n",
      "Training loss: 1.7022471490154858....\n",
      "Validation loss: 1.7798098725409819....\n",
      "-----------------------------------\n",
      "Training loss: 1.701277482982142....\n",
      "Validation loss: 1.7789384924391625....\n",
      "-----------------------------------\n",
      "Training loss: 1.7003079185952592....\n",
      "Validation loss: 1.7780669862457203....\n",
      "-----------------------------------\n",
      "Training loss: 1.699338221427076....\n",
      "Validation loss: 1.7771961882139105....\n",
      "-----------------------------------\n",
      "Training loss: 1.6983687854550122....\n",
      "Validation loss: 1.7763265287868049....\n",
      "-----------------------------------\n",
      "Training loss: 1.6974005171288615....\n",
      "Validation loss: 1.7754571605161236....\n",
      "-----------------------------------\n",
      "Training loss: 1.6964331516513176....\n",
      "Validation loss: 1.7745887738325532....\n",
      "-----------------------------------\n",
      "Training loss: 1.6954663211012648....\n",
      "Validation loss: 1.7737218990993386....\n",
      "-----------------------------------\n",
      "Training loss: 1.6945004328009945....\n",
      "Validation loss: 1.7728564438892034....\n",
      "-----------------------------------\n",
      "Training loss: 1.693534919804802....\n",
      "Validation loss: 1.7719913051625813....\n",
      "-----------------------------------\n",
      "Training loss: 1.692569948799125....\n",
      "Validation loss: 1.7711270713807994....\n",
      "-----------------------------------\n",
      "Training loss: 1.6916060027860442....\n",
      "Validation loss: 1.7702631667185116....\n",
      "-----------------------------------\n",
      "Training loss: 1.690642357286466....\n",
      "Validation loss: 1.7694004043113614....\n",
      "-----------------------------------\n",
      "Training loss: 1.689679675205537....\n",
      "Validation loss: 1.7685383364500868....\n",
      "-----------------------------------\n",
      "Training loss: 1.6887166923869956....\n",
      "Validation loss: 1.7676765484077965....\n",
      "-----------------------------------\n",
      "Training loss: 1.687754358873503....\n",
      "Validation loss: 1.7668145058431552....\n",
      "-----------------------------------\n",
      "Training loss: 1.6867924396742082....\n",
      "Validation loss: 1.7659533568780743....\n",
      "-----------------------------------\n",
      "Training loss: 1.6858312832484679....\n",
      "Validation loss: 1.7650926276674321....\n",
      "-----------------------------------\n",
      "Training loss: 1.6848709188852204....\n",
      "Validation loss: 1.7642325340271152....\n",
      "-----------------------------------\n",
      "Training loss: 1.683911968738293....\n",
      "Validation loss: 1.7633724799853148....\n",
      "-----------------------------------\n",
      "Training loss: 1.6829530669474446....\n",
      "Validation loss: 1.762512773649299....\n",
      "-----------------------------------\n",
      "Training loss: 1.6819948764439796....\n",
      "Validation loss: 1.7616528588681357....\n",
      "-----------------------------------\n",
      "Training loss: 1.6810375318506892....\n",
      "Validation loss: 1.760793739149942....\n",
      "-----------------------------------\n",
      "Training loss: 1.6800809783397919....\n",
      "Validation loss: 1.7599351195751478....\n",
      "-----------------------------------\n",
      "Training loss: 1.6791250887007079....\n",
      "Validation loss: 1.7590766206025812....\n",
      "-----------------------------------\n",
      "Training loss: 1.6781695918365875....\n",
      "Validation loss: 1.75821862554807....\n",
      "-----------------------------------\n",
      "Training loss: 1.6772145995627012....\n",
      "Validation loss: 1.7573617482045236....\n",
      "-----------------------------------\n",
      "Training loss: 1.6762607854338571....\n",
      "Validation loss: 1.756504862738121....\n",
      "-----------------------------------\n",
      "Training loss: 1.675307329107283....\n",
      "Validation loss: 1.7556487660752302....\n",
      "-----------------------------------\n",
      "Training loss: 1.6743543169443256....\n",
      "Validation loss: 1.754793080600542....\n",
      "-----------------------------------\n",
      "Training loss: 1.6734014114582743....\n",
      "Validation loss: 1.7539381431874466....\n",
      "-----------------------------------\n",
      "Training loss: 1.672449209488207....\n",
      "Validation loss: 1.7530841583643346....\n",
      "-----------------------------------\n",
      "Training loss: 1.671497563610524....\n",
      "Validation loss: 1.7522306798760896....\n",
      "-----------------------------------\n",
      "Training loss: 1.6705463603166366....\n",
      "Validation loss: 1.751377726380992....\n",
      "-----------------------------------\n",
      "Training loss: 1.6695962834141924....\n",
      "Validation loss: 1.7505259249388951....\n",
      "-----------------------------------\n",
      "Training loss: 1.668647063951995....\n",
      "Validation loss: 1.7496752530136253....\n",
      "-----------------------------------\n",
      "Training loss: 1.6676986854871054....\n",
      "Validation loss: 1.7488258597975674....\n",
      "-----------------------------------\n",
      "Training loss: 1.666750872145394....\n",
      "Validation loss: 1.7479778028738997....\n",
      "-----------------------------------\n",
      "Training loss: 1.6658037324108292....\n",
      "Validation loss: 1.7471304939809393....\n",
      "-----------------------------------\n",
      "Training loss: 1.6648574308617115....\n",
      "Validation loss: 1.7462842120192819....\n",
      "-----------------------------------\n",
      "Training loss: 1.6639121577360925....\n",
      "Validation loss: 1.7454390761509817....\n",
      "-----------------------------------\n",
      "Training loss: 1.6629684653671932....\n",
      "Validation loss: 1.74459551057892....\n",
      "-----------------------------------\n",
      "Training loss: 1.662026166460574....\n",
      "Validation loss: 1.743751896339154....\n",
      "-----------------------------------\n",
      "Training loss: 1.6610837474907454....\n",
      "Validation loss: 1.7429094017151985....\n",
      "-----------------------------------\n",
      "Training loss: 1.66014197729242....\n",
      "Validation loss: 1.7420680257728292....\n",
      "-----------------------------------\n",
      "Training loss: 1.6592010496157117....\n",
      "Validation loss: 1.7412271786167353....\n",
      "-----------------------------------\n",
      "Training loss: 1.658260132353155....\n",
      "Validation loss: 1.740386691605449....\n",
      "-----------------------------------\n",
      "Training loss: 1.657318921405098....\n",
      "Validation loss: 1.739547280916407....\n",
      "-----------------------------------\n",
      "Training loss: 1.6563781910852389....\n",
      "Validation loss: 1.7387083690775098....\n",
      "-----------------------------------\n",
      "Training loss: 1.6554386119629776....\n",
      "Validation loss: 1.7378707436863203....\n",
      "-----------------------------------\n",
      "Training loss: 1.654500086280129....\n",
      "Validation loss: 1.7370330561513907....\n",
      "-----------------------------------\n",
      "Training loss: 1.6535618568107506....\n",
      "Validation loss: 1.7361964662218268....\n",
      "-----------------------------------\n",
      "Training loss: 1.6526241449170593....\n",
      "Validation loss: 1.7353602608160972....\n",
      "-----------------------------------\n",
      "Training loss: 1.6516870181529026....\n",
      "Validation loss: 1.7345248287956478....\n",
      "-----------------------------------\n",
      "Training loss: 1.6507505725243448....\n",
      "Validation loss: 1.733690307857938....\n",
      "-----------------------------------\n",
      "Training loss: 1.6498147940782553....\n",
      "Validation loss: 1.7328570324650188....\n",
      "-----------------------------------\n",
      "Training loss: 1.6488793604613052....\n",
      "Validation loss: 1.7320234059481574....\n",
      "-----------------------------------\n",
      "Training loss: 1.6479434648891003....\n",
      "Validation loss: 1.7311904654204995....\n",
      "-----------------------------------\n",
      "Training loss: 1.6470082287310754....\n",
      "Validation loss: 1.7303579121567398....\n",
      "-----------------------------------\n",
      "Training loss: 1.646073563111681....\n",
      "Validation loss: 1.729525709527445....\n",
      "-----------------------------------\n",
      "Training loss: 1.6451393397596061....\n",
      "Validation loss: 1.7286943617241997....\n",
      "-----------------------------------\n",
      "Training loss: 1.6442055004747254....\n",
      "Validation loss: 1.7278632624102712....\n",
      "-----------------------------------\n",
      "Training loss: 1.6432718216761826....\n",
      "Validation loss: 1.727032669981856....\n",
      "-----------------------------------\n",
      "Training loss: 1.6423387808967824....\n",
      "Validation loss: 1.7262031726286506....\n",
      "-----------------------------------\n",
      "Training loss: 1.6414065535338247....\n",
      "Validation loss: 1.7253747012024765....\n",
      "-----------------------------------\n",
      "Training loss: 1.6404749482389802....\n",
      "Validation loss: 1.7245468516606572....\n",
      "-----------------------------------\n",
      "Training loss: 1.639544544225204....\n",
      "Validation loss: 1.7237198852020275....\n",
      "-----------------------------------\n",
      "Training loss: 1.6386144620674168....\n",
      "Validation loss: 1.7228936441766447....\n",
      "-----------------------------------\n",
      "Training loss: 1.637685295949225....\n",
      "Validation loss: 1.72206769145364....\n",
      "-----------------------------------\n",
      "Training loss: 1.636756440145621....\n",
      "Validation loss: 1.7212427105408037....\n",
      "-----------------------------------\n",
      "Training loss: 1.6358286214522355....\n",
      "Validation loss: 1.720418405811448....\n",
      "-----------------------------------\n",
      "Training loss: 1.6349017251285265....\n",
      "Validation loss: 1.7195945991143555....\n",
      "-----------------------------------\n",
      "Training loss: 1.633975479312447....\n",
      "Validation loss: 1.71877126325571....\n",
      "-----------------------------------\n",
      "Training loss: 1.6330504179623546....\n",
      "Validation loss: 1.7179489223219495....\n",
      "-----------------------------------\n",
      "Training loss: 1.6321264605972043....\n",
      "Validation loss: 1.7171276801566386....\n",
      "-----------------------------------\n",
      "Training loss: 1.6312035311159665....\n",
      "Validation loss: 1.716307328124443....\n",
      "-----------------------------------\n",
      "Training loss: 1.6302819686256018....\n",
      "Validation loss: 1.7154880314627026....\n",
      "-----------------------------------\n",
      "Training loss: 1.6293610558463039....\n",
      "Validation loss: 1.7146693017011898....\n",
      "-----------------------------------\n",
      "Training loss: 1.628440672013439....\n",
      "Validation loss: 1.7138512911187582....\n",
      "-----------------------------------\n",
      "Training loss: 1.627520327821374....\n",
      "Validation loss: 1.7130331923508875....\n",
      "-----------------------------------\n",
      "Training loss: 1.6265997347155812....\n",
      "Validation loss: 1.712216421281426....\n",
      "-----------------------------------\n",
      "Training loss: 1.6256799978215897....\n",
      "Validation loss: 1.711400724724717....\n",
      "-----------------------------------\n",
      "Training loss: 1.6247607917800737....\n",
      "Validation loss: 1.710585813865575....\n",
      "-----------------------------------\n",
      "Training loss: 1.6238416237675894....\n",
      "Validation loss: 1.7097711604268189....\n",
      "-----------------------------------\n",
      "Training loss: 1.622923076367442....\n",
      "Validation loss: 1.7089572286611132....\n",
      "-----------------------------------\n",
      "Training loss: 1.6220048315530111....\n",
      "Validation loss: 1.7081436601988205....\n",
      "-----------------------------------\n",
      "Training loss: 1.621086967785384....\n",
      "Validation loss: 1.707330821825067....\n",
      "-----------------------------------\n",
      "Training loss: 1.6201699617969372....\n",
      "Validation loss: 1.7065194150392533....\n",
      "-----------------------------------\n",
      "Training loss: 1.6192536692512038....\n",
      "Validation loss: 1.7057087134266598....\n",
      "-----------------------------------\n",
      "Training loss: 1.6183380078850187....\n",
      "Validation loss: 1.7048984980985544....\n",
      "-----------------------------------\n",
      "Training loss: 1.617423086435597....\n",
      "Validation loss: 1.704089967435835....\n",
      "-----------------------------------\n",
      "Training loss: 1.6165092198328996....\n",
      "Validation loss: 1.7032829712298025....\n",
      "-----------------------------------\n",
      "Training loss: 1.6155964193587247....\n",
      "Validation loss: 1.7024770271614973....\n",
      "-----------------------------------\n",
      "Training loss: 1.6146853335674791....\n",
      "Validation loss: 1.7016717507090449....\n",
      "-----------------------------------\n",
      "Training loss: 1.6137751390776582....\n",
      "Validation loss: 1.7008679162519247....\n",
      "-----------------------------------\n",
      "Training loss: 1.612866102222631....\n",
      "Validation loss: 1.700064493403233....\n",
      "-----------------------------------\n",
      "Training loss: 1.6119572130111353....\n",
      "Validation loss: 1.699262222015058....\n",
      "-----------------------------------\n",
      "Training loss: 1.6110489784177418....\n",
      "Validation loss: 1.6984608395785379....\n",
      "-----------------------------------\n",
      "Training loss: 1.610141505382498....\n",
      "Validation loss: 1.6976601356224323....\n",
      "-----------------------------------\n",
      "Training loss: 1.6092347812062737....\n",
      "Validation loss: 1.6968594398886168....\n",
      "-----------------------------------\n",
      "Training loss: 1.608328630503236....\n",
      "Validation loss: 1.6960595954951037....\n",
      "-----------------------------------\n",
      "Training loss: 1.6074223752686463....\n",
      "Validation loss: 1.6952611794585524....\n",
      "-----------------------------------\n",
      "Training loss: 1.6065164035065767....\n",
      "Validation loss: 1.6944640796529133....\n",
      "-----------------------------------\n",
      "Training loss: 1.6056115306689374....\n",
      "Validation loss: 1.69366710472625....\n",
      "-----------------------------------\n",
      "Training loss: 1.6047077035013826....\n",
      "Validation loss: 1.6928708698046055....\n",
      "-----------------------------------\n",
      "Training loss: 1.6038046144792613....\n",
      "Validation loss: 1.6920740491681063....\n",
      "-----------------------------------\n",
      "Training loss: 1.602901505226468....\n",
      "Validation loss: 1.691277836956454....\n",
      "-----------------------------------\n",
      "Training loss: 1.6445380905865585....\n",
      "Validation loss: 1.6054687978830247....\n",
      "-----------------------------------\n",
      "Training loss: 1.6435674922601815....\n",
      "Validation loss: 1.6047157963491845....\n",
      "-----------------------------------\n",
      "Training loss: 1.6425976418614885....\n",
      "Validation loss: 1.6039624356481186....\n",
      "-----------------------------------\n",
      "Training loss: 1.6416287581244993....\n",
      "Validation loss: 1.603208808163394....\n",
      "-----------------------------------\n",
      "Training loss: 1.640662437437394....\n",
      "Validation loss: 1.602455393374149....\n",
      "-----------------------------------\n",
      "Training loss: 1.6396981741193999....\n",
      "Validation loss: 1.6017020960697552....\n",
      "-----------------------------------\n",
      "Training loss: 1.6387349378445133....\n",
      "Validation loss: 1.600949358052882....\n",
      "-----------------------------------\n",
      "Training loss: 1.6377732270059797....\n",
      "Validation loss: 1.6001968706364957....\n",
      "-----------------------------------\n",
      "Training loss: 1.63681353699518....\n",
      "Validation loss: 1.599444660012577....\n",
      "-----------------------------------\n",
      "Training loss: 1.6358550108888137....\n",
      "Validation loss: 1.5986930006833435....\n",
      "-----------------------------------\n",
      "Training loss: 1.6348976444136916....\n",
      "Validation loss: 1.5979411542420767....\n",
      "-----------------------------------\n",
      "Training loss: 1.6339405594854735....\n",
      "Validation loss: 1.5971893246415723....\n",
      "-----------------------------------\n",
      "Training loss: 1.6329849771267757....\n",
      "Validation loss: 1.596437211566134....\n",
      "-----------------------------------\n",
      "Training loss: 1.6320296636023386....\n",
      "Validation loss: 1.5956850569101368....\n",
      "-----------------------------------\n",
      "Training loss: 1.631075188557987....\n",
      "Validation loss: 1.594933668911098....\n",
      "-----------------------------------\n",
      "Training loss: 1.6301221997981747....\n",
      "Validation loss: 1.5941835130106006....\n",
      "-----------------------------------\n",
      "Training loss: 1.629170743795433....\n",
      "Validation loss: 1.5934336735873935....\n",
      "-----------------------------------\n",
      "Training loss: 1.6282197744920561....\n",
      "Validation loss: 1.5926849296257264....\n",
      "-----------------------------------\n",
      "Training loss: 1.6272689192787333....\n",
      "Validation loss: 1.5919369736279245....\n",
      "-----------------------------------\n",
      "Training loss: 1.6263182757687156....\n",
      "Validation loss: 1.5911899348467264....\n",
      "-----------------------------------\n",
      "Training loss: 1.6253689623707364....\n",
      "Validation loss: 1.590443307716759....\n",
      "-----------------------------------\n",
      "Training loss: 1.6244196642880682....\n",
      "Validation loss: 1.5896971821730568....\n",
      "-----------------------------------\n",
      "Training loss: 1.623470517645013....\n",
      "Validation loss: 1.5889512281117473....\n",
      "-----------------------------------\n",
      "Training loss: 1.6225215843004233....\n",
      "Validation loss: 1.5882058705505426....\n",
      "-----------------------------------\n",
      "Training loss: 1.6215729106990353....\n",
      "Validation loss: 1.58746087460329....\n",
      "-----------------------------------\n",
      "Training loss: 1.6206249986649741....\n",
      "Validation loss: 1.5867166347179427....\n",
      "-----------------------------------\n",
      "Training loss: 1.6196779361139015....\n",
      "Validation loss: 1.585971418723991....\n",
      "-----------------------------------\n",
      "Training loss: 1.6187317282961873....\n",
      "Validation loss: 1.5852269898764535....\n",
      "-----------------------------------\n",
      "Training loss: 1.6177863610940193....\n",
      "Validation loss: 1.584483899331935....\n",
      "-----------------------------------\n",
      "Training loss: 1.6168423917930597....\n",
      "Validation loss: 1.583741465512419....\n",
      "-----------------------------------\n",
      "Training loss: 1.6159000577288785....\n",
      "Validation loss: 1.5829985152730377....\n",
      "-----------------------------------\n",
      "Training loss: 1.6149584884757768....\n",
      "Validation loss: 1.582256176955477....\n",
      "-----------------------------------\n",
      "Training loss: 1.6140182259246107....\n",
      "Validation loss: 1.5815148418040572....\n",
      "-----------------------------------\n",
      "Training loss: 1.6130801345396024....\n",
      "Validation loss: 1.5807739298848014....\n",
      "-----------------------------------\n",
      "Training loss: 1.612142762194227....\n",
      "Validation loss: 1.580033330578196....\n",
      "-----------------------------------\n",
      "Training loss: 1.6112061125355128....\n",
      "Validation loss: 1.5792925237311148....\n",
      "-----------------------------------\n",
      "Training loss: 1.6102693951872267....\n",
      "Validation loss: 1.5785525413703212....\n",
      "-----------------------------------\n",
      "Training loss: 1.6093339404551297....\n",
      "Validation loss: 1.5778121324225414....\n",
      "-----------------------------------\n",
      "Training loss: 1.608398551688058....\n",
      "Validation loss: 1.5770719785377565....\n",
      "-----------------------------------\n",
      "Training loss: 1.6074646909692147....\n",
      "Validation loss: 1.5763317914363484....\n",
      "-----------------------------------\n",
      "Training loss: 1.6065320576112683....\n",
      "Validation loss: 1.575592027098991....\n",
      "-----------------------------------\n",
      "Training loss: 1.6056014909553367....\n",
      "Validation loss: 1.5748533639833533....\n",
      "-----------------------------------\n",
      "Training loss: 1.60467195368737....\n",
      "Validation loss: 1.5741156531547795....\n",
      "-----------------------------------\n",
      "Training loss: 1.6037427233039412....\n",
      "Validation loss: 1.5733787822873593....\n",
      "-----------------------------------\n",
      "Training loss: 1.6028138614310763....\n",
      "Validation loss: 1.572642505195136....\n",
      "-----------------------------------\n",
      "Training loss: 1.6018857354797926....\n",
      "Validation loss: 1.5719065914716368....\n",
      "-----------------------------------\n",
      "Training loss: 1.600958568904301....\n",
      "Validation loss: 1.5711709765301232....\n",
      "-----------------------------------\n",
      "Training loss: 1.6000324664495826....\n",
      "Validation loss: 1.57043601134519....\n",
      "-----------------------------------\n",
      "Training loss: 1.5991075424536723....\n",
      "Validation loss: 1.5697022872960402....\n",
      "-----------------------------------\n",
      "Training loss: 1.5981840791331023....\n",
      "Validation loss: 1.5689701291409288....\n",
      "-----------------------------------\n",
      "Training loss: 1.5972621406039182....\n",
      "Validation loss: 1.568238230575906....\n",
      "-----------------------------------\n",
      "Training loss: 1.5963408234673357....\n",
      "Validation loss: 1.5675066941897058....\n",
      "-----------------------------------\n",
      "Training loss: 1.5954201167434516....\n",
      "Validation loss: 1.5667758447929832....\n",
      "-----------------------------------\n",
      "Training loss: 1.5945004465515342....\n",
      "Validation loss: 1.5660455804527262....\n",
      "-----------------------------------\n",
      "Training loss: 1.5935817883434558....\n",
      "Validation loss: 1.5653165220321075....\n",
      "-----------------------------------\n",
      "Training loss: 1.5926645447057908....\n",
      "Validation loss: 1.5645885049077064....\n",
      "-----------------------------------\n",
      "Training loss: 1.5917484580933425....\n",
      "Validation loss: 1.5638611018972455....\n",
      "-----------------------------------\n",
      "Training loss: 1.5908333889809123....\n",
      "Validation loss: 1.5631346403168618....\n",
      "-----------------------------------\n",
      "Training loss: 1.5899195837540359....\n",
      "Validation loss: 1.5624096962609992....\n",
      "-----------------------------------\n",
      "Training loss: 1.5890072136931204....\n",
      "Validation loss: 1.5616853642947768....\n",
      "-----------------------------------\n",
      "Training loss: 1.5880961978666415....\n",
      "Validation loss: 1.5609617759338839....\n",
      "-----------------------------------\n",
      "Training loss: 1.5871860440007985....\n",
      "Validation loss: 1.5602389163780406....\n",
      "-----------------------------------\n",
      "Training loss: 1.5862765486298325....\n",
      "Validation loss: 1.5595168221463542....\n",
      "-----------------------------------\n",
      "Training loss: 1.5853686451584208....\n",
      "Validation loss: 1.5587947925324892....\n",
      "-----------------------------------\n",
      "Training loss: 1.5844619873649226....\n",
      "Validation loss: 1.5580743407090523....\n",
      "-----------------------------------\n",
      "Training loss: 1.5835562594257746....\n",
      "Validation loss: 1.5573537633198078....\n",
      "-----------------------------------\n",
      "Training loss: 1.5826510336661075....\n",
      "Validation loss: 1.5566339789481398....\n",
      "-----------------------------------\n",
      "Training loss: 1.5817461515171312....\n",
      "Validation loss: 1.5559143148927....\n",
      "-----------------------------------\n",
      "Training loss: 1.580841909920593....\n",
      "Validation loss: 1.5551956395986062....\n",
      "-----------------------------------\n",
      "Training loss: 1.5799393288616064....\n",
      "Validation loss: 1.5544775367563932....\n",
      "-----------------------------------\n",
      "Training loss: 1.5790375752827857....\n",
      "Validation loss: 1.5537597495058786....\n",
      "-----------------------------------\n",
      "Training loss: 1.5781360453483924....\n",
      "Validation loss: 1.553042527931941....\n",
      "-----------------------------------\n",
      "Training loss: 1.5772349465116116....\n",
      "Validation loss: 1.5523262260791841....\n",
      "-----------------------------------\n",
      "Training loss: 1.576334997030565....\n",
      "Validation loss: 1.5516097059116485....\n",
      "-----------------------------------\n",
      "Training loss: 1.5754356872278505....\n",
      "Validation loss: 1.5508940890540122....\n",
      "-----------------------------------\n",
      "Training loss: 1.5745371666634889....\n",
      "Validation loss: 1.5501788932972214....\n",
      "-----------------------------------\n",
      "Training loss: 1.5736398292634495....\n",
      "Validation loss: 1.5494646505971967....\n",
      "-----------------------------------\n",
      "Training loss: 1.5727438945291279....\n",
      "Validation loss: 1.5487509999878397....\n",
      "-----------------------------------\n",
      "Training loss: 1.5718484122617389....\n",
      "Validation loss: 1.5480384129539688....\n",
      "-----------------------------------\n",
      "Training loss: 1.570953888461905....\n",
      "Validation loss: 1.547326535923837....\n",
      "-----------------------------------\n",
      "Training loss: 1.5700601274840003....\n",
      "Validation loss: 1.546616203759549....\n",
      "-----------------------------------\n",
      "Training loss: 1.5691672549366908....\n",
      "Validation loss: 1.5459068645382403....\n",
      "-----------------------------------\n",
      "Training loss: 1.5682750876451272....\n",
      "Validation loss: 1.5451982973552545....\n",
      "-----------------------------------\n",
      "Training loss: 1.5673835867558872....\n",
      "Validation loss: 1.544490558633886....\n",
      "-----------------------------------\n",
      "Training loss: 1.5664931552063999....\n",
      "Validation loss: 1.5437837309676934....\n",
      "-----------------------------------\n",
      "Training loss: 1.5656039527153964....\n",
      "Validation loss: 1.5430780371294164....\n",
      "-----------------------------------\n",
      "Training loss: 1.564716122733683....\n",
      "Validation loss: 1.5423730877778472....\n",
      "-----------------------------------\n",
      "Training loss: 1.5638296191864471....\n",
      "Validation loss: 1.541668487674637....\n",
      "-----------------------------------\n",
      "Training loss: 1.5629437066658347....\n",
      "Validation loss: 1.5409642207046....\n",
      "-----------------------------------\n",
      "Training loss: 1.5620581639780193....\n",
      "Validation loss: 1.540261474455517....\n",
      "-----------------------------------\n",
      "Training loss: 1.5611729879770222....\n",
      "Validation loss: 1.5395594106001276....\n",
      "-----------------------------------\n",
      "Training loss: 1.5602882714495026....\n",
      "Validation loss: 1.538857487380004....\n",
      "-----------------------------------\n",
      "Training loss: 1.559404678564153....\n",
      "Validation loss: 1.5381561922043556....\n",
      "-----------------------------------\n",
      "Training loss: 1.5585222540206147....\n",
      "Validation loss: 1.5374558096165385....\n",
      "-----------------------------------\n",
      "Training loss: 1.5576411020621967....\n",
      "Validation loss: 1.5367563148683836....\n",
      "-----------------------------------\n",
      "Training loss: 1.5567610031057635....\n",
      "Validation loss: 1.5360581661388815....\n",
      "-----------------------------------\n",
      "Training loss: 1.5558814703865054....\n",
      "Validation loss: 1.5353607713518425....\n",
      "-----------------------------------\n",
      "Training loss: 1.5550031916489884....\n",
      "Validation loss: 1.534664241708221....\n",
      "-----------------------------------\n",
      "Training loss: 1.5541258360927837....\n",
      "Validation loss: 1.5339677311526672....\n",
      "-----------------------------------\n",
      "Training loss: 1.553249004598359....\n",
      "Validation loss: 1.5332719124135425....\n",
      "-----------------------------------\n",
      "Training loss: 1.5523731193720853....\n",
      "Validation loss: 1.5325773428443947....\n",
      "-----------------------------------\n",
      "Training loss: 1.5514982055314548....\n",
      "Validation loss: 1.5318839164452105....\n",
      "-----------------------------------\n",
      "Training loss: 1.5506242302548123....\n",
      "Validation loss: 1.5311915317531475....\n",
      "-----------------------------------\n",
      "Training loss: 1.5497512255991308....\n",
      "Validation loss: 1.530500509166869....\n",
      "-----------------------------------\n",
      "Training loss: 1.5488796269812959....\n",
      "Validation loss: 1.5298103820041822....\n",
      "-----------------------------------\n",
      "Training loss: 1.548009321887258....\n",
      "Validation loss: 1.529120969385018....\n",
      "-----------------------------------\n",
      "Training loss: 1.5471403230763257....\n",
      "Validation loss: 1.5284329462731319....\n",
      "-----------------------------------\n",
      "Training loss: 1.5462726500190522....\n",
      "Validation loss: 1.5277457894482072....\n",
      "-----------------------------------\n",
      "Training loss: 1.5454057873080302....\n",
      "Validation loss: 1.5270595242676612....\n",
      "-----------------------------------\n",
      "Training loss: 1.5445390487575181....\n",
      "Validation loss: 1.5263742389049704....\n",
      "-----------------------------------\n",
      "Training loss: 1.5436727088632887....\n",
      "Validation loss: 1.525689166171464....\n",
      "-----------------------------------\n",
      "Training loss: 1.5428068122971645....\n",
      "Validation loss: 1.5250048876716924....\n",
      "-----------------------------------\n",
      "Training loss: 1.541942260400212....\n",
      "Validation loss: 1.5243213941216505....\n",
      "-----------------------------------\n",
      "Training loss: 1.5410784997000682....\n",
      "Validation loss: 1.5236384062892085....\n",
      "-----------------------------------\n",
      "Training loss: 1.540215475454773....\n",
      "Validation loss: 1.5229557298640128....\n",
      "-----------------------------------\n",
      "Training loss: 1.5393528304162454....\n",
      "Validation loss: 1.5222737790875356....\n",
      "-----------------------------------\n",
      "Training loss: 1.5384912235930333....\n",
      "Validation loss: 1.521592754825238....\n",
      "-----------------------------------\n",
      "Training loss: 1.5376306842167846....\n",
      "Validation loss: 1.5209124866098378....\n",
      "-----------------------------------\n",
      "Training loss: 1.5367712038640675....\n",
      "Validation loss: 1.520232701361033....\n",
      "-----------------------------------\n",
      "Training loss: 1.5359123077623227....\n",
      "Validation loss: 1.5195539842221495....\n",
      "-----------------------------------\n",
      "Training loss: 1.5350548901078112....\n",
      "Validation loss: 1.5188759652507182....\n",
      "-----------------------------------\n",
      "Training loss: 1.534198960224462....\n",
      "Validation loss: 1.5181990045733593....\n",
      "-----------------------------------\n",
      "Training loss: 1.5333440736164754....\n",
      "Validation loss: 1.5175226110989695....\n",
      "-----------------------------------\n",
      "Training loss: 1.5324900634184901....\n",
      "Validation loss: 1.5168464575148441....\n",
      "-----------------------------------\n",
      "Training loss: 1.5316363914489106....\n",
      "Validation loss: 1.5161710544142148....\n",
      "-----------------------------------\n",
      "Training loss: 1.5307837275494034....\n",
      "Validation loss: 1.5154953347018731....\n",
      "-----------------------------------\n",
      "Training loss: 1.529931831225717....\n",
      "Validation loss: 1.5148209866729134....\n",
      "-----------------------------------\n",
      "Training loss: 1.5290810687981413....\n",
      "Validation loss: 1.5141478265363235....\n",
      "-----------------------------------\n",
      "Training loss: 1.5282308419747495....\n",
      "Validation loss: 1.5134752042738902....\n",
      "-----------------------------------\n",
      "Training loss: 1.527381342521276....\n",
      "Validation loss: 1.5128032147977488....\n",
      "-----------------------------------\n",
      "Training loss: 1.526532264841743....\n",
      "Validation loss: 1.5121319490801268....\n",
      "-----------------------------------\n",
      "Training loss: 1.5256839287734512....\n",
      "Validation loss: 1.511461528220834....\n",
      "-----------------------------------\n",
      "Training loss: 1.5248369222790805....\n",
      "Validation loss: 1.510792344266099....\n",
      "-----------------------------------\n",
      "Training loss: 1.523991004380346....\n",
      "Validation loss: 1.5101241899424722....\n",
      "-----------------------------------\n",
      "Training loss: 1.5231460962152579....\n",
      "Validation loss: 1.5094564125716965....\n",
      "-----------------------------------\n",
      "Training loss: 1.5223018325122053....\n",
      "Validation loss: 1.5087896702056574....\n",
      "-----------------------------------\n",
      "Training loss: 1.5214588638129374....\n",
      "Validation loss: 1.5081249609148577....\n",
      "-----------------------------------\n",
      "Training loss: 1.5206170729709103....\n",
      "Validation loss: 1.5074599075735362....\n",
      "-----------------------------------\n",
      "Training loss: 1.5197756273652423....\n",
      "Validation loss: 1.5067951399039834....\n",
      "-----------------------------------\n",
      "Training loss: 1.5189350930192156....\n",
      "Validation loss: 1.5061309631135078....\n",
      "-----------------------------------\n",
      "Training loss: 1.5180948456371814....\n",
      "Validation loss: 1.5054680502355606....\n",
      "-----------------------------------\n",
      "Training loss: 1.517255635681636....\n",
      "Validation loss: 1.504805659465103....\n",
      "-----------------------------------\n",
      "Training loss: 1.516417134263469....\n",
      "Validation loss: 1.5041441949189815....\n",
      "-----------------------------------\n",
      "Training loss: 1.5155794167957228....\n",
      "Validation loss: 1.5034839004802174....\n",
      "-----------------------------------\n",
      "Training loss: 1.514742471363584....\n",
      "Validation loss: 1.5028246583072347....\n",
      "-----------------------------------\n",
      "Training loss: 1.5139066799266783....\n",
      "Validation loss: 1.5021662122596275....\n",
      "-----------------------------------\n",
      "Training loss: 1.5130715319381334....\n",
      "Validation loss: 1.5015084415652833....\n",
      "-----------------------------------\n",
      "Training loss: 1.5122368217619184....\n",
      "Validation loss: 1.5008515578223314....\n",
      "-----------------------------------\n",
      "Training loss: 1.5114032247107942....\n",
      "Validation loss: 1.5001956644489596....\n",
      "-----------------------------------\n",
      "Training loss: 1.510571215687216....\n",
      "Validation loss: 1.4995405891752995....\n",
      "-----------------------------------\n",
      "Training loss: 1.509740557098753....\n",
      "Validation loss: 1.4988863061913449....\n",
      "-----------------------------------\n",
      "Training loss: 1.5089111926459469....\n",
      "Validation loss: 1.498232720835325....\n",
      "-----------------------------------\n",
      "Training loss: 1.5080827762871658....\n",
      "Validation loss: 1.4975798102321325....\n",
      "-----------------------------------\n",
      "Training loss: 1.5072554284427107....\n",
      "Validation loss: 1.4969265287122508....\n",
      "-----------------------------------\n",
      "Training loss: 1.5064278462460883....\n",
      "Validation loss: 1.4962738501489252....\n",
      "-----------------------------------\n",
      "Training loss: 1.5056010715521448....\n",
      "Validation loss: 1.495621205145497....\n",
      "-----------------------------------\n",
      "Training loss: 1.5047742782805922....\n",
      "Validation loss: 1.4949694970813339....\n",
      "-----------------------------------\n",
      "Training loss: 1.5039486293309832....\n",
      "Validation loss: 1.4943178831240753....\n",
      "-----------------------------------\n",
      "Training loss: 1.503124199841022....\n",
      "Validation loss: 1.4936660970460307....\n",
      "-----------------------------------\n",
      "Training loss: 1.5022999231126537....\n",
      "Validation loss: 1.4930149448574646....\n",
      "-----------------------------------\n",
      "Training loss: 1.5014756748798723....\n",
      "Validation loss: 1.492364218323114....\n",
      "-----------------------------------\n",
      "Training loss: 1.5006523470510822....\n",
      "Validation loss: 1.4917143556566943....\n",
      "-----------------------------------\n",
      "Training loss: 1.4998301488012713....\n",
      "Validation loss: 1.4910652579932793....\n",
      "-----------------------------------\n",
      "Training loss: 1.4990085926774468....\n",
      "Validation loss: 1.4904170033380384....\n",
      "-----------------------------------\n",
      "Training loss: 1.4981876623052426....\n",
      "Validation loss: 1.4897691948652123....\n",
      "-----------------------------------\n",
      "Training loss: 1.4973675056573226....\n",
      "Validation loss: 1.4891226553833492....\n",
      "-----------------------------------\n",
      "Training loss: 1.4965485124903297....\n",
      "Validation loss: 1.488477084923277....\n",
      "-----------------------------------\n",
      "Training loss: 1.4957301348373806....\n",
      "Validation loss: 1.487832490007494....\n",
      "-----------------------------------\n",
      "Training loss: 1.4949122086496944....\n",
      "Validation loss: 1.4871889030709204....\n",
      "-----------------------------------\n",
      "Training loss: 1.4940955531414248....\n",
      "Validation loss: 1.4865460136405455....\n",
      "-----------------------------------\n",
      "Training loss: 1.4932792450472776....\n",
      "Validation loss: 1.4859034919412517....\n",
      "-----------------------------------\n",
      "Training loss: 1.4924639370657526....\n",
      "Validation loss: 1.4852620186269416....\n",
      "-----------------------------------\n",
      "Training loss: 1.4916492408226893....\n",
      "Validation loss: 1.4846213158164991....\n",
      "-----------------------------------\n",
      "Training loss: 1.4908353238730343....\n",
      "Validation loss: 1.4839820492756466....\n",
      "-----------------------------------\n",
      "Training loss: 1.4900222743416962....\n",
      "Validation loss: 1.4833430715666833....\n",
      "-----------------------------------\n",
      "Training loss: 1.4892098505457843....\n",
      "Validation loss: 1.4827041878990272....\n",
      "-----------------------------------\n",
      "Training loss: 1.4883977843281926....\n",
      "Validation loss: 1.4820653712954868....\n",
      "-----------------------------------\n",
      "Training loss: 1.4875860211768233....\n",
      "Validation loss: 1.4814272558346033....\n",
      "-----------------------------------\n",
      "Training loss: 1.4867746308660013....\n",
      "Validation loss: 1.4807895432800042....\n",
      "-----------------------------------\n",
      "Training loss: 1.4859640467645803....\n",
      "Validation loss: 1.4801529612245778....\n",
      "-----------------------------------\n",
      "Training loss: 1.4851541402119135....\n",
      "Validation loss: 1.4795169598385638....\n",
      "-----------------------------------\n",
      "Training loss: 1.4843446925735777....\n",
      "Validation loss: 1.4788814315769783....\n",
      "-----------------------------------\n",
      "Training loss: 1.4835360916891012....\n",
      "Validation loss: 1.4782461763368824....\n",
      "-----------------------------------\n",
      "Training loss: 1.4827279928460122....\n",
      "Validation loss: 1.4776110579578596....\n",
      "-----------------------------------\n",
      "Training loss: 1.4819206711939887....\n",
      "Validation loss: 1.4769764136687173....\n",
      "-----------------------------------\n",
      "Training loss: 1.481114279773822....\n",
      "Validation loss: 1.4763427169305918....\n",
      "-----------------------------------\n",
      "Training loss: 1.480308823562533....\n",
      "Validation loss: 1.475709868638179....\n",
      "-----------------------------------\n",
      "Training loss: 1.4795040316049817....\n",
      "Validation loss: 1.475078089611283....\n",
      "-----------------------------------\n",
      "Training loss: 1.4787001230917058....\n",
      "Validation loss: 1.474446428600284....\n",
      "-----------------------------------\n",
      "Training loss: 1.4778966535176448....\n",
      "Validation loss: 1.473815563268947....\n",
      "-----------------------------------\n",
      "Training loss: 1.477094005921272....\n",
      "Validation loss: 1.4731858083624343....\n",
      "-----------------------------------\n",
      "Training loss: 1.4762926774603085....\n",
      "Validation loss: 1.472557063156199....\n",
      "-----------------------------------\n",
      "Training loss: 1.4754916187473792....\n",
      "Validation loss: 1.4719293157110522....\n",
      "-----------------------------------\n",
      "Training loss: 1.4746914505994093....\n",
      "Validation loss: 1.4713025202339995....\n",
      "-----------------------------------\n",
      "Training loss: 1.4738923599944234....\n",
      "Validation loss: 1.470676525680152....\n",
      "-----------------------------------\n",
      "Training loss: 1.4730940259560512....\n",
      "Validation loss: 1.470050687369664....\n",
      "-----------------------------------\n",
      "Training loss: 1.472296736682063....\n",
      "Validation loss: 1.4694254798806643....\n",
      "-----------------------------------\n",
      "Training loss: 1.471500487502951....\n",
      "Validation loss: 1.468801275769758....\n",
      "-----------------------------------\n",
      "Training loss: 1.4707052654301482....\n",
      "Validation loss: 1.4681778810249293....\n",
      "-----------------------------------\n",
      "Training loss: 1.4699106573815097....\n",
      "Validation loss: 1.4675548292871523....\n",
      "-----------------------------------\n",
      "Training loss: 1.4691167310078936....\n",
      "Validation loss: 1.4669319479742449....\n",
      "-----------------------------------\n",
      "Training loss: 1.4683232806148032....\n",
      "Validation loss: 1.4663098849034244....\n",
      "-----------------------------------\n",
      "Training loss: 1.467530517630196....\n",
      "Validation loss: 1.465688988038473....\n",
      "-----------------------------------\n",
      "Training loss: 1.4667382801349607....\n",
      "Validation loss: 1.4650686192114175....\n",
      "-----------------------------------\n",
      "Training loss: 1.4659466222623692....\n",
      "Validation loss: 1.4644489942372443....\n",
      "-----------------------------------\n",
      "Training loss: 1.4651556567939235....\n",
      "Validation loss: 1.4638301579748714....\n",
      "-----------------------------------\n",
      "Training loss: 1.4643654483827016....\n",
      "Validation loss: 1.463211965519518....\n",
      "-----------------------------------\n",
      "Training loss: 1.4635757451780187....\n",
      "Validation loss: 1.4625947890693638....\n",
      "-----------------------------------\n",
      "Training loss: 1.4627869567988145....\n",
      "Validation loss: 1.4619782321059742....\n",
      "-----------------------------------\n",
      "Training loss: 1.4619987083500456....\n",
      "Validation loss: 1.4613617240174892....\n",
      "-----------------------------------\n",
      "Training loss: 1.4612104577748444....\n",
      "Validation loss: 1.4607462072853383....\n",
      "-----------------------------------\n",
      "Training loss: 1.460423021233034....\n",
      "Validation loss: 1.460131937973131....\n",
      "-----------------------------------\n",
      "Training loss: 1.459636550905798....\n",
      "Validation loss: 1.4595188420714533....\n",
      "-----------------------------------\n",
      "Training loss: 1.458851157931105....\n",
      "Validation loss: 1.4589060378925334....\n",
      "-----------------------------------\n",
      "Training loss: 1.4580670450592648....\n",
      "Validation loss: 1.4582938434261665....\n",
      "-----------------------------------\n",
      "Training loss: 1.4572837682925646....\n",
      "Validation loss: 1.4576827574358955....\n",
      "-----------------------------------\n",
      "Training loss: 1.4565012516237248....\n",
      "Validation loss: 1.4570724464885751....\n",
      "-----------------------------------\n",
      "Training loss: 1.4557195945433252....\n",
      "Validation loss: 1.4564628490249594....\n",
      "-----------------------------------\n",
      "Training loss: 1.45493869143807....\n",
      "Validation loss: 1.4558535962195565....\n",
      "-----------------------------------\n",
      "Training loss: 1.4541584232792482....\n",
      "Validation loss: 1.4552446745313499....\n",
      "-----------------------------------\n",
      "Training loss: 1.4533784442081612....\n",
      "Validation loss: 1.4546360608592728....\n",
      "-----------------------------------\n",
      "Training loss: 1.4525992474996101....\n",
      "Validation loss: 1.454028017518262....\n",
      "-----------------------------------\n",
      "Training loss: 1.4518207871338167....\n",
      "Validation loss: 1.4534204924542435....\n",
      "-----------------------------------\n",
      "Training loss: 1.4510431522420522....\n",
      "Validation loss: 1.4528137313744052....\n",
      "-----------------------------------\n",
      "Training loss: 1.4502663576916386....\n",
      "Validation loss: 1.4522078119230168....\n",
      "-----------------------------------\n",
      "Training loss: 1.4494900486341395....\n",
      "Validation loss: 1.4516023641163893....\n",
      "-----------------------------------\n",
      "Training loss: 1.448714614787616....\n",
      "Validation loss: 1.45099731273909....\n",
      "-----------------------------------\n",
      "Training loss: 1.4479401583572915....\n",
      "Validation loss: 1.4503930198635275....\n",
      "-----------------------------------\n",
      "Training loss: 1.4471661318144944....\n",
      "Validation loss: 1.4497891518990877....\n",
      "-----------------------------------\n",
      "Training loss: 1.4463927223057338....\n",
      "Validation loss: 1.449186674782716....\n",
      "-----------------------------------\n",
      "Training loss: 1.4456200121632101....\n",
      "Validation loss: 1.4485849038623473....\n",
      "-----------------------------------\n",
      "Training loss: 1.4448477608415438....\n",
      "Validation loss: 1.4479838522065298....\n",
      "-----------------------------------\n",
      "Training loss: 1.4440766540227616....\n",
      "Validation loss: 1.447383406182902....\n",
      "-----------------------------------\n",
      "Training loss: 1.443306372569219....\n",
      "Validation loss: 1.4467837830476442....\n",
      "-----------------------------------\n",
      "Training loss: 1.4425365120477192....\n",
      "Validation loss: 1.4461849158026072....\n",
      "-----------------------------------\n",
      "Training loss: 1.4417671537874448....\n",
      "Validation loss: 1.4455865469694604....\n",
      "-----------------------------------\n",
      "Training loss: 1.4409981305481656....\n",
      "Validation loss: 1.4449890192205093....\n",
      "-----------------------------------\n",
      "Training loss: 1.440230193701479....\n",
      "Validation loss: 1.4443919040497655....\n",
      "-----------------------------------\n",
      "Training loss: 1.4394632225910944....\n",
      "Validation loss: 1.4437952871454953....\n",
      "-----------------------------------\n",
      "Training loss: 1.4386973633923879....\n",
      "Validation loss: 1.443199829354341....\n",
      "-----------------------------------\n",
      "Training loss: 1.437932654273438....\n",
      "Validation loss: 1.4426055493191352....\n",
      "-----------------------------------\n",
      "Training loss: 1.4371690467555378....\n",
      "Validation loss: 1.4420127366954523....\n",
      "-----------------------------------\n",
      "Training loss: 1.436406416970951....\n",
      "Validation loss: 1.4414210482114205....\n",
      "-----------------------------------\n",
      "Training loss: 1.4356445990087539....\n",
      "Validation loss: 1.4408296803067877....\n",
      "-----------------------------------\n",
      "Training loss: 1.434883455648508....\n",
      "Validation loss: 1.4402388176512555....\n",
      "-----------------------------------\n",
      "Training loss: 1.434123152300215....\n",
      "Validation loss: 1.4396485920933908....\n",
      "-----------------------------------\n",
      "Training loss: 1.4333636300634474....\n",
      "Validation loss: 1.439058966272015....\n",
      "-----------------------------------\n",
      "Training loss: 1.4326050705005857....\n",
      "Validation loss: 1.438469815506458....\n",
      "-----------------------------------\n",
      "Training loss: 1.4318475785768925....\n",
      "Validation loss: 1.4378814138233753....\n",
      "-----------------------------------\n",
      "Training loss: 1.4310905191938788....\n",
      "Validation loss: 1.4372939793163337....\n",
      "-----------------------------------\n",
      "Training loss: 1.4303337257152826....\n",
      "Validation loss: 1.4367075998867491....\n",
      "-----------------------------------\n",
      "Training loss: 1.4295774288864107....\n",
      "Validation loss: 1.43612190461497....\n",
      "-----------------------------------\n",
      "Training loss: 1.4288216584281126....\n",
      "Validation loss: 1.4355366827169154....\n",
      "-----------------------------------\n",
      "Training loss: 1.4280668373008283....\n",
      "Validation loss: 1.4349524689556301....\n",
      "-----------------------------------\n",
      "Training loss: 1.4273130799725853....\n",
      "Validation loss: 1.4343695034826862....\n",
      "-----------------------------------\n",
      "Training loss: 1.4265598795256624....\n",
      "Validation loss: 1.4337873217717....\n",
      "-----------------------------------\n",
      "Training loss: 1.4258074615863112....\n",
      "Validation loss: 1.4332059216283233....\n",
      "-----------------------------------\n",
      "Training loss: 1.4250556130132737....\n",
      "Validation loss: 1.432625529584685....\n",
      "-----------------------------------\n",
      "Training loss: 1.4243047002636118....\n",
      "Validation loss: 1.432045971535675....\n",
      "-----------------------------------\n",
      "Training loss: 1.4235547468679997....\n",
      "Validation loss: 1.4314671155730199....\n",
      "-----------------------------------\n",
      "Training loss: 1.4228055956288688....\n",
      "Validation loss: 1.4308886171276933....\n",
      "-----------------------------------\n",
      "Training loss: 1.422057408322124....\n",
      "Validation loss: 1.4303112681729573....\n",
      "-----------------------------------\n",
      "Training loss: 1.421310429387821....\n",
      "Validation loss: 1.4297347503506115....\n",
      "-----------------------------------\n",
      "Training loss: 1.4205639706006419....\n",
      "Validation loss: 1.4291587605630285....\n",
      "-----------------------------------\n",
      "Training loss: 1.4198184218331518....\n",
      "Validation loss: 1.4285834520622267....\n",
      "-----------------------------------\n",
      "Training loss: 1.4190739051448766....\n",
      "Validation loss: 1.4280087152751297....\n",
      "-----------------------------------\n",
      "Training loss: 1.4183303360339619....\n",
      "Validation loss: 1.427434079535707....\n",
      "-----------------------------------\n",
      "Training loss: 1.4175872993343357....\n",
      "Validation loss: 1.4268596848391453....\n",
      "-----------------------------------\n",
      "Training loss: 1.4168450262297703....\n",
      "Validation loss: 1.4262862706513377....\n",
      "-----------------------------------\n",
      "Training loss: 1.4161037904863452....\n",
      "Validation loss: 1.4257130199166053....\n",
      "-----------------------------------\n",
      "Training loss: 1.415363285999658....\n",
      "Validation loss: 1.4251406675006235....\n",
      "-----------------------------------\n",
      "Training loss: 1.4146239471756978....\n",
      "Validation loss: 1.424569121799704....\n",
      "-----------------------------------\n",
      "Training loss: 1.413885688747782....\n",
      "Validation loss: 1.4239981650791227....\n",
      "-----------------------------------\n",
      "Training loss: 1.4131484300749728....\n",
      "Validation loss: 1.4234273141500422....\n",
      "-----------------------------------\n",
      "Training loss: 1.412411837290747....\n",
      "Validation loss: 1.422857189865579....\n",
      "-----------------------------------\n",
      "Training loss: 1.411676418515602....\n",
      "Validation loss: 1.4222880935228155....\n",
      "-----------------------------------\n",
      "Training loss: 1.410941821414571....\n",
      "Validation loss: 1.4217199917934955....\n",
      "-----------------------------------\n",
      "Training loss: 1.410207636647643....\n",
      "Validation loss: 1.4211528451412487....\n",
      "-----------------------------------\n",
      "Training loss: 1.4094741516509737....\n",
      "Validation loss: 1.4205865997337264....\n",
      "-----------------------------------\n",
      "Training loss: 1.4087414241440126....\n",
      "Validation loss: 1.4200214057774057....\n",
      "-----------------------------------\n",
      "Training loss: 1.4080094310966587....\n",
      "Validation loss: 1.4194567046509263....\n",
      "-----------------------------------\n",
      "Training loss: 1.4072775852671158....\n",
      "Validation loss: 1.4188924702870147....\n",
      "-----------------------------------\n",
      "Training loss: 1.4065463475917963....\n",
      "Validation loss: 1.418328514518872....\n",
      "-----------------------------------\n",
      "Training loss: 1.4058156897484855....\n",
      "Validation loss: 1.417764629500909....\n",
      "-----------------------------------\n",
      "Training loss: 1.4050856032188093....\n",
      "Validation loss: 1.4172016240335579....\n",
      "-----------------------------------\n",
      "Training loss: 1.4043565999403365....\n",
      "Validation loss: 1.416639836017014....\n",
      "-----------------------------------\n",
      "Training loss: 1.4036283397740963....\n",
      "Validation loss: 1.416078784263035....\n",
      "-----------------------------------\n",
      "Training loss: 1.4029005716573486....\n",
      "Validation loss: 1.4155184427217105....\n",
      "-----------------------------------\n",
      "Training loss: 1.4021739486954408....\n",
      "Validation loss: 1.41495901891809....\n",
      "-----------------------------------\n",
      "Training loss: 1.4014477960948408....\n",
      "Validation loss: 1.4143999519108088....\n",
      "-----------------------------------\n",
      "Training loss: 1.400722269773517....\n",
      "Validation loss: 1.4138415408476335....\n",
      "-----------------------------------\n",
      "Training loss: 1.39999759035648....\n",
      "Validation loss: 1.4132839560937691....\n",
      "-----------------------------------\n",
      "Training loss: 1.399273427755272....\n",
      "Validation loss: 1.4127268867107856....\n",
      "-----------------------------------\n",
      "Training loss: 1.3985503122567067....\n",
      "Validation loss: 1.412170727595208....\n",
      "-----------------------------------\n",
      "Training loss: 1.3978280336274818....\n",
      "Validation loss: 1.4116155023060277....\n",
      "-----------------------------------\n",
      "Training loss: 1.3971066680741135....\n",
      "Validation loss: 1.4110612222815873....\n",
      "-----------------------------------\n",
      "Training loss: 1.396385723830836....\n",
      "Validation loss: 1.4105081158954498....\n",
      "-----------------------------------\n",
      "Training loss: 1.3956658221729452....\n",
      "Validation loss: 1.4099561807121008....\n",
      "-----------------------------------\n",
      "Training loss: 1.3949466187896493....\n",
      "Validation loss: 1.4094047668197398....\n",
      "-----------------------------------\n",
      "Training loss: 1.3942282146152125....\n",
      "Validation loss: 1.408854207716472....\n",
      "-----------------------------------\n",
      "Training loss: 1.3935104432189507....\n",
      "Validation loss: 1.4083042094120966....\n",
      "-----------------------------------\n",
      "Training loss: 1.3927932740874707....\n",
      "Validation loss: 1.4077554804879573....\n",
      "-----------------------------------\n",
      "Training loss: 1.392076487309078....\n",
      "Validation loss: 1.4072070734767563....\n",
      "-----------------------------------\n",
      "Training loss: 1.3913603764674918....\n",
      "Validation loss: 1.4066590515425836....\n",
      "-----------------------------------\n",
      "Training loss: 1.390644948522914....\n",
      "Validation loss: 1.4061118432765012....\n",
      "-----------------------------------\n",
      "Training loss: 1.389930049217301....\n",
      "Validation loss: 1.4055645015167528....\n",
      "-----------------------------------\n",
      "Training loss: 1.3892156044259918....\n",
      "Validation loss: 1.405017206440807....\n",
      "-----------------------------------\n",
      "Training loss: 1.3885016970131538....\n",
      "Validation loss: 1.404470077584872....\n",
      "-----------------------------------\n",
      "Training loss: 1.3877883711863046....\n",
      "Validation loss: 1.403924138629933....\n",
      "-----------------------------------\n",
      "Training loss: 1.387075712084596....\n",
      "Validation loss: 1.403378039538521....\n",
      "-----------------------------------\n",
      "Training loss: 1.3863638751958793....\n",
      "Validation loss: 1.402832845945199....\n",
      "-----------------------------------\n",
      "Training loss: 1.3856529356580822....\n",
      "Validation loss: 1.402288305783791....\n",
      "-----------------------------------\n",
      "Training loss: 1.3849430089993349....\n",
      "Validation loss: 1.4017443748374219....\n",
      "-----------------------------------\n",
      "Training loss: 1.3842339494356146....\n",
      "Validation loss: 1.4012010316732986....\n",
      "-----------------------------------\n",
      "Training loss: 1.383525999566054....\n",
      "Validation loss: 1.4006584373399291....\n",
      "-----------------------------------\n",
      "Training loss: 1.3828189200657492....\n",
      "Validation loss: 1.4001160890844166....\n",
      "-----------------------------------\n",
      "Training loss: 1.3821124002120468....\n",
      "Validation loss: 1.3995748561898804....\n",
      "-----------------------------------\n",
      "Training loss: 1.3814068531770214....\n",
      "Validation loss: 1.399034451471869....\n",
      "-----------------------------------\n",
      "Training loss: 1.38070215569503....\n",
      "Validation loss: 1.3984947460858614....\n",
      "-----------------------------------\n",
      "Training loss: 1.3799985693238568....\n",
      "Validation loss: 1.3979555191585644....\n",
      "-----------------------------------\n",
      "Training loss: 1.3792965228615552....\n",
      "Validation loss: 1.3974172674483785....\n",
      "-----------------------------------\n",
      "Training loss: 1.3785954331482062....\n",
      "Validation loss: 1.3968795236003289....\n",
      "-----------------------------------\n",
      "Training loss: 1.3778956870347747....\n",
      "Validation loss: 1.396342617877074....\n",
      "-----------------------------------\n",
      "Training loss: 1.3771969061740543....\n",
      "Validation loss: 1.395806241719264....\n",
      "-----------------------------------\n",
      "Training loss: 1.3764990955892795....\n",
      "Validation loss: 1.395270652505536....\n",
      "-----------------------------------\n",
      "Training loss: 1.3758020532376312....\n",
      "Validation loss: 1.3947356780146465....\n",
      "-----------------------------------\n",
      "Training loss: 1.3751057575395471....\n",
      "Validation loss: 1.3942016356388829....\n",
      "-----------------------------------\n",
      "Training loss: 1.3744100505488412....\n",
      "Validation loss: 1.3936683084920594....\n",
      "-----------------------------------\n",
      "Training loss: 1.3737148892951523....\n",
      "Validation loss: 1.3931355248887363....\n",
      "-----------------------------------\n",
      "Training loss: 1.3730207006505892....\n",
      "Validation loss: 1.3926033792757648....\n",
      "-----------------------------------\n",
      "Training loss: 1.3723275923782778....\n",
      "Validation loss: 1.3920719347871533....\n",
      "-----------------------------------\n",
      "Training loss: 1.37163533957873....\n",
      "Validation loss: 1.3915409030839228....\n",
      "-----------------------------------\n",
      "Training loss: 1.3709441264687108....\n",
      "Validation loss: 1.391010769871517....\n",
      "-----------------------------------\n",
      "Training loss: 1.3702536784926282....\n",
      "Validation loss: 1.3904810172796365....\n",
      "-----------------------------------\n",
      "Training loss: 1.3695641457587526....\n",
      "Validation loss: 1.3899520501246996....\n",
      "-----------------------------------\n",
      "Training loss: 1.3688754208017158....\n",
      "Validation loss: 1.3894240324808398....\n",
      "-----------------------------------\n",
      "Training loss: 1.3681874246060783....\n",
      "Validation loss: 1.388896353591972....\n",
      "-----------------------------------\n",
      "Training loss: 1.3675000829558341....\n",
      "Validation loss: 1.3883693609409304....\n",
      "-----------------------------------\n",
      "Training loss: 1.3668138357873025....\n",
      "Validation loss: 1.3878432002298717....\n",
      "-----------------------------------\n",
      "Training loss: 1.3661286321562012....\n",
      "Validation loss: 1.3873179600199461....\n",
      "-----------------------------------\n",
      "Training loss: 1.365444433211243....\n",
      "Validation loss: 1.3867938234939063....\n",
      "-----------------------------------\n",
      "Training loss: 1.364761276587483....\n",
      "Validation loss: 1.386270359006341....\n",
      "-----------------------------------\n",
      "Training loss: 1.3640790801939988....\n",
      "Validation loss: 1.3857473670221043....\n",
      "-----------------------------------\n",
      "Training loss: 1.3633978861380374....\n",
      "Validation loss: 1.3852250829404675....\n",
      "-----------------------------------\n",
      "Training loss: 1.3627174615882074....\n",
      "Validation loss: 1.3847033255021046....\n",
      "-----------------------------------\n",
      "Training loss: 1.3620378531932387....\n",
      "Validation loss: 1.3841824263803164....\n",
      "-----------------------------------\n",
      "Training loss: 1.3613589087111717....\n",
      "Validation loss: 1.3836617921349081....\n",
      "-----------------------------------\n",
      "Training loss: 1.360680348827775....\n",
      "Validation loss: 1.3831420785720985....\n",
      "-----------------------------------\n",
      "Training loss: 1.3600024326570062....\n",
      "Validation loss: 1.3826229209423164....\n",
      "-----------------------------------\n",
      "Training loss: 1.3593253474098261....\n",
      "Validation loss: 1.3821053931319638....\n",
      "-----------------------------------\n",
      "Training loss: 1.3586486775968005....\n",
      "Validation loss: 1.3815883422962256....\n",
      "-----------------------------------\n",
      "Training loss: 1.3579727991350328....\n",
      "Validation loss: 1.381071898637023....\n",
      "-----------------------------------\n",
      "Training loss: 1.357297496538707....\n",
      "Validation loss: 1.380555862293707....\n",
      "-----------------------------------\n",
      "Training loss: 1.3566225767320843....\n",
      "Validation loss: 1.3800405099152337....\n",
      "-----------------------------------\n",
      "Training loss: 1.355948157754547....\n",
      "Validation loss: 1.379525777877801....\n",
      "-----------------------------------\n",
      "Training loss: 1.3552745511394177....\n",
      "Validation loss: 1.3790119832947452....\n",
      "-----------------------------------\n",
      "Training loss: 1.3546017974571967....\n",
      "Validation loss: 1.378499205200208....\n",
      "-----------------------------------\n",
      "Training loss: 1.353930233004629....\n",
      "Validation loss: 1.3779871419880756....\n",
      "-----------------------------------\n",
      "Training loss: 1.353259664626616....\n",
      "Validation loss: 1.3774762300200862....\n",
      "-----------------------------------\n",
      "Training loss: 1.3525899245126054....\n",
      "Validation loss: 1.3769660902410055....\n",
      "-----------------------------------\n",
      "Training loss: 1.3519207845387726....\n",
      "Validation loss: 1.3764564848931768....\n",
      "-----------------------------------\n",
      "Training loss: 1.3512525242388065....\n",
      "Validation loss: 1.3759475610367935....\n",
      "-----------------------------------\n",
      "Training loss: 1.3505852148608446....\n",
      "Validation loss: 1.3754396284997514....\n",
      "-----------------------------------\n",
      "Training loss: 1.3499187589163115....\n",
      "Validation loss: 1.3749323496200736....\n",
      "-----------------------------------\n",
      "Training loss: 1.3492529775784323....\n",
      "Validation loss: 1.3744251980116589....\n",
      "-----------------------------------\n",
      "Training loss: 1.348587785358689....\n",
      "Validation loss: 1.373919027587399....\n",
      "-----------------------------------\n",
      "Training loss: 1.3479234442734245....\n",
      "Validation loss: 1.3734132210689969....\n",
      "-----------------------------------\n",
      "Training loss: 1.3472597524123404....\n",
      "Validation loss: 1.3729081067714402....\n",
      "-----------------------------------\n",
      "Training loss: 1.346596562183525....\n",
      "Validation loss: 1.372403770933769....\n",
      "-----------------------------------\n",
      "Training loss: 1.3459343342148837....\n",
      "Validation loss: 1.3718997725844189....\n",
      "-----------------------------------\n",
      "Training loss: 1.3452729956528668....\n",
      "Validation loss: 1.3713968335761246....\n",
      "-----------------------------------\n",
      "Training loss: 1.3446122961553224....\n",
      "Validation loss: 1.37089415452779....\n",
      "-----------------------------------\n",
      "Training loss: 1.3439523726468172....\n",
      "Validation loss: 1.3703925432324637....\n",
      "-----------------------------------\n",
      "Training loss: 1.3432932948299108....\n",
      "Validation loss: 1.3698920356691475....\n",
      "-----------------------------------\n",
      "Training loss: 1.3426352008752591....\n",
      "Validation loss: 1.369391721692673....\n",
      "-----------------------------------\n",
      "Training loss: 1.3419777493417153....\n",
      "Validation loss: 1.3688920815919163....\n",
      "-----------------------------------\n",
      "Training loss: 1.3413210258390016....\n",
      "Validation loss: 1.3683928584332976....\n",
      "-----------------------------------\n",
      "Training loss: 1.340664962304714....\n",
      "Validation loss: 1.3678941105205247....\n",
      "-----------------------------------\n",
      "Training loss: 1.3400098431600553....\n",
      "Validation loss: 1.3673962463418876....\n",
      "-----------------------------------\n",
      "Training loss: 1.339355276291521....\n",
      "Validation loss: 1.36689894158575....\n",
      "-----------------------------------\n",
      "Training loss: 1.3387014646734048....\n",
      "Validation loss: 1.3664024366311993....\n",
      "-----------------------------------\n",
      "Training loss: 1.3380483920930095....\n",
      "Validation loss: 1.365906095095621....\n",
      "-----------------------------------\n",
      "Training loss: 1.3373959638142767....\n",
      "Validation loss: 1.3654108403395804....\n",
      "-----------------------------------\n",
      "Training loss: 1.3367442265039695....\n",
      "Validation loss: 1.3649156823415802....\n",
      "-----------------------------------\n",
      "Training loss: 1.3360929449464072....\n",
      "Validation loss: 1.3644208818073007....\n",
      "-----------------------------------\n",
      "Training loss: 1.335442168548445....\n",
      "Validation loss: 1.3639268719669253....\n",
      "-----------------------------------\n",
      "Training loss: 1.3347922234307052....\n",
      "Validation loss: 1.3634329352584442....\n",
      "-----------------------------------\n",
      "Training loss: 1.334142996071068....\n",
      "Validation loss: 1.3629397668109007....\n",
      "-----------------------------------\n",
      "Training loss: 1.3334941227695747....\n",
      "Validation loss: 1.3624469527017373....\n",
      "-----------------------------------\n",
      "Training loss: 1.3328460655316408....\n",
      "Validation loss: 1.361955167669333....\n",
      "-----------------------------------\n",
      "Training loss: 1.3321987670385285....\n",
      "Validation loss: 1.3614636621848588....\n",
      "-----------------------------------\n",
      "Training loss: 1.3315521431096338....\n",
      "Validation loss: 1.3609728534692331....\n",
      "-----------------------------------\n",
      "Training loss: 1.3309062931301376....\n",
      "Validation loss: 1.3604831144830525....\n",
      "-----------------------------------\n",
      "Training loss: 1.3302610350277853....\n",
      "Validation loss: 1.3599939138454757....\n",
      "-----------------------------------\n",
      "Training loss: 1.3296166088364794....\n",
      "Validation loss: 1.359506152872521....\n",
      "-----------------------------------\n",
      "Training loss: 1.328972937872533....\n",
      "Validation loss: 1.3590187686985598....\n",
      "-----------------------------------\n",
      "Training loss: 1.3283302735899387....\n",
      "Validation loss: 1.3585322830799336....\n",
      "-----------------------------------\n",
      "Training loss: 1.3276886025059953....\n",
      "Validation loss: 1.35804598747102....\n",
      "-----------------------------------\n",
      "Training loss: 1.327047473983292....\n",
      "Validation loss: 1.3575604729445014....\n",
      "-----------------------------------\n",
      "Training loss: 1.3264072803196625....\n",
      "Validation loss: 1.3570755364567604....\n",
      "-----------------------------------\n",
      "Training loss: 1.3257679396974564....\n",
      "Validation loss: 1.3565913794124453....\n",
      "-----------------------------------\n",
      "Training loss: 1.325129068309909....\n",
      "Validation loss: 1.3561082797023283....\n",
      "-----------------------------------\n",
      "Training loss: 1.3244905612967592....\n",
      "Validation loss: 1.3556256314467072....\n",
      "-----------------------------------\n",
      "Training loss: 1.323853018755825....\n",
      "Validation loss: 1.3551437311257768....\n",
      "-----------------------------------\n",
      "Training loss: 1.3232163502779477....\n",
      "Validation loss: 1.3546625027332602....\n",
      "-----------------------------------\n",
      "Training loss: 1.3225803903713027....\n",
      "Validation loss: 1.354181484375977....\n",
      "-----------------------------------\n",
      "Training loss: 1.3219451618201774....\n",
      "Validation loss: 1.3537007262392053....\n",
      "-----------------------------------\n",
      "Training loss: 1.3213106121728788....\n",
      "Validation loss: 1.3532209828220925....\n",
      "-----------------------------------\n",
      "Training loss: 1.3206766821456404....\n",
      "Validation loss: 1.3527413796626822....\n",
      "-----------------------------------\n",
      "Training loss: 1.32004337623864....\n",
      "Validation loss: 1.3522626789995247....\n",
      "-----------------------------------\n",
      "Training loss: 1.3194107978962981....\n",
      "Validation loss: 1.3517849648600073....\n",
      "-----------------------------------\n",
      "Training loss: 1.3187791617998643....\n",
      "Validation loss: 1.3513076385306104....\n",
      "-----------------------------------\n",
      "Training loss: 1.3181482871120662....\n",
      "Validation loss: 1.3508311690624428....\n",
      "-----------------------------------\n",
      "Training loss: 1.317517976981028....\n",
      "Validation loss: 1.3503547639120388....\n",
      "-----------------------------------\n",
      "Training loss: 1.3168882387517375....\n",
      "Validation loss: 1.3498791717633725....\n",
      "-----------------------------------\n",
      "Training loss: 1.316258997451389....\n",
      "Validation loss: 1.3494042287959946....\n",
      "-----------------------------------\n",
      "Training loss: 1.3156304851238172....\n",
      "Validation loss: 1.3489301865058791....\n",
      "-----------------------------------\n",
      "Training loss: 1.315003033931792....\n",
      "Validation loss: 1.3484565309407825....\n",
      "-----------------------------------\n",
      "Training loss: 1.3143763404482802....\n",
      "Validation loss: 1.3479832083430865....\n",
      "-----------------------------------\n",
      "Training loss: 1.3137503137542619....\n",
      "Validation loss: 1.3475099276840967....\n",
      "-----------------------------------\n",
      "Training loss: 1.31312476832129....\n",
      "Validation loss: 1.3470377378927785....\n",
      "-----------------------------------\n",
      "Training loss: 1.3124999879157162....\n",
      "Validation loss: 1.346566084481202....\n",
      "-----------------------------------\n",
      "Training loss: 1.311876068426337....\n",
      "Validation loss: 1.3460948906099302....\n",
      "-----------------------------------\n",
      "Training loss: 1.3112529624210802....\n",
      "Validation loss: 1.345624152697073....\n",
      "-----------------------------------\n",
      "Training loss: 1.3106303611460306....\n",
      "Validation loss: 1.345153729736293....\n",
      "-----------------------------------\n",
      "Training loss: 1.310008554642391....\n",
      "Validation loss: 1.3446838348784245....\n",
      "-----------------------------------\n",
      "Training loss: 1.3093875491622327....\n",
      "Validation loss: 1.3442145357691782....\n",
      "-----------------------------------\n",
      "Training loss: 1.308767310994691....\n",
      "Validation loss: 1.343745578664729....\n",
      "-----------------------------------\n",
      "Training loss: 1.3081478692540334....\n",
      "Validation loss: 1.3432770293423841....\n",
      "-----------------------------------\n",
      "Training loss: 1.3075287788824053....\n",
      "Validation loss: 1.342809130342173....\n",
      "-----------------------------------\n",
      "Training loss: 1.3069103776022952....\n",
      "Validation loss: 1.342341659212299....\n",
      "-----------------------------------\n",
      "Training loss: 1.3062927659640144....\n",
      "Validation loss: 1.3418746439255906....\n",
      "-----------------------------------\n",
      "Training loss: 1.3056759477979825....\n",
      "Validation loss: 1.3414083473114402....\n",
      "-----------------------------------\n",
      "Training loss: 1.30505983854482....\n",
      "Validation loss: 1.340942410537667....\n",
      "-----------------------------------\n",
      "Training loss: 1.3044442578795568....\n",
      "Validation loss: 1.340477426965136....\n",
      "-----------------------------------\n",
      "Training loss: 1.3038296157648708....\n",
      "Validation loss: 1.3400133494463093....\n",
      "-----------------------------------\n",
      "Training loss: 1.303215909926827....\n",
      "Validation loss: 1.3395498892658309....\n",
      "-----------------------------------\n",
      "Training loss: 1.302603197661416....\n",
      "Validation loss: 1.3390870601966263....\n",
      "-----------------------------------\n",
      "Training loss: 1.3019912290586....\n",
      "Validation loss: 1.3386245163364143....\n",
      "-----------------------------------\n",
      "Training loss: 1.3013798503262115....\n",
      "Validation loss: 1.3381624471053968....\n",
      "-----------------------------------\n",
      "Training loss: 1.3007692335068282....\n",
      "Validation loss: 1.3377009782017113....\n",
      "-----------------------------------\n",
      "Training loss: 1.3001590245832975....\n",
      "Validation loss: 1.337239805594265....\n",
      "-----------------------------------\n",
      "Training loss: 1.2995490928061493....\n",
      "Validation loss: 1.3367796573119013....\n",
      "-----------------------------------\n",
      "Training loss: 1.2989399440541785....\n",
      "Validation loss: 1.336320135329879....\n",
      "-----------------------------------\n",
      "Training loss: 1.2983316107967744....\n",
      "Validation loss: 1.335861295654261....\n",
      "-----------------------------------\n",
      "Training loss: 1.2977242028110583....\n",
      "Validation loss: 1.3354035839439757....\n",
      "-----------------------------------\n",
      "Training loss: 1.2971175866246036....\n",
      "Validation loss: 1.3349467862886546....\n",
      "-----------------------------------\n",
      "Training loss: 1.29651153833838....\n",
      "Validation loss: 1.3344906505846799....\n",
      "-----------------------------------\n",
      "Training loss: 1.2959061995295134....\n",
      "Validation loss: 1.334035125014443....\n",
      "-----------------------------------\n",
      "Training loss: 1.2953014868268347....\n",
      "Validation loss: 1.3335800010668966....\n",
      "-----------------------------------\n",
      "Training loss: 1.2946974302541723....\n",
      "Validation loss: 1.3331253085237686....\n",
      "-----------------------------------\n",
      "Training loss: 1.2940941722284804....\n",
      "Validation loss: 1.3326713952629234....\n",
      "-----------------------------------\n",
      "Training loss: 1.29349171683158....\n",
      "Validation loss: 1.332218066106772....\n",
      "-----------------------------------\n",
      "Training loss: 1.2928900047332745....\n",
      "Validation loss: 1.33176509945404....\n",
      "-----------------------------------\n",
      "Training loss: 1.2922890338218775....\n",
      "Validation loss: 1.3313130015027341....\n",
      "-----------------------------------\n",
      "Training loss: 1.2916887449261043....\n",
      "Validation loss: 1.3308608723723792....\n",
      "-----------------------------------\n",
      "Training loss: 1.2910889301701902....\n",
      "Validation loss: 1.3304093471422511....\n",
      "-----------------------------------\n",
      "Training loss: 1.2904895040734063....\n",
      "Validation loss: 1.3299577200300203....\n",
      "-----------------------------------\n",
      "Training loss: 1.2898906153103202....\n",
      "Validation loss: 1.3295066708545564....\n",
      "-----------------------------------\n",
      "Training loss: 1.2892923817783097....\n",
      "Validation loss: 1.3290565572975979....\n",
      "-----------------------------------\n",
      "Training loss: 1.288694887717494....\n",
      "Validation loss: 1.328607077494247....\n",
      "-----------------------------------\n",
      "Training loss: 1.2880980337595627....\n",
      "Validation loss: 1.3281579911469439....\n",
      "-----------------------------------\n",
      "Training loss: 1.2875018768157094....\n",
      "Validation loss: 1.3277098529245555....\n",
      "-----------------------------------\n",
      "Training loss: 1.2869064630502096....\n",
      "Validation loss: 1.3272621490398766....\n",
      "-----------------------------------\n",
      "Training loss: 1.286311540628646....\n",
      "Validation loss: 1.3268146991054124....\n",
      "-----------------------------------\n",
      "Training loss: 1.285717358764311....\n",
      "Validation loss: 1.3263676220681435....\n",
      "-----------------------------------\n",
      "Training loss: 1.285123856373424....\n",
      "Validation loss: 1.3259215263690332....\n",
      "-----------------------------------\n",
      "Training loss: 1.2845309162769187....\n",
      "Validation loss: 1.3254760836557953....\n",
      "-----------------------------------\n",
      "Training loss: 1.2839385950853284....\n",
      "Validation loss: 1.3250317430495333....\n",
      "-----------------------------------\n",
      "Training loss: 1.2833469116113674....\n",
      "Validation loss: 1.3245880382601605....\n",
      "-----------------------------------\n",
      "Training loss: 1.2827560216680056....\n",
      "Validation loss: 1.3241450152370697....\n",
      "-----------------------------------\n",
      "Training loss: 1.282165842447657....\n",
      "Validation loss: 1.3237027506433878....\n",
      "-----------------------------------\n",
      "Training loss: 1.2815764083925376....\n",
      "Validation loss: 1.3232611306899402....\n",
      "-----------------------------------\n",
      "Training loss: 1.2809877373928125....\n",
      "Validation loss: 1.3228196274713238....\n",
      "-----------------------------------\n",
      "Training loss: 1.2803996974908363....\n",
      "Validation loss: 1.3223786554043742....\n",
      "-----------------------------------\n",
      "Training loss: 1.2798124146538006....\n",
      "Validation loss: 1.3219376206084439....\n",
      "-----------------------------------\n",
      "Training loss: 1.2792258111875527....\n",
      "Validation loss: 1.3214971551881258....\n",
      "-----------------------------------\n",
      "Training loss: 1.2786400047844295....\n",
      "Validation loss: 1.3210569268361656....\n",
      "-----------------------------------\n",
      "Training loss: 1.2780546604836527....\n",
      "Validation loss: 1.3206171931278556....\n",
      "-----------------------------------\n",
      "Training loss: 1.277469598399634....\n",
      "Validation loss: 1.3201779021150413....\n",
      "-----------------------------------\n",
      "Training loss: 1.276885293683831....\n",
      "Validation loss: 1.3197394781923664....\n",
      "-----------------------------------\n",
      "Training loss: 1.2763015912488231....\n",
      "Validation loss: 1.3193018889528658....\n",
      "-----------------------------------\n",
      "Training loss: 1.2757184623868485....\n",
      "Validation loss: 1.3188648548165873....\n",
      "-----------------------------------\n",
      "Training loss: 1.2751359904538013....\n",
      "Validation loss: 1.3184283475384388....\n",
      "-----------------------------------\n",
      "Training loss: 1.2745541439662211....\n",
      "Validation loss: 1.317992180540631....\n",
      "-----------------------------------\n",
      "Training loss: 1.2739728510708888....\n",
      "Validation loss: 1.3175567983810834....\n",
      "-----------------------------------\n",
      "Training loss: 1.2733920526377636....\n",
      "Validation loss: 1.3171217553137387....\n",
      "-----------------------------------\n",
      "Training loss: 1.2728117520508324....\n",
      "Validation loss: 1.3166878526696941....\n",
      "-----------------------------------\n",
      "Training loss: 1.272232067206026....\n",
      "Validation loss: 1.316254496313839....\n",
      "-----------------------------------\n",
      "Training loss: 1.2716531252795347....\n",
      "Validation loss: 1.3158217476617342....\n",
      "-----------------------------------\n",
      "Training loss: 1.271074785825081....\n",
      "Validation loss: 1.3153897421825826....\n",
      "-----------------------------------\n",
      "Training loss: 1.2704971439246786....\n",
      "Validation loss: 1.3149581695347414....\n",
      "-----------------------------------\n",
      "Training loss: 1.2699201337441415....\n",
      "Validation loss: 1.3145272251283087....\n",
      "-----------------------------------\n",
      "Training loss: 1.2693437224939648....\n",
      "Validation loss: 1.314096577850497....\n",
      "-----------------------------------\n",
      "Training loss: 1.2687679447451872....\n",
      "Validation loss: 1.313666348786116....\n",
      "-----------------------------------\n",
      "Training loss: 1.2681927051025956....\n",
      "Validation loss: 1.31323636109253....\n",
      "-----------------------------------\n",
      "Training loss: 1.267617907542176....\n",
      "Validation loss: 1.3128072814418914....\n",
      "-----------------------------------\n",
      "Training loss: 1.2670436975550339....\n",
      "Validation loss: 1.3123788347924634....\n",
      "-----------------------------------\n",
      "Training loss: 1.2664703867591371....\n",
      "Validation loss: 1.311951043651427....\n",
      "-----------------------------------\n",
      "Training loss: 1.265897897291726....\n",
      "Validation loss: 1.311523994971201....\n",
      "-----------------------------------\n",
      "Training loss: 1.2653260408037732....\n",
      "Validation loss: 1.31109775058907....\n",
      "-----------------------------------\n",
      "Training loss: 1.2647549422963567....\n",
      "Validation loss: 1.3106721983793672....\n",
      "-----------------------------------\n",
      "Training loss: 1.2641845280797386....\n",
      "Validation loss: 1.3102475821573543....\n",
      "-----------------------------------\n",
      "Training loss: 1.2636150136994193....\n",
      "Validation loss: 1.3098236307267022....\n",
      "-----------------------------------\n",
      "Training loss: 1.2630462352020209....\n",
      "Validation loss: 1.3094005192106974....\n",
      "-----------------------------------\n",
      "Training loss: 1.2624779422259982....\n",
      "Validation loss: 1.3089779712315686....\n",
      "-----------------------------------\n",
      "Training loss: 1.2619102814851129....\n",
      "Validation loss: 1.3085560272444015....\n",
      "-----------------------------------\n",
      "Training loss: 1.2613432043275934....\n",
      "Validation loss: 1.3081349709818242....\n",
      "-----------------------------------\n",
      "Training loss: 1.260776848265369....\n",
      "Validation loss: 1.3077144752636427....\n",
      "-----------------------------------\n",
      "Training loss: 1.260210975550904....\n",
      "Validation loss: 1.3072946442714883....\n",
      "-----------------------------------\n",
      "Training loss: 1.2596458469432428....\n",
      "Validation loss: 1.3068754331950454....\n",
      "-----------------------------------\n",
      "Training loss: 1.259081502063094....\n",
      "Validation loss: 1.3064569751866102....\n",
      "-----------------------------------\n",
      "Training loss: 1.2585177923422481....\n",
      "Validation loss: 1.3060392113985972....\n",
      "-----------------------------------\n",
      "Training loss: 1.2579547817017087....\n",
      "Validation loss: 1.3056222313868666....\n",
      "-----------------------------------\n",
      "Training loss: 1.2573923772016127....\n",
      "Validation loss: 1.3052060131878085....\n",
      "-----------------------------------\n",
      "Training loss: 1.2568306767449078....\n",
      "Validation loss: 1.30479038335923....\n",
      "-----------------------------------\n",
      "Training loss: 1.2562694404270327....\n",
      "Validation loss: 1.3043751742331502....\n",
      "-----------------------------------\n",
      "Training loss: 1.2557087622727496....\n",
      "Validation loss: 1.3039603209117545....\n",
      "-----------------------------------\n",
      "Training loss: 1.2551486964970044....\n",
      "Validation loss: 1.3035457813210116....\n",
      "-----------------------------------\n",
      "Training loss: 1.2545893567437192....\n",
      "Validation loss: 1.3031318785823922....\n",
      "-----------------------------------\n",
      "Training loss: 1.25403077755323....\n",
      "Validation loss: 1.302718600481446....\n",
      "-----------------------------------\n",
      "Training loss: 1.2534729510670288....\n",
      "Validation loss: 1.302306119160068....\n",
      "-----------------------------------\n",
      "Training loss: 1.2529156245399287....\n",
      "Validation loss: 1.301894227869987....\n",
      "-----------------------------------\n",
      "Training loss: 1.2523590187367708....\n",
      "Validation loss: 1.301482656019791....\n",
      "-----------------------------------\n",
      "Training loss: 1.2518031130885223....\n",
      "Validation loss: 1.3010721439520148....\n",
      "-----------------------------------\n",
      "Training loss: 1.251247877738716....\n",
      "Validation loss: 1.3006619519595115....\n",
      "-----------------------------------\n",
      "Training loss: 1.2506933901595543....\n",
      "Validation loss: 1.3002525885703415....\n",
      "-----------------------------------\n",
      "Training loss: 1.2501395652767249....\n",
      "Validation loss: 1.2998433097953868....\n",
      "-----------------------------------\n",
      "Training loss: 1.2495864288700413....\n",
      "Validation loss: 1.299434383435159....\n",
      "-----------------------------------\n",
      "Training loss: 1.2490339247450208....\n",
      "Validation loss: 1.2990261637158786....\n",
      "-----------------------------------\n",
      "Training loss: 1.2484820713165574....\n",
      "Validation loss: 1.2986186609525339....\n",
      "-----------------------------------\n",
      "Training loss: 1.2479308913096763....\n",
      "Validation loss: 1.298211881245529....\n",
      "-----------------------------------\n",
      "Training loss: 1.2473803524738525....\n",
      "Validation loss: 1.2978054091158948....\n",
      "-----------------------------------\n",
      "Training loss: 1.2468303990478142....\n",
      "Validation loss: 1.2973992946604163....\n",
      "-----------------------------------\n",
      "Training loss: 1.2462813850452783....\n",
      "Validation loss: 1.2969937067877382....\n",
      "-----------------------------------\n",
      "Training loss: 1.2457329765426606....\n",
      "Validation loss: 1.2965883861031202....\n",
      "-----------------------------------\n",
      "Training loss: 1.2451850029134999....\n",
      "Validation loss: 1.296184063936534....\n",
      "-----------------------------------\n",
      "Training loss: 1.244637349201651....\n",
      "Validation loss: 1.29578009935619....\n",
      "-----------------------------------\n",
      "Training loss: 1.2440903350661372....\n",
      "Validation loss: 1.2953767690380034....\n",
      "-----------------------------------\n",
      "Training loss: 1.2435438349575332....\n",
      "Validation loss: 1.294973840901433....\n",
      "-----------------------------------\n",
      "Training loss: 1.2429980239257994....\n",
      "Validation loss: 1.2945715142698668....\n",
      "-----------------------------------\n",
      "Training loss: 1.2424530287591402....\n",
      "Validation loss: 1.2941698706656812....\n",
      "-----------------------------------\n",
      "Training loss: 1.2419088081487393....\n",
      "Validation loss: 1.2937692928422317....\n",
      "-----------------------------------\n",
      "Training loss: 1.2413652735397243....\n",
      "Validation loss: 1.2933689380800706....\n",
      "-----------------------------------\n",
      "Training loss: 1.2408222282993644....\n",
      "Validation loss: 1.2929692284694725....\n",
      "-----------------------------------\n",
      "Training loss: 1.2402798745053978....\n",
      "Validation loss: 1.2925702136635495....\n",
      "-----------------------------------\n",
      "Training loss: 1.2397380572236285....\n",
      "Validation loss: 1.2921713008716373....\n",
      "-----------------------------------\n",
      "Training loss: 1.2391967082792652....\n",
      "Validation loss: 1.2917730254711048....\n",
      "-----------------------------------\n",
      "Training loss: 1.2386559835802662....\n",
      "Validation loss: 1.291375256704345....\n",
      "-----------------------------------\n",
      "Training loss: 1.2381159641128678....\n",
      "Validation loss: 1.2909781074545046....\n",
      "-----------------------------------\n",
      "Training loss: 1.2375766056933981....\n",
      "Validation loss: 1.2905816724017267....\n",
      "-----------------------------------\n",
      "Training loss: 1.2370380823556726....\n",
      "Validation loss: 1.29018600980662....\n",
      "-----------------------------------\n",
      "Training loss: 1.2365002647355912....\n",
      "Validation loss: 1.289790642034163....\n",
      "-----------------------------------\n",
      "Training loss: 1.2359628797263442....\n",
      "Validation loss: 1.2893958503227205....\n",
      "-----------------------------------\n",
      "Training loss: 1.2354261882651165....\n",
      "Validation loss: 1.2890017193757026....\n",
      "-----------------------------------\n",
      "Training loss: 1.234890034889451....\n",
      "Validation loss: 1.2886080816545589....\n",
      "-----------------------------------\n",
      "Training loss: 1.2343544941147664....\n",
      "Validation loss: 1.2882151242869009....\n",
      "-----------------------------------\n",
      "Training loss: 1.2338194677665835....\n",
      "Validation loss: 1.2878225444636469....\n",
      "-----------------------------------\n",
      "Training loss: 1.233285016796028....\n",
      "Validation loss: 1.287430504390879....\n",
      "-----------------------------------\n",
      "Training loss: 1.2327511409024956....\n",
      "Validation loss: 1.2870393542307075....\n",
      "-----------------------------------\n",
      "Training loss: 1.2322177080453591....\n",
      "Validation loss: 1.286648454246772....\n",
      "-----------------------------------\n",
      "Training loss: 1.231684834116643....\n",
      "Validation loss: 1.286258275362165....\n",
      "-----------------------------------\n",
      "Training loss: 1.2311527335533305....\n",
      "Validation loss: 1.2858685095097138....\n",
      "-----------------------------------\n",
      "Training loss: 1.2306210124125836....\n",
      "Validation loss: 1.2854789319463056....\n",
      "-----------------------------------\n",
      "Training loss: 1.2300898800666662....\n",
      "Validation loss: 1.2850900875602707....\n",
      "-----------------------------------\n",
      "Training loss: 1.2295593085536356....\n",
      "Validation loss: 1.2847017338212852....\n",
      "-----------------------------------\n",
      "Training loss: 1.2290292592598129....\n",
      "Validation loss: 1.2843134285421225....\n",
      "-----------------------------------\n",
      "Training loss: 1.228499569139307....\n",
      "Validation loss: 1.2839262087254575....\n",
      "-----------------------------------\n",
      "Training loss: 1.227970345977551....\n",
      "Validation loss: 1.2835390650779697....\n",
      "-----------------------------------\n",
      "Training loss: 1.2274419921202129....\n",
      "Validation loss: 1.2831527860443483....\n",
      "-----------------------------------\n",
      "Training loss: 1.2269141912559554....\n",
      "Validation loss: 1.2827669698725932....\n",
      "-----------------------------------\n",
      "Training loss: 1.2263869716671287....\n",
      "Validation loss: 1.282381742012862....\n",
      "-----------------------------------\n",
      "Training loss: 1.225860355979666....\n",
      "Validation loss: 1.2819968316574502....\n",
      "-----------------------------------\n",
      "Training loss: 1.2253341717975834....\n",
      "Validation loss: 1.2816120694418995....\n",
      "-----------------------------------\n",
      "Training loss: 1.2248084095333056....\n",
      "Validation loss: 1.2812279109234017....\n",
      "-----------------------------------\n",
      "Training loss: 1.2242833577988221....\n",
      "Validation loss: 1.280844429183112....\n",
      "-----------------------------------\n",
      "Training loss: 1.223758996936412....\n",
      "Validation loss: 1.2804618412049662....\n",
      "-----------------------------------\n",
      "Training loss: 1.223235412040709....\n",
      "Validation loss: 1.2800798291003557....\n",
      "-----------------------------------\n",
      "Training loss: 1.2227124972802945....\n",
      "Validation loss: 1.279698226383782....\n",
      "-----------------------------------\n",
      "Training loss: 1.2221902072774014....\n",
      "Validation loss: 1.279317187435653....\n",
      "-----------------------------------\n",
      "Training loss: 1.2216681338486683....\n",
      "Validation loss: 1.2789368407256985....\n",
      "-----------------------------------\n",
      "Training loss: 1.2211465722388588....\n",
      "Validation loss: 1.2785569096959308....\n",
      "-----------------------------------\n",
      "Training loss: 1.2206257408845562....\n",
      "Validation loss: 1.278177892351912....\n",
      "-----------------------------------\n",
      "Training loss: 1.220105691834124....\n",
      "Validation loss: 1.2777991489510088....\n",
      "-----------------------------------\n",
      "Training loss: 1.2195862085812508....\n",
      "Validation loss: 1.2774209490132642....\n",
      "-----------------------------------\n",
      "Training loss: 1.2190669970543544....\n",
      "Validation loss: 1.277043360285053....\n",
      "-----------------------------------\n",
      "Training loss: 1.218548412072302....\n",
      "Validation loss: 1.276666199616312....\n",
      "-----------------------------------\n",
      "Training loss: 1.2180305108739815....\n",
      "Validation loss: 1.2762903089249444....\n",
      "-----------------------------------\n",
      "Training loss: 1.217513071342957....\n",
      "Validation loss: 1.2759149630989415....\n",
      "-----------------------------------\n",
      "Training loss: 1.2169965031554584....\n",
      "Validation loss: 1.2755406032219057....\n",
      "-----------------------------------\n",
      "Training loss: 1.2164805389083975....\n",
      "Validation loss: 1.2751666732849212....\n",
      "-----------------------------------\n",
      "Training loss: 1.2159651016399433....\n",
      "Validation loss: 1.274793385048944....\n",
      "-----------------------------------\n",
      "Training loss: 1.215450502890677....\n",
      "Validation loss: 1.2744203974955055....\n",
      "-----------------------------------\n",
      "Training loss: 1.214936473588137....\n",
      "Validation loss: 1.2740479612955673....\n",
      "-----------------------------------\n",
      "Training loss: 1.2144231492297848....\n",
      "Validation loss: 1.2736763280587686....\n",
      "-----------------------------------\n",
      "Training loss: 1.2139104117024488....\n",
      "Validation loss: 1.2733049559699021....\n",
      "-----------------------------------\n",
      "Training loss: 1.2133982366635108....\n",
      "Validation loss: 1.2729344668040394....\n",
      "-----------------------------------\n",
      "Training loss: 1.2128866269310399....\n",
      "Validation loss: 1.2725649945094417....\n",
      "-----------------------------------\n",
      "Training loss: 1.2123756249309368....\n",
      "Validation loss: 1.2721955538033327....\n",
      "-----------------------------------\n",
      "Training loss: 1.2118652809229933....\n",
      "Validation loss: 1.2718267423634744....\n",
      "-----------------------------------\n",
      "Training loss: 1.211355556034317....\n",
      "Validation loss: 1.271457950259174....\n",
      "-----------------------------------\n",
      "Training loss: 1.2108464772386989....\n",
      "Validation loss: 1.271090012684138....\n",
      "-----------------------------------\n",
      "Training loss: 1.2103380720405483....\n",
      "Validation loss: 1.2707225670334128....\n",
      "-----------------------------------\n",
      "Training loss: 1.209830398191463....\n",
      "Validation loss: 1.2703556334074406....\n",
      "-----------------------------------\n",
      "Training loss: 1.2093231803723976....\n",
      "Validation loss: 1.2699886640922438....\n",
      "-----------------------------------\n",
      "Training loss: 1.208816363584096....\n",
      "Validation loss: 1.2696222606137353....\n",
      "-----------------------------------\n",
      "Training loss: 1.2083099427619526....\n",
      "Validation loss: 1.2692561469783563....\n",
      "-----------------------------------\n",
      "Training loss: 1.2078039750735559....\n",
      "Validation loss: 1.2688907897486925....\n",
      "-----------------------------------\n",
      "Training loss: 1.2072986445661198....\n",
      "Validation loss: 1.2685258005216042....\n",
      "-----------------------------------\n",
      "Training loss: 1.2067938177807171....\n",
      "Validation loss: 1.2681614741926321....\n",
      "-----------------------------------\n",
      "Training loss: 1.2062895855229427....\n",
      "Validation loss: 1.2677976260659176....\n",
      "-----------------------------------\n",
      "Training loss: 1.2057857013022852....\n",
      "Validation loss: 1.26743393552607....\n",
      "-----------------------------------\n",
      "Training loss: 1.2052820376232702....\n",
      "Validation loss: 1.267070624124985....\n",
      "-----------------------------------\n",
      "Training loss: 1.2047789156236988....\n",
      "Validation loss: 1.266707994761948....\n",
      "-----------------------------------\n",
      "Training loss: 1.2042766574877628....\n",
      "Validation loss: 1.2663461704077168....\n",
      "-----------------------------------\n",
      "Training loss: 1.2037749997004894....\n",
      "Validation loss: 1.2659849392232214....\n",
      "-----------------------------------\n",
      "Training loss: 1.2032739062929718....\n",
      "Validation loss: 1.2656239880105884....\n",
      "-----------------------------------\n",
      "Training loss: 1.2027733515315213....\n",
      "Validation loss: 1.2652634608742523....\n",
      "-----------------------------------\n",
      "Training loss: 1.2022733301518975....\n",
      "Validation loss: 1.2649037966133627....\n",
      "-----------------------------------\n",
      "Training loss: 1.2017736974537387....\n",
      "Validation loss: 1.2645453169812413....\n",
      "-----------------------------------\n",
      "Training loss: 1.2012744364931647....\n",
      "Validation loss: 1.2641870264535688....\n",
      "-----------------------------------\n",
      "Training loss: 1.2007753792627083....\n",
      "Validation loss: 1.2638295300789892....\n",
      "-----------------------------------\n",
      "Training loss: 1.200276700350164....\n",
      "Validation loss: 1.2634722933042735....\n",
      "-----------------------------------\n",
      "Training loss: 1.199778208008328....\n",
      "Validation loss: 1.2631157665142407....\n",
      "-----------------------------------\n",
      "Training loss: 1.1992803620313093....\n",
      "Validation loss: 1.2627595596237877....\n",
      "-----------------------------------\n",
      "Training loss: 1.1987831042603496....\n",
      "Validation loss: 1.262403724093274....\n",
      "-----------------------------------\n",
      "Training loss: 1.1982862884767653....\n",
      "Validation loss: 1.2620488516868413....\n",
      "-----------------------------------\n",
      "Training loss: 1.1977900217042452....\n",
      "Validation loss: 1.261694397931779....\n",
      "-----------------------------------\n",
      "Training loss: 1.1972943632869761....\n",
      "Validation loss: 1.2613405315085762....\n",
      "-----------------------------------\n",
      "Training loss: 1.1967993252889617....\n",
      "Validation loss: 1.2609870595465664....\n",
      "-----------------------------------\n",
      "Training loss: 1.1963047695262108....\n",
      "Validation loss: 1.2606341626116473....\n",
      "-----------------------------------\n",
      "Training loss: 1.1958106378927236....\n",
      "Validation loss: 1.2602817390502208....\n",
      "-----------------------------------\n",
      "Training loss: 1.1953171684592103....\n",
      "Validation loss: 1.2599295601063223....\n",
      "-----------------------------------\n",
      "Training loss: 1.1948242748282107....\n",
      "Validation loss: 1.2595778337370844....\n",
      "-----------------------------------\n",
      "Training loss: 1.1943320159900983....\n",
      "Validation loss: 1.2592266565871437....\n",
      "-----------------------------------\n",
      "Training loss: 1.1938403968718563....\n",
      "Validation loss: 1.25887596164222....\n",
      "-----------------------------------\n",
      "Training loss: 1.1933493892276918....\n",
      "Validation loss: 1.2585261018299918....\n",
      "-----------------------------------\n",
      "Training loss: 1.1928589040869342....\n",
      "Validation loss: 1.258176675519423....\n",
      "-----------------------------------\n",
      "Training loss: 1.192368789763416....\n",
      "Validation loss: 1.257827948184932....\n",
      "-----------------------------------\n",
      "Training loss: 1.191879064155311....\n",
      "Validation loss: 1.2574796225028448....\n",
      "-----------------------------------\n",
      "Training loss: 1.191389922643571....\n",
      "Validation loss: 1.2571315175770978....\n",
      "-----------------------------------\n",
      "Training loss: 1.190901180383542....\n",
      "Validation loss: 1.2567843053036032....\n",
      "-----------------------------------\n",
      "Training loss: 1.1904130684862113....\n",
      "Validation loss: 1.2564374302319847....\n",
      "-----------------------------------\n",
      "Training loss: 1.1899255939331628....\n",
      "Validation loss: 1.2560909574829267....\n",
      "-----------------------------------\n",
      "Training loss: 1.189438848843186....\n",
      "Validation loss: 1.2557448091438674....\n",
      "-----------------------------------\n",
      "Training loss: 1.1889527367354555....\n",
      "Validation loss: 1.2553996546522....\n",
      "-----------------------------------\n",
      "Training loss: 1.1884670810860545....\n",
      "Validation loss: 1.25505494411985....\n",
      "-----------------------------------\n",
      "Training loss: 1.1879819616578133....\n",
      "Validation loss: 1.25471087600178....\n",
      "-----------------------------------\n",
      "Training loss: 1.1874973627321928....\n",
      "Validation loss: 1.2543670432522718....\n",
      "-----------------------------------\n",
      "Training loss: 1.1870133325790873....\n",
      "Validation loss: 1.25402412192662....\n",
      "-----------------------------------\n",
      "Training loss: 1.1865300764195958....\n",
      "Validation loss: 1.2536817819273638....\n",
      "-----------------------------------\n",
      "Training loss: 1.1860473624779024....\n",
      "Validation loss: 1.2533398299579808....\n",
      "-----------------------------------\n",
      "Training loss: 1.18556532543635....\n",
      "Validation loss: 1.2529985734792803....\n",
      "-----------------------------------\n",
      "Training loss: 1.1850837003733365....\n",
      "Validation loss: 1.252657019175336....\n",
      "-----------------------------------\n",
      "Training loss: 1.1846026097276918....\n",
      "Validation loss: 1.2523163584256207....\n",
      "-----------------------------------\n",
      "Training loss: 1.1841219094089916....\n",
      "Validation loss: 1.2519760203634747....\n",
      "-----------------------------------\n",
      "Training loss: 1.1836416682121007....\n",
      "Validation loss: 1.2516364243838785....\n",
      "-----------------------------------\n",
      "Training loss: 1.183162017454046....\n",
      "Validation loss: 1.251297228290936....\n",
      "-----------------------------------\n",
      "Training loss: 1.1826828208036697....\n",
      "Validation loss: 1.250958569972774....\n",
      "-----------------------------------\n",
      "Training loss: 1.1822039615276438....\n",
      "Validation loss: 1.2506202946255394....\n",
      "-----------------------------------\n",
      "Training loss: 1.1817256361818784....\n",
      "Validation loss: 1.2502822479633096....\n",
      "-----------------------------------\n",
      "Training loss: 1.1812479204376165....\n",
      "Validation loss: 1.2499446970304489....\n",
      "-----------------------------------\n",
      "Training loss: 1.1807709156555297....\n",
      "Validation loss: 1.2496076296364014....\n",
      "-----------------------------------\n",
      "Training loss: 1.180294414214854....\n",
      "Validation loss: 1.2492706659931299....\n",
      "-----------------------------------\n",
      "Training loss: 1.179818504122046....\n",
      "Validation loss: 1.2489341159330665....\n",
      "-----------------------------------\n",
      "Training loss: 1.1793429320378488....\n",
      "Validation loss: 1.2485979398298876....\n",
      "-----------------------------------\n",
      "Training loss: 1.1788677619857537....\n",
      "Validation loss: 1.2482623089663982....\n",
      "-----------------------------------\n",
      "Training loss: 1.1783928827446535....\n",
      "Validation loss: 1.2479270560801015....\n",
      "-----------------------------------\n",
      "Training loss: 1.177918451880845....\n",
      "Validation loss: 1.2475925373908168....\n",
      "-----------------------------------\n",
      "Training loss: 1.1774445691837643....\n",
      "Validation loss: 1.247258785936315....\n",
      "-----------------------------------\n",
      "Training loss: 1.176971177407013....\n",
      "Validation loss: 1.2469256734066727....\n",
      "-----------------------------------\n",
      "Training loss: 1.1764983615786602....\n",
      "Validation loss: 1.24659307345173....\n",
      "-----------------------------------\n",
      "Training loss: 1.176026131186323....\n",
      "Validation loss: 1.2462610236907363....\n",
      "-----------------------------------\n",
      "Training loss: 1.1755542124044123....\n",
      "Validation loss: 1.2459296825670354....\n",
      "-----------------------------------\n",
      "Training loss: 1.1750827447055514....\n",
      "Validation loss: 1.2455985125888982....\n",
      "-----------------------------------\n",
      "Training loss: 1.1746118292759131....\n",
      "Validation loss: 1.2452677837572956....\n",
      "-----------------------------------\n",
      "Training loss: 1.1741415863488192....\n",
      "Validation loss: 1.2449373306834755....\n",
      "-----------------------------------\n",
      "Training loss: 1.173671961159876....\n",
      "Validation loss: 1.2446075569316168....\n",
      "-----------------------------------\n",
      "Training loss: 1.1732027643409157....\n",
      "Validation loss: 1.2442782802791166....\n",
      "-----------------------------------\n",
      "Training loss: 1.1727339602816325....\n",
      "Validation loss: 1.2439491155344267....\n",
      "-----------------------------------\n",
      "Training loss: 1.1722653816516178....\n",
      "Validation loss: 1.243620342695101....\n",
      "-----------------------------------\n",
      "Training loss: 1.171797267106038....\n",
      "Validation loss: 1.243291925001321....\n",
      "-----------------------------------\n",
      "Training loss: 1.1713295309391674....\n",
      "Validation loss: 1.2429640589995068....\n",
      "-----------------------------------\n",
      "Training loss: 1.1708622699882039....\n",
      "Validation loss: 1.2426366116287464....\n",
      "-----------------------------------\n",
      "Training loss: 1.1703956017390897....\n",
      "Validation loss: 1.2423095354990132....\n",
      "-----------------------------------\n",
      "Training loss: 1.1699295206930285....\n",
      "Validation loss: 1.241982991629407....\n",
      "-----------------------------------\n",
      "Training loss: 1.1694640351442145....\n",
      "Validation loss: 1.2416569407851763....\n",
      "-----------------------------------\n",
      "Training loss: 1.1689991428768698....\n",
      "Validation loss: 1.2413313978417646....\n",
      "-----------------------------------\n",
      "Training loss: 1.1685348731649432....\n",
      "Validation loss: 1.241006170295733....\n",
      "-----------------------------------\n",
      "Training loss: 1.1680710615210745....\n",
      "Validation loss: 1.240680990928444....\n",
      "-----------------------------------\n",
      "Training loss: 1.1676076231469177....\n",
      "Validation loss: 1.2403564804135905....\n",
      "-----------------------------------\n",
      "Training loss: 1.1671446153133815....\n",
      "Validation loss: 1.2400328073978863....\n",
      "-----------------------------------\n",
      "Training loss: 1.166682301774356....\n",
      "Validation loss: 1.239709431487642....\n",
      "-----------------------------------\n",
      "Training loss: 1.166220519710634....\n",
      "Validation loss: 1.2393865312458598....\n",
      "-----------------------------------\n",
      "Training loss: 1.1657591758962798....\n",
      "Validation loss: 1.239064243386596....\n",
      "-----------------------------------\n",
      "Training loss: 1.1652979235496497....\n",
      "Validation loss: 1.2387422093908542....\n",
      "-----------------------------------\n",
      "Training loss: 1.1648370364365068....\n",
      "Validation loss: 1.2384207396898694....\n",
      "-----------------------------------\n",
      "Training loss: 1.164376637159023....\n",
      "Validation loss: 1.2380991782136834....\n",
      "-----------------------------------\n",
      "Training loss: 1.1639167659327483....\n",
      "Validation loss: 1.2377782384268412....\n",
      "-----------------------------------\n",
      "Training loss: 1.1634576735741413....\n",
      "Validation loss: 1.2374577150724373....\n",
      "-----------------------------------\n",
      "Training loss: 1.1629992175622648....\n",
      "Validation loss: 1.2371379041105985....\n",
      "-----------------------------------\n",
      "Training loss: 1.1625412283617336....\n",
      "Validation loss: 1.2368186528540526....\n",
      "-----------------------------------\n",
      "Training loss: 1.1620837887738789....\n",
      "Validation loss: 1.2365004095568706....\n",
      "-----------------------------------\n",
      "Training loss: 1.1616268154787786....\n",
      "Validation loss: 1.2361825514789322....\n",
      "-----------------------------------\n",
      "Training loss: 1.1611703640456832....\n",
      "Validation loss: 1.2358654107715001....\n",
      "-----------------------------------\n",
      "Training loss: 1.1607144604651523....\n",
      "Validation loss: 1.235548803817298....\n",
      "-----------------------------------\n",
      "Training loss: 1.1602589845585198....\n",
      "Validation loss: 1.235232636164091....\n",
      "-----------------------------------\n",
      "Training loss: 1.159804110755203....\n",
      "Validation loss: 1.2349167103562035....\n",
      "-----------------------------------\n",
      "Training loss: 1.1593497725952362....\n",
      "Validation loss: 1.2346015048317731....\n",
      "-----------------------------------\n",
      "Training loss: 1.1588961107162352....\n",
      "Validation loss: 1.2342864810495944....\n",
      "-----------------------------------\n",
      "Training loss: 1.1584430014494982....\n",
      "Validation loss: 1.2339719715738235....\n",
      "-----------------------------------\n",
      "Training loss: 1.1579904101458836....\n",
      "Validation loss: 1.233658203435808....\n",
      "-----------------------------------\n",
      "Training loss: 1.1575384855199777....\n",
      "Validation loss: 1.2333446006208586....\n",
      "-----------------------------------\n",
      "Training loss: 1.1570872025064038....\n",
      "Validation loss: 1.2330314225557544....\n",
      "-----------------------------------\n",
      "Training loss: 1.1566363836686713....\n",
      "Validation loss: 1.232718713201898....\n",
      "-----------------------------------\n",
      "Training loss: 1.1561859789643523....\n",
      "Validation loss: 1.2324067562218566....\n",
      "-----------------------------------\n",
      "Training loss: 1.1557360840955653....\n",
      "Validation loss: 1.2320950446463754....\n",
      "-----------------------------------\n",
      "Training loss: 1.155286799796107....\n",
      "Validation loss: 1.2317841987953435....\n",
      "-----------------------------------\n",
      "Training loss: 1.1548380779225325....\n",
      "Validation loss: 1.2314737781014717....\n",
      "-----------------------------------\n",
      "Training loss: 1.1543897534414316....\n",
      "Validation loss: 1.2311636727269648....\n",
      "-----------------------------------\n",
      "Training loss: 1.1539419716016186....\n",
      "Validation loss: 1.2308538780521279....\n",
      "-----------------------------------\n",
      "Training loss: 1.1534946620092152....\n",
      "Validation loss: 1.2305440817449902....\n",
      "-----------------------------------\n",
      "Training loss: 1.1530479242863467....\n",
      "Validation loss: 1.2302351703162748....\n",
      "-----------------------------------\n",
      "Training loss: 1.1526016356214885....\n",
      "Validation loss: 1.2299262692802215....\n",
      "-----------------------------------\n",
      "Training loss: 1.1521558380484693....\n",
      "Validation loss: 1.2296177252422575....\n",
      "-----------------------------------\n",
      "Training loss: 1.1517106506943289....\n",
      "Validation loss: 1.2293092893005568....\n",
      "-----------------------------------\n",
      "Training loss: 1.1512659633862732....\n",
      "Validation loss: 1.2290016644396828....\n",
      "-----------------------------------\n",
      "Training loss: 1.150821809240624....\n",
      "Validation loss: 1.228694363186662....\n",
      "-----------------------------------\n",
      "Training loss: 1.1503781691211543....\n",
      "Validation loss: 1.2283879455788749....\n",
      "-----------------------------------\n",
      "Training loss: 1.1499350166689888....\n",
      "Validation loss: 1.2280818478220945....\n",
      "-----------------------------------\n",
      "Training loss: 1.1494923814089864....\n",
      "Validation loss: 1.2277768033121943....\n",
      "-----------------------------------\n",
      "Training loss: 1.149050514040994....\n",
      "Validation loss: 1.227471975748754....\n",
      "-----------------------------------\n",
      "Training loss: 1.14860941015608....\n",
      "Validation loss: 1.2271674800801604....\n",
      "-----------------------------------\n",
      "Training loss: 1.1481687006725165....\n",
      "Validation loss: 1.2268635038587565....\n",
      "-----------------------------------\n",
      "Training loss: 1.1477282330350702....\n",
      "Validation loss: 1.2265602102280337....\n",
      "-----------------------------------\n",
      "Training loss: 1.1472882757168825....\n",
      "Validation loss: 1.2262571776306321....\n",
      "-----------------------------------\n",
      "Training loss: 1.1468490131208524....\n",
      "Validation loss: 1.2259547363982097....\n",
      "-----------------------------------\n",
      "Training loss: 1.1464103381451765....\n",
      "Validation loss: 1.225652464611163....\n",
      "-----------------------------------\n",
      "Training loss: 1.1459722242091426....\n",
      "Validation loss: 1.2253507041401568....\n",
      "-----------------------------------\n",
      "Training loss: 1.1455347388112493....\n",
      "Validation loss: 1.2250494533680343....\n",
      "-----------------------------------\n",
      "Training loss: 1.1450979522478346....\n",
      "Validation loss: 1.2247488950033838....\n",
      "-----------------------------------\n",
      "Training loss: 1.1446616480035958....\n",
      "Validation loss: 1.2244483698365507....\n",
      "-----------------------------------\n",
      "Training loss: 1.1442257371431896....\n",
      "Validation loss: 1.224148356935145....\n",
      "-----------------------------------\n",
      "Training loss: 1.1437902382187308....\n",
      "Validation loss: 1.2238487489891632....\n",
      "-----------------------------------\n",
      "Training loss: 1.143355112160271....\n",
      "Validation loss: 1.2235499572183295....\n",
      "-----------------------------------\n",
      "Training loss: 1.1429204051212531....\n",
      "Validation loss: 1.2232514249843573....\n",
      "-----------------------------------\n",
      "Training loss: 1.1424860792988965....\n",
      "Validation loss: 1.2229537085695215....\n",
      "-----------------------------------\n",
      "Training loss: 1.1420520819926832....\n",
      "Validation loss: 1.2226562675238004....\n",
      "-----------------------------------\n",
      "Training loss: 1.1416186488953464....\n",
      "Validation loss: 1.2223593173075082....\n",
      "-----------------------------------\n",
      "Training loss: 1.14118584324923....\n",
      "Validation loss: 1.2220631069713732....\n",
      "-----------------------------------\n",
      "Training loss: 1.2013883676410246....\n",
      "Validation loss: 1.1005116777488044....\n",
      "-----------------------------------\n",
      "Training loss: 1.200869358696356....\n",
      "Validation loss: 1.1002519952438803....\n",
      "-----------------------------------\n",
      "Training loss: 1.200359737607628....\n",
      "Validation loss: 1.099994164853919....\n",
      "-----------------------------------\n",
      "Training loss: 1.1998575522337425....\n",
      "Validation loss: 1.0997376527749247....\n",
      "-----------------------------------\n",
      "Training loss: 1.1993616724494616....\n",
      "Validation loss: 1.0994820805747805....\n",
      "-----------------------------------\n",
      "Training loss: 1.1988714494693795....\n",
      "Validation loss: 1.0992268586080973....\n",
      "-----------------------------------\n",
      "Training loss: 1.1983860574140945....\n",
      "Validation loss: 1.0989716666782388....\n",
      "-----------------------------------\n",
      "Training loss: 1.197904104045459....\n",
      "Validation loss: 1.098716262352658....\n",
      "-----------------------------------\n",
      "Training loss: 1.197425444683801....\n",
      "Validation loss: 1.0984601097432023....\n",
      "-----------------------------------\n",
      "Training loss: 1.1969496050826105....\n",
      "Validation loss: 1.0982034399457834....\n",
      "-----------------------------------\n",
      "Training loss: 1.1964761883082533....\n",
      "Validation loss: 1.0979460869817168....\n",
      "-----------------------------------\n",
      "Training loss: 1.1960048785308777....\n",
      "Validation loss: 1.0976880635377586....\n",
      "-----------------------------------\n",
      "Training loss: 1.195535537368805....\n",
      "Validation loss: 1.097429544108976....\n",
      "-----------------------------------\n",
      "Training loss: 1.1950682284298204....\n",
      "Validation loss: 1.0971705710194157....\n",
      "-----------------------------------\n",
      "Training loss: 1.194602502462594....\n",
      "Validation loss: 1.096911328253627....\n",
      "-----------------------------------\n",
      "Training loss: 1.1941382831660599....\n",
      "Validation loss: 1.0966515856065493....\n",
      "-----------------------------------\n",
      "Training loss: 1.193675280118401....\n",
      "Validation loss: 1.0963915207349455....\n",
      "-----------------------------------\n",
      "Training loss: 1.1932135274587046....\n",
      "Validation loss: 1.0961308614727905....\n",
      "-----------------------------------\n",
      "Training loss: 1.1927530456452882....\n",
      "Validation loss: 1.0958698740942754....\n",
      "-----------------------------------\n",
      "Training loss: 1.192293585638095....\n",
      "Validation loss: 1.0956085468241024....\n",
      "-----------------------------------\n",
      "Training loss: 1.1918351242694398....\n",
      "Validation loss: 1.0953468232331511....\n",
      "-----------------------------------\n",
      "Training loss: 1.1913775446794408....\n",
      "Validation loss: 1.0950850960701093....\n",
      "-----------------------------------\n",
      "Training loss: 1.190920734873473....\n",
      "Validation loss: 1.0948230608690326....\n",
      "-----------------------------------\n",
      "Training loss: 1.1904644428036877....\n",
      "Validation loss: 1.0945606311422604....\n",
      "-----------------------------------\n",
      "Training loss: 1.190009025538831....\n",
      "Validation loss: 1.094298158166669....\n",
      "-----------------------------------\n",
      "Training loss: 1.1895541596993784....\n",
      "Validation loss: 1.0940356084768228....\n",
      "-----------------------------------\n",
      "Training loss: 1.1891001316597978....\n",
      "Validation loss: 1.0937729093135211....\n",
      "-----------------------------------\n",
      "Training loss: 1.1886468775434393....\n",
      "Validation loss: 1.0935102954749458....\n",
      "-----------------------------------\n",
      "Training loss: 1.1881943589974622....\n",
      "Validation loss: 1.0932477796444429....\n",
      "-----------------------------------\n",
      "Training loss: 1.1877425900639609....\n",
      "Validation loss: 1.0929852629922494....\n",
      "-----------------------------------\n",
      "Training loss: 1.1872915194341387....\n",
      "Validation loss: 1.092722918110887....\n",
      "-----------------------------------\n",
      "Training loss: 1.18684101589211....\n",
      "Validation loss: 1.0924604970360041....\n",
      "-----------------------------------\n",
      "Training loss: 1.1863911814756753....\n",
      "Validation loss: 1.0921984585937197....\n",
      "-----------------------------------\n",
      "Training loss: 1.1859418262525496....\n",
      "Validation loss: 1.0919365739576674....\n",
      "-----------------------------------\n",
      "Training loss: 1.1854931367601516....\n",
      "Validation loss: 1.0916748323942154....\n",
      "-----------------------------------\n",
      "Training loss: 1.1850451950967134....\n",
      "Validation loss: 1.0914134064376722....\n",
      "-----------------------------------\n",
      "Training loss: 1.1845978236717867....\n",
      "Validation loss: 1.0911520818241536....\n",
      "-----------------------------------\n",
      "Training loss: 1.1841511178422106....\n",
      "Validation loss: 1.0908910540459238....\n",
      "-----------------------------------\n",
      "Training loss: 1.1837050014237693....\n",
      "Validation loss: 1.090630344582059....\n",
      "-----------------------------------\n",
      "Training loss: 1.1832595462726407....\n",
      "Validation loss: 1.0903698166415328....\n",
      "-----------------------------------\n",
      "Training loss: 1.1828146026574682....\n",
      "Validation loss: 1.0901099523851778....\n",
      "-----------------------------------\n",
      "Training loss: 1.1823702199571886....\n",
      "Validation loss: 1.0898503312357577....\n",
      "-----------------------------------\n",
      "Training loss: 1.1819265509445396....\n",
      "Validation loss: 1.089590908347119....\n",
      "-----------------------------------\n",
      "Training loss: 1.1814835084080646....\n",
      "Validation loss: 1.0893317502813387....\n",
      "-----------------------------------\n",
      "Training loss: 1.1810410594941791....\n",
      "Validation loss: 1.0890730109577995....\n",
      "-----------------------------------\n",
      "Training loss: 1.1805992241096808....\n",
      "Validation loss: 1.088814510512475....\n",
      "-----------------------------------\n",
      "Training loss: 1.180157992962501....\n",
      "Validation loss: 1.0885565265824293....\n",
      "-----------------------------------\n",
      "Training loss: 1.1797172697523375....\n",
      "Validation loss: 1.0882984709251735....\n",
      "-----------------------------------\n",
      "Training loss: 1.1792771128392119....\n",
      "Validation loss: 1.0880408558214791....\n",
      "-----------------------------------\n",
      "Training loss: 1.17883755292199....\n",
      "Validation loss: 1.0877834485643405....\n",
      "-----------------------------------\n",
      "Training loss: 1.178398343622911....\n",
      "Validation loss: 1.0875262580539373....\n",
      "-----------------------------------\n",
      "Training loss: 1.1779596704860225....\n",
      "Validation loss: 1.0872691879678869....\n",
      "-----------------------------------\n",
      "Training loss: 1.1775215714735636....\n",
      "Validation loss: 1.087012644181342....\n",
      "-----------------------------------\n",
      "Training loss: 1.1770840554597297....\n",
      "Validation loss: 1.0867564164671546....\n",
      "-----------------------------------\n",
      "Training loss: 1.1766470366550967....\n",
      "Validation loss: 1.086500422245222....\n",
      "-----------------------------------\n",
      "Training loss: 1.1762105916125385....\n",
      "Validation loss: 1.0862448706588301....\n",
      "-----------------------------------\n",
      "Training loss: 1.1757747074097056....\n",
      "Validation loss: 1.0859897552516338....\n",
      "-----------------------------------\n",
      "Training loss: 1.1753394423532486....\n",
      "Validation loss: 1.0857349592215009....\n",
      "-----------------------------------\n",
      "Training loss: 1.1749048332531533....\n",
      "Validation loss: 1.0854805412793....\n",
      "-----------------------------------\n",
      "Training loss: 1.1744707930443619....\n",
      "Validation loss: 1.0852265313199043....\n",
      "-----------------------------------\n",
      "Training loss: 1.1740373608804335....\n",
      "Validation loss: 1.0849729352902795....\n",
      "-----------------------------------\n",
      "Training loss: 1.17360455822829....\n",
      "Validation loss: 1.084719856341724....\n",
      "-----------------------------------\n",
      "Training loss: 1.173172299591231....\n",
      "Validation loss: 1.0844670416656983....\n",
      "-----------------------------------\n",
      "Training loss: 1.1727405392996346....\n",
      "Validation loss: 1.084214871970332....\n",
      "-----------------------------------\n",
      "Training loss: 1.1723092516485099....\n",
      "Validation loss: 1.0839632467255198....\n",
      "-----------------------------------\n",
      "Training loss: 1.1718781622015448....\n",
      "Validation loss: 1.0837118001877089....\n",
      "-----------------------------------\n",
      "Training loss: 1.1714477235462604....\n",
      "Validation loss: 1.0834608810114514....\n",
      "-----------------------------------\n",
      "Training loss: 1.17101797045286....\n",
      "Validation loss: 1.0832103777639264....\n",
      "-----------------------------------\n",
      "Training loss: 1.170588868880855....\n",
      "Validation loss: 1.0829599593646864....\n",
      "-----------------------------------\n",
      "Training loss: 1.1701605062132234....\n",
      "Validation loss: 1.0827101545350775....\n",
      "-----------------------------------\n",
      "Training loss: 1.1697325585546852....\n",
      "Validation loss: 1.082460676426928....\n",
      "-----------------------------------\n",
      "Training loss: 1.1693050879119582....\n",
      "Validation loss: 1.082211414640954....\n",
      "-----------------------------------\n",
      "Training loss: 1.168878357872305....\n",
      "Validation loss: 1.081962619431853....\n",
      "-----------------------------------\n",
      "Training loss: 1.1684521687563847....\n",
      "Validation loss: 1.0817142061599847....\n",
      "-----------------------------------\n",
      "Training loss: 1.168026551718902....\n",
      "Validation loss: 1.0814659512910918....\n",
      "-----------------------------------\n",
      "Training loss: 1.1676014973900823....\n",
      "Validation loss: 1.0812182571636952....\n",
      "-----------------------------------\n",
      "Training loss: 1.167176985730503....\n",
      "Validation loss: 1.080970767670814....\n",
      "-----------------------------------\n",
      "Training loss: 1.1667530620945175....\n",
      "Validation loss: 1.080723860299331....\n",
      "-----------------------------------\n",
      "Training loss: 1.1663298537225384....\n",
      "Validation loss: 1.0804774652312268....\n",
      "-----------------------------------\n",
      "Training loss: 1.1659073890844256....\n",
      "Validation loss: 1.0802313889712971....\n",
      "-----------------------------------\n",
      "Training loss: 1.1654854859593857....\n",
      "Validation loss: 1.079985786798589....\n",
      "-----------------------------------\n",
      "Training loss: 1.1650641586273531....\n",
      "Validation loss: 1.0797406560769018....\n",
      "-----------------------------------\n",
      "Training loss: 1.1646433877963125....\n",
      "Validation loss: 1.0794959330434366....\n",
      "-----------------------------------\n",
      "Training loss: 1.1642230294258615....\n",
      "Validation loss: 1.079251434936953....\n",
      "-----------------------------------\n",
      "Training loss: 1.1638032911496077....\n",
      "Validation loss: 1.0790074412780442....\n",
      "-----------------------------------\n",
      "Training loss: 1.1633840635947108....\n",
      "Validation loss: 1.0787638510383526....\n",
      "-----------------------------------\n",
      "Training loss: 1.1629653968921663....\n",
      "Validation loss: 1.0785205844177006....\n",
      "-----------------------------------\n",
      "Training loss: 1.162547393063266....\n",
      "Validation loss: 1.0782775355416636....\n",
      "-----------------------------------\n",
      "Training loss: 1.162129984847076....\n",
      "Validation loss: 1.0780350599226163....\n",
      "-----------------------------------\n",
      "Training loss: 1.1617131533878042....\n",
      "Validation loss: 1.0777929263952342....\n",
      "-----------------------------------\n",
      "Training loss: 1.1612969035385854....\n",
      "Validation loss: 1.0775511959513853....\n",
      "-----------------------------------\n",
      "Training loss: 1.160881074715416....\n",
      "Validation loss: 1.0773096312614514....\n",
      "-----------------------------------\n",
      "Training loss: 1.1604657590417164....\n",
      "Validation loss: 1.0770686265693392....\n",
      "-----------------------------------\n",
      "Training loss: 1.160050972217947....\n",
      "Validation loss: 1.0768280140371225....\n",
      "-----------------------------------\n",
      "Training loss: 1.1596366967502567....\n",
      "Validation loss: 1.0765876812308146....\n",
      "-----------------------------------\n",
      "Training loss: 1.1592228226328216....\n",
      "Validation loss: 1.0763479128724345....\n",
      "-----------------------------------\n",
      "Training loss: 1.1588092655307713....\n",
      "Validation loss: 1.0761082833146158....\n",
      "-----------------------------------\n",
      "Training loss: 1.1583961493133346....\n",
      "Validation loss: 1.0758692810180301....\n",
      "-----------------------------------\n",
      "Training loss: 1.1579834984587924....\n",
      "Validation loss: 1.0756306754627902....\n",
      "-----------------------------------\n",
      "Training loss: 1.1575711816304075....\n",
      "Validation loss: 1.075392148796069....\n",
      "-----------------------------------\n",
      "Training loss: 1.1571592829957984....\n",
      "Validation loss: 1.0751541503955082....\n",
      "-----------------------------------\n",
      "Training loss: 1.1567478465806331....\n",
      "Validation loss: 1.0749163250958926....\n",
      "-----------------------------------\n",
      "Training loss: 1.1563367527825716....\n",
      "Validation loss: 1.0746790970733502....\n",
      "-----------------------------------\n",
      "Training loss: 1.1559261172759274....\n",
      "Validation loss: 1.0744419363733062....\n",
      "-----------------------------------\n",
      "Training loss: 1.155516138815373....\n",
      "Validation loss: 1.0742054372731367....\n",
      "-----------------------------------\n",
      "Training loss: 1.1551065920906773....\n",
      "Validation loss: 1.0739692363427935....\n",
      "-----------------------------------\n",
      "Training loss: 1.1546974732252224....\n",
      "Validation loss: 1.0737333652673908....\n",
      "-----------------------------------\n",
      "Training loss: 1.1542888637827915....\n",
      "Validation loss: 1.0734981736068612....\n",
      "-----------------------------------\n",
      "Training loss: 1.153880753929109....\n",
      "Validation loss: 1.073263375352798....\n",
      "-----------------------------------\n",
      "Training loss: 1.153473135853447....\n",
      "Validation loss: 1.073029012048288....\n",
      "-----------------------------------\n",
      "Training loss: 1.1530661203852404....\n",
      "Validation loss: 1.072794858478682....\n",
      "-----------------------------------\n",
      "Training loss: 1.1526596028579565....\n",
      "Validation loss: 1.0725612308636583....\n",
      "-----------------------------------\n",
      "Training loss: 1.1522534631747368....\n",
      "Validation loss: 1.0723277308161523....\n",
      "-----------------------------------\n",
      "Training loss: 1.151847869791221....\n",
      "Validation loss: 1.0720946675803944....\n",
      "-----------------------------------\n",
      "Training loss: 1.1514426652005654....\n",
      "Validation loss: 1.0718617367993637....\n",
      "-----------------------------------\n",
      "Training loss: 1.151037936028717....\n",
      "Validation loss: 1.0716292016724498....\n",
      "-----------------------------------\n",
      "Training loss: 1.1506337236709447....\n",
      "Validation loss: 1.0713969678291653....\n",
      "-----------------------------------\n",
      "Training loss: 1.1502299324133423....\n",
      "Validation loss: 1.0711651419355799....\n",
      "-----------------------------------\n",
      "Training loss: 1.1498265904492573....\n",
      "Validation loss: 1.070933572266539....\n",
      "-----------------------------------\n",
      "Training loss: 1.149423714726488....\n",
      "Validation loss: 1.0707025519025501....\n",
      "-----------------------------------\n",
      "Training loss: 1.1490214138181636....\n",
      "Validation loss: 1.070471512651529....\n",
      "-----------------------------------\n",
      "Training loss: 1.1486196446442196....\n",
      "Validation loss: 1.0702408466815174....\n",
      "-----------------------------------\n",
      "Training loss: 1.148218342564311....\n",
      "Validation loss: 1.0700106785475756....\n",
      "-----------------------------------\n",
      "Training loss: 1.1478174688928904....\n",
      "Validation loss: 1.0697808536098445....\n",
      "-----------------------------------\n",
      "Training loss: 1.1474171605962105....\n",
      "Validation loss: 1.0695515400671618....\n",
      "-----------------------------------\n",
      "Training loss: 1.1470171453879208....\n",
      "Validation loss: 1.069322362316402....\n",
      "-----------------------------------\n",
      "Training loss: 1.1466176822843077....\n",
      "Validation loss: 1.0690935586731924....\n",
      "-----------------------------------\n",
      "Training loss: 1.1462188946561505....\n",
      "Validation loss: 1.0688649863151347....\n",
      "-----------------------------------\n",
      "Training loss: 1.1458206095311423....\n",
      "Validation loss: 1.0686368855110515....\n",
      "-----------------------------------\n",
      "Training loss: 1.1454228315898691....\n",
      "Validation loss: 1.0684093247671724....\n",
      "-----------------------------------\n",
      "Training loss: 1.1450256310644031....\n",
      "Validation loss: 1.0681820791983683....\n",
      "-----------------------------------\n",
      "Training loss: 1.1446290217937671....\n",
      "Validation loss: 1.0679549574271805....\n",
      "-----------------------------------\n",
      "Training loss: 1.144232933277343....\n",
      "Validation loss: 1.0677283674811646....\n",
      "-----------------------------------\n",
      "Training loss: 1.1438373301533644....\n",
      "Validation loss: 1.067501817675896....\n",
      "-----------------------------------\n",
      "Training loss: 1.1434422170071765....\n",
      "Validation loss: 1.067276082387725....\n",
      "-----------------------------------\n",
      "Training loss: 1.1430477033715776....\n",
      "Validation loss: 1.0670505319309531....\n",
      "-----------------------------------\n",
      "Training loss: 1.1426537267656451....\n",
      "Validation loss: 1.0668252734388999....\n",
      "-----------------------------------\n",
      "Training loss: 1.142260282892114....\n",
      "Validation loss: 1.0666006549132023....\n",
      "-----------------------------------\n",
      "Training loss: 1.1418673458163213....\n",
      "Validation loss: 1.0663762696192916....\n",
      "-----------------------------------\n",
      "Training loss: 1.1414749064787046....\n",
      "Validation loss: 1.0661523746029984....\n",
      "-----------------------------------\n",
      "Training loss: 1.1410829720788842....\n",
      "Validation loss: 1.0659286148748595....\n",
      "-----------------------------------\n",
      "Training loss: 1.1406915272564735....\n",
      "Validation loss: 1.065705154695906....\n",
      "-----------------------------------\n",
      "Training loss: 1.140300517185775....\n",
      "Validation loss: 1.0654821680432367....\n",
      "-----------------------------------\n",
      "Training loss: 1.1399099607064715....\n",
      "Validation loss: 1.0652595238791087....\n",
      "-----------------------------------\n",
      "Training loss: 1.1395196774267993....\n",
      "Validation loss: 1.065037447304689....\n",
      "-----------------------------------\n",
      "Training loss: 1.1391298442891382....\n",
      "Validation loss: 1.0648155239904815....\n",
      "-----------------------------------\n",
      "Training loss: 1.1387404179926437....\n",
      "Validation loss: 1.0645939712368417....\n",
      "-----------------------------------\n",
      "Training loss: 1.1383512963674194....\n",
      "Validation loss: 1.0643728079824468....\n",
      "-----------------------------------\n",
      "Training loss: 1.137962545524892....\n",
      "Validation loss: 1.0641519495766094....\n",
      "-----------------------------------\n",
      "Training loss: 1.1375742394763988....\n",
      "Validation loss: 1.0639313259595615....\n",
      "-----------------------------------\n",
      "Training loss: 1.13718637758456....\n",
      "Validation loss: 1.063711430591315....\n",
      "-----------------------------------\n",
      "Training loss: 1.136798943640826....\n",
      "Validation loss: 1.0634916819703526....\n",
      "-----------------------------------\n",
      "Training loss: 1.1364119514407025....\n",
      "Validation loss: 1.06327243778024....\n",
      "-----------------------------------\n",
      "Training loss: 1.1360253343897107....\n",
      "Validation loss: 1.0630536326694489....\n",
      "-----------------------------------\n",
      "Training loss: 1.1356392242019897....\n",
      "Validation loss: 1.0628348906051166....\n",
      "-----------------------------------\n",
      "Training loss: 1.1352535332528377....\n",
      "Validation loss: 1.0626165536176766....\n",
      "-----------------------------------\n",
      "Training loss: 1.1348683508817803....\n",
      "Validation loss: 1.0623986097442055....\n",
      "-----------------------------------\n",
      "Training loss: 1.134483678933331....\n",
      "Validation loss: 1.062180811398627....\n",
      "-----------------------------------\n",
      "Training loss: 1.134099401978197....\n",
      "Validation loss: 1.0619633799479875....\n",
      "-----------------------------------\n",
      "Training loss: 1.1337154074750655....\n",
      "Validation loss: 1.0617462145107344....\n",
      "-----------------------------------\n",
      "Training loss: 1.1333318482156305....\n",
      "Validation loss: 1.061529428743139....\n",
      "-----------------------------------\n",
      "Training loss: 1.1329488061485686....\n",
      "Validation loss: 1.061312623171706....\n",
      "-----------------------------------\n",
      "Training loss: 1.1325662101821126....\n",
      "Validation loss: 1.0610963032887981....\n",
      "-----------------------------------\n",
      "Training loss: 1.1321839771099544....\n",
      "Validation loss: 1.0608802902019407....\n",
      "-----------------------------------\n",
      "Training loss: 1.1318022047547733....\n",
      "Validation loss: 1.060664633431185....\n",
      "-----------------------------------\n",
      "Training loss: 1.1314208873921698....\n",
      "Validation loss: 1.0604493064934006....\n",
      "-----------------------------------\n",
      "Training loss: 1.1310400814383748....\n",
      "Validation loss: 1.0602343066291662....\n",
      "-----------------------------------\n",
      "Training loss: 1.1306597225059614....\n",
      "Validation loss: 1.0600196106130049....\n",
      "-----------------------------------\n",
      "Training loss: 1.1302797362974375....\n",
      "Validation loss: 1.0598054144594826....\n",
      "-----------------------------------\n",
      "Training loss: 1.1299002077315823....\n",
      "Validation loss: 1.0595917710533875....\n",
      "-----------------------------------\n",
      "Training loss: 1.1295210967854505....\n",
      "Validation loss: 1.0593786742826417....\n",
      "-----------------------------------\n",
      "Training loss: 1.1291423388061261....\n",
      "Validation loss: 1.05916602511042....\n",
      "-----------------------------------\n",
      "Training loss: 1.128764017817889....\n",
      "Validation loss: 1.0589537726024438....\n",
      "-----------------------------------\n",
      "Training loss: 1.1283861194175144....\n",
      "Validation loss: 1.0587419020123896....\n",
      "-----------------------------------\n",
      "Training loss: 1.1280085032144647....\n",
      "Validation loss: 1.0585303110812143....\n",
      "-----------------------------------\n",
      "Training loss: 1.1276311788930844....\n",
      "Validation loss: 1.0583192551393699....\n",
      "-----------------------------------\n",
      "Training loss: 1.127254332747017....\n",
      "Validation loss: 1.0581083579801358....\n",
      "-----------------------------------\n",
      "Training loss: 1.1268779038735612....\n",
      "Validation loss: 1.0578979551393852....\n",
      "-----------------------------------\n",
      "Training loss: 1.1265019287355724....\n",
      "Validation loss: 1.057687909201415....\n",
      "-----------------------------------\n",
      "Training loss: 1.1261264131610524....\n",
      "Validation loss: 1.0574780960113488....\n",
      "-----------------------------------\n",
      "Training loss: 1.1257513526257232....\n",
      "Validation loss: 1.057268617596863....\n",
      "-----------------------------------\n",
      "Training loss: 1.1253766762981852....\n",
      "Validation loss: 1.0570594317015434....\n",
      "-----------------------------------\n",
      "Training loss: 1.12500249835492....\n",
      "Validation loss: 1.0568508847598599....\n",
      "-----------------------------------\n",
      "Training loss: 1.124628811828317....\n",
      "Validation loss: 1.056642298383266....\n",
      "-----------------------------------\n",
      "Training loss: 1.1242555972871182....\n",
      "Validation loss: 1.0564345323201605....\n",
      "-----------------------------------\n",
      "Training loss: 1.1238828774558924....\n",
      "Validation loss: 1.0562267536527497....\n",
      "-----------------------------------\n",
      "Training loss: 1.123510597639444....\n",
      "Validation loss: 1.0560193057106708....\n",
      "-----------------------------------\n",
      "Training loss: 1.1231387809053248....\n",
      "Validation loss: 1.0558125751795182....\n",
      "-----------------------------------\n",
      "Training loss: 1.1227674462808417....\n",
      "Validation loss: 1.0556053834351846....\n",
      "-----------------------------------\n",
      "Training loss: 1.1223965839195726....\n",
      "Validation loss: 1.0553989286570726....\n",
      "-----------------------------------\n",
      "Training loss: 1.122026216824952....\n",
      "Validation loss: 1.055192529478488....\n",
      "-----------------------------------\n",
      "Training loss: 1.1216561789528379....\n",
      "Validation loss: 1.054986047557055....\n",
      "-----------------------------------\n",
      "Training loss: 1.1212864332967998....\n",
      "Validation loss: 1.0547804573325692....\n",
      "-----------------------------------\n",
      "Training loss: 1.120917113283637....\n",
      "Validation loss: 1.054574324558821....\n",
      "-----------------------------------\n",
      "Training loss: 1.1205481927569068....\n",
      "Validation loss: 1.0543688668357194....\n",
      "-----------------------------------\n",
      "Training loss: 1.120179683128705....\n",
      "Validation loss: 1.0541636126887206....\n",
      "-----------------------------------\n",
      "Training loss: 1.1198116030036624....\n",
      "Validation loss: 1.0539589680810402....\n",
      "-----------------------------------\n",
      "Training loss: 1.1194437982370056....\n",
      "Validation loss: 1.053754537242585....\n",
      "-----------------------------------\n",
      "Training loss: 1.1190763973090514....\n",
      "Validation loss: 1.0535504036745522....\n",
      "-----------------------------------\n",
      "Training loss: 1.118709487991936....\n",
      "Validation loss: 1.0533468468712093....\n",
      "-----------------------------------\n",
      "Training loss: 1.1183429703351047....\n",
      "Validation loss: 1.053143127979581....\n",
      "-----------------------------------\n",
      "Training loss: 1.1179769320694213....\n",
      "Validation loss: 1.0529404856694238....\n",
      "-----------------------------------\n",
      "Training loss: 1.1176113662206237....\n",
      "Validation loss: 1.0527372616379498....\n",
      "-----------------------------------\n",
      "Training loss: 1.1172462387570317....\n",
      "Validation loss: 1.0525348360310027....\n",
      "-----------------------------------\n",
      "Training loss: 1.116881531904352....\n",
      "Validation loss: 1.0523324358432031....\n",
      "-----------------------------------\n",
      "Training loss: 1.1165172451653045....\n",
      "Validation loss: 1.0521306326006208....\n",
      "-----------------------------------\n",
      "Training loss: 1.1161534252970444....\n",
      "Validation loss: 1.0519288726569214....\n",
      "-----------------------------------\n",
      "Training loss: 1.1157901736173608....\n",
      "Validation loss: 1.0517277859965923....\n",
      "-----------------------------------\n",
      "Training loss: 1.1154274125227603....\n",
      "Validation loss: 1.0515267669888935....\n",
      "-----------------------------------\n",
      "Training loss: 1.1150650611260586....\n",
      "Validation loss: 1.051326439914325....\n",
      "-----------------------------------\n",
      "Training loss: 1.1147031844864692....\n",
      "Validation loss: 1.0511257966063916....\n",
      "-----------------------------------\n",
      "Training loss: 1.1143418571784622....\n",
      "Validation loss: 1.050926085126294....\n",
      "-----------------------------------\n",
      "Training loss: 1.1139809908254534....\n",
      "Validation loss: 1.0507263919883236....\n",
      "-----------------------------------\n",
      "Training loss: 1.113620633816507....\n",
      "Validation loss: 1.0505271105695422....\n",
      "-----------------------------------\n",
      "Training loss: 1.11326076086139....\n",
      "Validation loss: 1.05032799464183....\n",
      "-----------------------------------\n",
      "Training loss: 1.1129013298195967....\n",
      "Validation loss: 1.0501295474340828....\n",
      "-----------------------------------\n",
      "Training loss: 1.1125422517489274....\n",
      "Validation loss: 1.0499310273458582....\n",
      "-----------------------------------\n",
      "Training loss: 1.112183504164373....\n",
      "Validation loss: 1.0497333117423011....\n",
      "-----------------------------------\n",
      "Training loss: 1.1118251537998793....\n",
      "Validation loss: 1.049535826686345....\n",
      "-----------------------------------\n",
      "Training loss: 1.1114671314168671....\n",
      "Validation loss: 1.0493386954999544....\n",
      "-----------------------------------\n",
      "Training loss: 1.111109439873335....\n",
      "Validation loss: 1.0491416755504064....\n",
      "-----------------------------------\n",
      "Training loss: 1.1107522222015738....\n",
      "Validation loss: 1.048945825996667....\n",
      "-----------------------------------\n",
      "Training loss: 1.1103953891831975....\n",
      "Validation loss: 1.0487497915035033....\n",
      "-----------------------------------\n",
      "Training loss: 1.1100389663517842....\n",
      "Validation loss: 1.048554052075125....\n",
      "-----------------------------------\n",
      "Training loss: 1.1096829413364997....\n",
      "Validation loss: 1.0483590669902478....\n",
      "-----------------------------------\n",
      "Training loss: 1.1093273152565717....\n",
      "Validation loss: 1.0481638883539328....\n",
      "-----------------------------------\n",
      "Training loss: 1.1089720438706885....\n",
      "Validation loss: 1.0479693726873176....\n",
      "-----------------------------------\n",
      "Training loss: 1.1086171406647682....\n",
      "Validation loss: 1.0477751859676767....\n",
      "-----------------------------------\n",
      "Training loss: 1.1082626121268713....\n",
      "Validation loss: 1.0475812912927094....\n",
      "-----------------------------------\n",
      "Training loss: 1.107908720165742....\n",
      "Validation loss: 1.047387069763019....\n",
      "-----------------------------------\n",
      "Training loss: 1.107555251998809....\n",
      "Validation loss: 1.0471939364631615....\n",
      "-----------------------------------\n",
      "Training loss: 1.1072021577205127....\n",
      "Validation loss: 1.0470006430965004....\n",
      "-----------------------------------\n",
      "Training loss: 1.1068494126441821....\n",
      "Validation loss: 1.0468078500378786....\n",
      "-----------------------------------\n",
      "Training loss: 1.1064969877584463....\n",
      "Validation loss: 1.0466156372992044....\n",
      "-----------------------------------\n",
      "Training loss: 1.1061449491312765....\n",
      "Validation loss: 1.0464234111248933....\n",
      "-----------------------------------\n",
      "Training loss: 1.105793247936938....\n",
      "Validation loss: 1.0462311256130585....\n",
      "-----------------------------------\n",
      "Training loss: 1.1054418051387929....\n",
      "Validation loss: 1.0460395168436418....\n",
      "-----------------------------------\n",
      "Training loss: 1.1050908400602444....\n",
      "Validation loss: 1.0458481651939084....\n",
      "-----------------------------------\n",
      "Training loss: 1.1047403384410677....\n",
      "Validation loss: 1.0456571739017646....\n",
      "-----------------------------------\n",
      "Training loss: 1.104390286033357....\n",
      "Validation loss: 1.045466257628089....\n",
      "-----------------------------------\n",
      "Training loss: 1.1040406700502194....\n",
      "Validation loss: 1.045275576961465....\n",
      "-----------------------------------\n",
      "Training loss: 1.1036915259833318....\n",
      "Validation loss: 1.0450853896899237....\n",
      "-----------------------------------\n",
      "Training loss: 1.1033428020435962....\n",
      "Validation loss: 1.0448951544827323....\n",
      "-----------------------------------\n",
      "Training loss: 1.1029945359613285....\n",
      "Validation loss: 1.044705421092048....\n",
      "-----------------------------------\n",
      "Training loss: 1.1026466662254997....\n",
      "Validation loss: 1.0445160824086521....\n",
      "-----------------------------------\n",
      "Training loss: 1.1022991085138165....\n",
      "Validation loss: 1.0443265970963054....\n",
      "-----------------------------------\n",
      "Training loss: 1.1019519760924819....\n",
      "Validation loss: 1.0441374949206794....\n",
      "-----------------------------------\n",
      "Training loss: 1.1016052617207066....\n",
      "Validation loss: 1.0439490637231235....\n",
      "-----------------------------------\n",
      "Training loss: 1.1012589623285234....\n",
      "Validation loss: 1.0437602890394857....\n",
      "-----------------------------------\n",
      "Training loss: 1.1009130562695533....\n",
      "Validation loss: 1.0435719004117896....\n",
      "-----------------------------------\n",
      "Training loss: 1.1005675327693403....\n",
      "Validation loss: 1.0433841548563167....\n",
      "-----------------------------------\n",
      "Training loss: 1.1002224263816793....\n",
      "Validation loss: 1.0431963620456157....\n",
      "-----------------------------------\n",
      "Training loss: 1.0998777404587299....\n",
      "Validation loss: 1.0430089577053296....\n",
      "-----------------------------------\n",
      "Training loss: 1.0995333475075504....\n",
      "Validation loss: 1.0428221963424724....\n",
      "-----------------------------------\n",
      "Training loss: 1.0991892103881533....\n",
      "Validation loss: 1.042635688831716....\n",
      "-----------------------------------\n",
      "Training loss: 1.0988453738804242....\n",
      "Validation loss: 1.0424492799044431....\n",
      "-----------------------------------\n",
      "Training loss: 1.0985019006625867....\n",
      "Validation loss: 1.0422628718563305....\n",
      "-----------------------------------\n",
      "Training loss: 1.0981587855687716....\n",
      "Validation loss: 1.0420774129510553....\n",
      "-----------------------------------\n",
      "Training loss: 1.0978160271813744....\n",
      "Validation loss: 1.0418916527761362....\n",
      "-----------------------------------\n",
      "Training loss: 1.097473681462189....\n",
      "Validation loss: 1.0417061102282845....\n",
      "-----------------------------------\n",
      "Training loss: 1.0971316741320556....\n",
      "Validation loss: 1.0415208820559263....\n",
      "-----------------------------------\n",
      "Training loss: 1.0967899865831325....\n",
      "Validation loss: 1.0413362153043018....\n",
      "-----------------------------------\n",
      "Training loss: 1.096448648339779....\n",
      "Validation loss: 1.0411514516289326....\n",
      "-----------------------------------\n",
      "Training loss: 1.096107680592567....\n",
      "Validation loss: 1.0409672570135569....\n",
      "-----------------------------------\n",
      "Training loss: 1.0957670607387382....\n",
      "Validation loss: 1.0407829385863248....\n",
      "-----------------------------------\n",
      "Training loss: 1.095426802208151....\n",
      "Validation loss: 1.0405991489133064....\n",
      "-----------------------------------\n",
      "Training loss: 1.09508696010735....\n",
      "Validation loss: 1.0404156933938884....\n",
      "-----------------------------------\n",
      "Training loss: 1.094747641077543....\n",
      "Validation loss: 1.0402324268228438....\n",
      "-----------------------------------\n",
      "Training loss: 1.0944086882569108....\n",
      "Validation loss: 1.0400491267731218....\n",
      "-----------------------------------\n",
      "Training loss: 1.0940699676172967....\n",
      "Validation loss: 1.0398660036509484....\n",
      "-----------------------------------\n",
      "Training loss: 1.093731554718488....\n",
      "Validation loss: 1.039683288799322....\n",
      "-----------------------------------\n",
      "Training loss: 1.093393491201583....\n",
      "Validation loss: 1.0395005804269717....\n",
      "-----------------------------------\n",
      "Training loss: 1.0930558337916554....\n",
      "Validation loss: 1.0393182921798263....\n",
      "-----------------------------------\n",
      "Training loss: 1.0927185485429955....\n",
      "Validation loss: 1.0391361619321886....\n",
      "-----------------------------------\n",
      "Training loss: 1.092381583038457....\n",
      "Validation loss: 1.0389547942489321....\n",
      "-----------------------------------\n",
      "Training loss: 1.092045034693332....\n",
      "Validation loss: 1.0387732488961237....\n",
      "-----------------------------------\n",
      "Training loss: 1.0917089521315988....\n",
      "Validation loss: 1.0385924017943342....\n",
      "-----------------------------------\n",
      "Training loss: 1.0913733283233291....\n",
      "Validation loss: 1.0384115598862487....\n",
      "-----------------------------------\n",
      "Training loss: 1.091038036432204....\n",
      "Validation loss: 1.0382311637192678....\n",
      "-----------------------------------\n",
      "Training loss: 1.0907031736174124....\n",
      "Validation loss: 1.038050940340935....\n",
      "-----------------------------------\n",
      "Training loss: 1.0903686894669116....\n",
      "Validation loss: 1.037871287917734....\n",
      "-----------------------------------\n",
      "Training loss: 1.0900345741242838....\n",
      "Validation loss: 1.0376915982644386....\n",
      "-----------------------------------\n",
      "Training loss: 1.089700818527725....\n",
      "Validation loss: 1.0375124823993631....\n",
      "-----------------------------------\n",
      "Training loss: 1.0893675291100222....\n",
      "Validation loss: 1.0373335434516124....\n",
      "-----------------------------------\n",
      "Training loss: 1.089034743964623....\n",
      "Validation loss: 1.037155256475706....\n",
      "-----------------------------------\n",
      "Training loss: 1.0887023156500246....\n",
      "Validation loss: 1.0369765896316192....\n",
      "-----------------------------------\n",
      "Training loss: 1.0883701201260119....\n",
      "Validation loss: 1.0367983706068071....\n",
      "-----------------------------------\n",
      "Training loss: 1.0880382821802428....\n",
      "Validation loss: 1.0366207522341242....\n",
      "-----------------------------------\n",
      "Training loss: 1.0877067064784725....\n",
      "Validation loss: 1.036443222000238....\n",
      "-----------------------------------\n",
      "Training loss: 1.0873755701367644....\n",
      "Validation loss: 1.0362661592829776....\n",
      "-----------------------------------\n",
      "Training loss: 1.087044806066606....\n",
      "Validation loss: 1.0360893865159517....\n",
      "-----------------------------------\n",
      "Training loss: 1.0867144072537724....\n",
      "Validation loss: 1.0359125066101693....\n",
      "-----------------------------------\n",
      "Training loss: 1.0863844001333895....\n",
      "Validation loss: 1.0357363717885477....\n",
      "-----------------------------------\n",
      "Training loss: 1.0860547145408104....\n",
      "Validation loss: 1.0355601947163915....\n",
      "-----------------------------------\n",
      "Training loss: 1.085725360219683....\n",
      "Validation loss: 1.0353847998521313....\n",
      "-----------------------------------\n",
      "Training loss: 1.0853963185202524....\n",
      "Validation loss: 1.0352091020996073....\n",
      "-----------------------------------\n",
      "Training loss: 1.0850675546193194....\n",
      "Validation loss: 1.0350338482023271....\n",
      "-----------------------------------\n",
      "Training loss: 1.0847391030236024....\n",
      "Validation loss: 1.034858950523018....\n",
      "-----------------------------------\n",
      "Training loss: 1.0844109520025358....\n",
      "Validation loss: 1.0346845478795104....\n",
      "-----------------------------------\n",
      "Training loss: 1.0840831902092414....\n",
      "Validation loss: 1.0345100152325204....\n",
      "-----------------------------------\n",
      "Training loss: 1.083755796590819....\n",
      "Validation loss: 1.0343362475966982....\n",
      "-----------------------------------\n",
      "Training loss: 1.0834287303246142....\n",
      "Validation loss: 1.0341623547133052....\n",
      "-----------------------------------\n",
      "Training loss: 1.083101965385901....\n",
      "Validation loss: 1.0339890182423401....\n",
      "-----------------------------------\n",
      "Training loss: 1.0827756157352915....\n",
      "Validation loss: 1.0338158377570639....\n",
      "-----------------------------------\n",
      "Training loss: 1.0824496604119929....\n",
      "Validation loss: 1.0336430585647458....\n",
      "-----------------------------------\n",
      "Training loss: 1.0821241186577295....\n",
      "Validation loss: 1.0334702338186181....\n",
      "-----------------------------------\n",
      "Training loss: 1.0817988895431858....\n",
      "Validation loss: 1.0332978733957239....\n",
      "-----------------------------------\n",
      "Training loss: 1.0814738453105743....\n",
      "Validation loss: 1.033125688671542....\n",
      "-----------------------------------\n",
      "Training loss: 1.08114897451179....\n",
      "Validation loss: 1.0329537332431238....\n",
      "-----------------------------------\n",
      "Training loss: 1.0808244593929937....\n",
      "Validation loss: 1.03278244847914....\n",
      "-----------------------------------\n",
      "Training loss: 1.0805003297450189....\n",
      "Validation loss: 1.0326106956118937....\n",
      "-----------------------------------\n",
      "Training loss: 1.0801764591954095....\n",
      "Validation loss: 1.0324396158222997....\n",
      "-----------------------------------\n",
      "Training loss: 1.079852996201748....\n",
      "Validation loss: 1.0322684246627167....\n",
      "-----------------------------------\n",
      "Training loss: 1.0795298775860225....\n",
      "Validation loss: 1.0320979523634872....\n",
      "-----------------------------------\n",
      "Training loss: 1.0792071279988045....\n",
      "Validation loss: 1.0319275168955098....\n",
      "-----------------------------------\n",
      "Training loss: 1.0788847376911908....\n",
      "Validation loss: 1.0317576788446234....\n",
      "-----------------------------------\n",
      "Training loss: 1.0785627904663075....\n",
      "Validation loss: 1.0315874541894254....\n",
      "-----------------------------------\n",
      "Training loss: 1.078241247421123....\n",
      "Validation loss: 1.0314181395951196....\n",
      "-----------------------------------\n",
      "Training loss: 1.0779200138274394....\n",
      "Validation loss: 1.0312489780087797....\n",
      "-----------------------------------\n",
      "Training loss: 1.0775990941872988....\n",
      "Validation loss: 1.031080307988968....\n",
      "-----------------------------------\n",
      "Training loss: 1.0772785760255037....\n",
      "Validation loss: 1.0309116136316645....\n",
      "-----------------------------------\n",
      "Training loss: 1.0769583901781445....\n",
      "Validation loss: 1.030743538569761....\n",
      "-----------------------------------\n",
      "Training loss: 1.0766386082410913....\n",
      "Validation loss: 1.0305758188122558....\n",
      "-----------------------------------\n",
      "Training loss: 1.076319183704084....\n",
      "Validation loss: 1.0304077055605625....\n",
      "-----------------------------------\n",
      "Training loss: 1.076000013796931....\n",
      "Validation loss: 1.0302403785818128....\n",
      "-----------------------------------\n",
      "Training loss: 1.0756812505090598....\n",
      "Validation loss: 1.0300731941865726....\n",
      "-----------------------------------\n",
      "Training loss: 1.0753627695820598....\n",
      "Validation loss: 1.0299058635957168....\n",
      "-----------------------------------\n",
      "Training loss: 1.075044604607894....\n",
      "Validation loss: 1.0297395250223136....\n",
      "-----------------------------------\n",
      "Training loss: 1.0747267615402805....\n",
      "Validation loss: 1.0295727375095998....\n",
      "-----------------------------------\n",
      "Training loss: 1.0744092992615202....\n",
      "Validation loss: 1.0294068409476318....\n",
      "-----------------------------------\n",
      "Training loss: 1.0740922707703608....\n",
      "Validation loss: 1.029240779126033....\n",
      "-----------------------------------\n",
      "Training loss: 1.073775592897637....\n",
      "Validation loss: 1.0290752196252888....\n",
      "-----------------------------------\n",
      "Training loss: 1.073459277741767....\n",
      "Validation loss: 1.0289099486757027....\n",
      "-----------------------------------\n",
      "Training loss: 1.0731433567856008....\n",
      "Validation loss: 1.0287446221012229....\n",
      "-----------------------------------\n",
      "Training loss: 1.0728276783457726....\n",
      "Validation loss: 1.0285799973486311....\n",
      "-----------------------------------\n",
      "Training loss: 1.0725123861989727....\n",
      "Validation loss: 1.028415358924013....\n",
      "-----------------------------------\n",
      "Training loss: 1.072197414992655....\n",
      "Validation loss: 1.0282507261086855....\n",
      "-----------------------------------\n",
      "Training loss: 1.0718828263959836....\n",
      "Validation loss: 1.0280869848141165....\n",
      "-----------------------------------\n",
      "Training loss: 1.0715685325718731....\n",
      "Validation loss: 1.027923392243753....\n",
      "-----------------------------------\n",
      "Training loss: 1.0712546172541313....\n",
      "Validation loss: 1.0277598522862872....\n",
      "-----------------------------------\n",
      "Training loss: 1.0709409896463344....\n",
      "Validation loss: 1.0275971739670942....\n",
      "-----------------------------------\n",
      "Training loss: 1.0706276607624952....\n",
      "Validation loss: 1.027434340990331....\n",
      "-----------------------------------\n",
      "Training loss: 1.0703147102976902....\n",
      "Validation loss: 1.027272033995742....\n",
      "-----------------------------------\n",
      "Training loss: 1.0700020267353243....\n",
      "Validation loss: 1.027109374717715....\n",
      "-----------------------------------\n",
      "Training loss: 1.0696896950672545....\n",
      "Validation loss: 1.02694747608062....\n",
      "-----------------------------------\n",
      "Training loss: 1.0693777277749248....\n",
      "Validation loss: 1.0267852885190047....\n",
      "-----------------------------------\n",
      "Training loss: 1.0690661456476631....\n",
      "Validation loss: 1.026623674170685....\n",
      "-----------------------------------\n",
      "Training loss: 1.0687548676399383....\n",
      "Validation loss: 1.0264621464931993....\n",
      "-----------------------------------\n",
      "Training loss: 1.0684438121533222....\n",
      "Validation loss: 1.0263008778890839....\n",
      "-----------------------------------\n",
      "Training loss: 1.0681331135380128....\n",
      "Validation loss: 1.0261400325187007....\n",
      "-----------------------------------\n",
      "Training loss: 1.0678227349045497....\n",
      "Validation loss: 1.0259787796080153....\n",
      "-----------------------------------\n",
      "Training loss: 1.0675125787076774....\n",
      "Validation loss: 1.0258181420954553....\n",
      "-----------------------------------\n",
      "Training loss: 1.0672026990340604....\n",
      "Validation loss: 1.0256573681412586....\n",
      "-----------------------------------\n",
      "Training loss: 1.0668931037198968....\n",
      "Validation loss: 1.02549716456842....\n",
      "-----------------------------------\n",
      "Training loss: 1.0665836685440626....\n",
      "Validation loss: 1.0253372568397392....\n",
      "-----------------------------------\n",
      "Training loss: 1.0662747057985036....\n",
      "Validation loss: 1.025177429425702....\n",
      "-----------------------------------\n",
      "Training loss: 1.0659660493055776....\n",
      "Validation loss: 1.0250181599185457....\n",
      "-----------------------------------\n",
      "Training loss: 1.0656577967964878....\n",
      "Validation loss: 1.0248587414111998....\n",
      "-----------------------------------\n",
      "Training loss: 1.065350055853467....\n",
      "Validation loss: 1.024700129201828....\n",
      "-----------------------------------\n",
      "Training loss: 1.0650426170307306....\n",
      "Validation loss: 1.0245415258995816....\n",
      "-----------------------------------\n",
      "Training loss: 1.064735416310884....\n",
      "Validation loss: 1.0243829869182854....\n",
      "-----------------------------------\n",
      "Training loss: 1.0644285276837069....\n",
      "Validation loss: 1.024224928662275....\n",
      "-----------------------------------\n",
      "Training loss: 1.0641219199522585....\n",
      "Validation loss: 1.0240669010718162....\n",
      "-----------------------------------\n",
      "Training loss: 1.0638156776608962....\n",
      "Validation loss: 1.0239096469626099....\n",
      "-----------------------------------\n",
      "Training loss: 1.0635097705291554....\n",
      "Validation loss: 1.0237521709384316....\n",
      "-----------------------------------\n",
      "Training loss: 1.0632042607290533....\n",
      "Validation loss: 1.0235955316913594....\n",
      "-----------------------------------\n",
      "Training loss: 1.0628990586985965....\n",
      "Validation loss: 1.0234389604349121....\n",
      "-----------------------------------\n",
      "Training loss: 1.0625941555905316....\n",
      "Validation loss: 1.0232824964692557....\n",
      "-----------------------------------\n",
      "Training loss: 1.0622894982178437....\n",
      "Validation loss: 1.0231265665994287....\n",
      "-----------------------------------\n",
      "Training loss: 1.0619851393106252....\n",
      "Validation loss: 1.0229702806776984....\n",
      "-----------------------------------\n",
      "Training loss: 1.0616811261721433....\n",
      "Validation loss: 1.022814837498949....\n",
      "-----------------------------------\n",
      "Training loss: 1.0613774236979452....\n",
      "Validation loss: 1.0226593350155007....\n",
      "-----------------------------------\n",
      "Training loss: 1.0610740280188273....\n",
      "Validation loss: 1.022504138237925....\n",
      "-----------------------------------\n",
      "Training loss: 1.0607710130286998....\n",
      "Validation loss: 1.0223488865304957....\n",
      "-----------------------------------\n",
      "Training loss: 1.060468363718739....\n",
      "Validation loss: 1.022194341991271....\n",
      "-----------------------------------\n",
      "Training loss: 1.0601660507359039....\n",
      "Validation loss: 1.0220398885442068....\n",
      "-----------------------------------\n",
      "Training loss: 1.059864094389433....\n",
      "Validation loss: 1.0218857163299027....\n",
      "-----------------------------------\n",
      "Training loss: 1.0595625114917597....\n",
      "Validation loss: 1.0217314961459592....\n",
      "-----------------------------------\n",
      "Training loss: 1.0592612968570212....\n",
      "Validation loss: 1.021578043188307....\n",
      "-----------------------------------\n",
      "Training loss: 1.0589603736715298....\n",
      "Validation loss: 1.021424753908841....\n",
      "-----------------------------------\n",
      "Training loss: 1.058659807538637....\n",
      "Validation loss: 1.0212718506494176....\n",
      "-----------------------------------\n",
      "Training loss: 1.0583595604672225....\n",
      "Validation loss: 1.0211187523327796....\n",
      "-----------------------------------\n",
      "Training loss: 1.0580596002871971....\n",
      "Validation loss: 1.0209663892308312....\n",
      "-----------------------------------\n",
      "Training loss: 1.0577599096280674....\n",
      "Validation loss: 1.0208142386825323....\n",
      "-----------------------------------\n",
      "Training loss: 1.0574606052999456....\n",
      "Validation loss: 1.0206624886086186....\n",
      "-----------------------------------\n",
      "Training loss: 1.0571615978553839....\n",
      "Validation loss: 1.0205105089193467....\n",
      "-----------------------------------\n",
      "Training loss: 1.0568627528315797....\n",
      "Validation loss: 1.0203592794505607....\n",
      "-----------------------------------\n",
      "Training loss: 1.0565642084850744....\n",
      "Validation loss: 1.020208281369717....\n",
      "-----------------------------------\n",
      "Training loss: 1.0562659334891786....\n",
      "Validation loss: 1.0200572602802866....\n",
      "-----------------------------------\n",
      "Training loss: 1.055967854734028....\n",
      "Validation loss: 1.0199067356107165....\n",
      "-----------------------------------\n",
      "Training loss: 1.0556700094163667....\n",
      "Validation loss: 1.0197559736493043....\n",
      "-----------------------------------\n",
      "Training loss: 1.0553724269189024....\n",
      "Validation loss: 1.0196060465252046....\n",
      "-----------------------------------\n",
      "Training loss: 1.055075173942915....\n",
      "Validation loss: 1.0194562826178775....\n",
      "-----------------------------------\n",
      "Training loss: 1.0547781833356098....\n",
      "Validation loss: 1.019306888027414....\n",
      "-----------------------------------\n",
      "Training loss: 1.054481473415196....\n",
      "Validation loss: 1.019157603625657....\n",
      "-----------------------------------\n",
      "Training loss: 1.054185121212912....\n",
      "Validation loss: 1.0190088395759311....\n",
      "-----------------------------------\n",
      "Training loss: 1.053889218635957....\n",
      "Validation loss: 1.0188598062102807....\n",
      "-----------------------------------\n",
      "Training loss: 1.0535937011101242....\n",
      "Validation loss: 1.0187113155277847....\n",
      "-----------------------------------\n",
      "Training loss: 1.0532983758375918....\n",
      "Validation loss: 1.0185627730507925....\n",
      "-----------------------------------\n",
      "Training loss: 1.0530033442302837....\n",
      "Validation loss: 1.0184145250136145....\n",
      "-----------------------------------\n",
      "Training loss: 1.0527085054499339....\n",
      "Validation loss: 1.0182662765105732....\n",
      "-----------------------------------\n",
      "Training loss: 1.052413909352986....\n",
      "Validation loss: 1.0181187178141353....\n",
      "-----------------------------------\n",
      "Training loss: 1.052119678125383....\n",
      "Validation loss: 1.0179708857368324....\n",
      "-----------------------------------\n",
      "Training loss: 1.0518257389398278....\n",
      "Validation loss: 1.0178237073110719....\n",
      "-----------------------------------\n",
      "Training loss: 1.0515319230085092....\n",
      "Validation loss: 1.017676640542007....\n",
      "-----------------------------------\n",
      "Training loss: 1.051238453159819....\n",
      "Validation loss: 1.0175293340500449....\n",
      "-----------------------------------\n",
      "Training loss: 1.0509452925341711....\n",
      "Validation loss: 1.0173828251603654....\n",
      "-----------------------------------\n",
      "Training loss: 1.0506524530180754....\n",
      "Validation loss: 1.0172363686104526....\n",
      "-----------------------------------\n",
      "Training loss: 1.0503599868546563....\n",
      "Validation loss: 1.0170902571094143....\n",
      "-----------------------------------\n",
      "Training loss: 1.0500679275571352....\n",
      "Validation loss: 1.0169441224977251....\n",
      "-----------------------------------\n",
      "Training loss: 1.0497761860595938....\n",
      "Validation loss: 1.0167983350266403....\n",
      "-----------------------------------\n",
      "Training loss: 1.0494847068670081....\n",
      "Validation loss: 1.0166522035467211....\n",
      "-----------------------------------\n",
      "Training loss: 1.0491934874258553....\n",
      "Validation loss: 1.0165067562393828....\n",
      "-----------------------------------\n",
      "Training loss: 1.0489025141650457....\n",
      "Validation loss: 1.016361410911868....\n",
      "-----------------------------------\n",
      "Training loss: 1.0486119255191175....\n",
      "Validation loss: 1.016216310773766....\n",
      "-----------------------------------\n",
      "Training loss: 1.0483216246192255....\n",
      "Validation loss: 1.016071278860939....\n",
      "-----------------------------------\n",
      "Training loss: 1.0480316121260969....\n",
      "Validation loss: 1.0159266019531148....\n",
      "-----------------------------------\n",
      "Training loss: 1.0477419866441546....\n",
      "Validation loss: 1.0157819848230591....\n",
      "-----------------------------------\n",
      "Training loss: 1.0474527022013873....\n",
      "Validation loss: 1.0156377166591704....\n",
      "-----------------------------------\n",
      "Training loss: 1.0471637395372675....\n",
      "Validation loss: 1.015493572205845....\n",
      "-----------------------------------\n",
      "Training loss: 1.0468750864952336....\n",
      "Validation loss: 1.0153498834876884....\n",
      "-----------------------------------\n",
      "Training loss: 1.0465867754219076....\n",
      "Validation loss: 1.015206132931433....\n",
      "-----------------------------------\n",
      "Training loss: 1.04629875723436....\n",
      "Validation loss: 1.0150628111168325....\n",
      "-----------------------------------\n",
      "Training loss: 1.0460111410213784....\n",
      "Validation loss: 1.0149193458704209....\n",
      "-----------------------------------\n",
      "Training loss: 1.045723762465314....\n",
      "Validation loss: 1.0147762034898613....\n",
      "-----------------------------------\n",
      "Training loss: 1.0454365806487687....\n",
      "Validation loss: 1.0146331253026175....\n",
      "-----------------------------------\n",
      "Training loss: 1.0451497258416627....\n",
      "Validation loss: 1.0144907826864165....\n",
      "-----------------------------------\n",
      "Training loss: 1.0448632002902265....\n",
      "Validation loss: 1.014348164183655....\n",
      "-----------------------------------\n",
      "Training loss: 1.044577013749246....\n",
      "Validation loss: 1.0142065052635698....\n",
      "-----------------------------------\n",
      "Training loss: 1.0442911046268974....\n",
      "Validation loss: 1.0140643716742703....\n",
      "-----------------------------------\n",
      "Training loss: 1.0440055202045764....\n",
      "Validation loss: 1.013922888473451....\n",
      "-----------------------------------\n",
      "Training loss: 1.0437201761221313....\n",
      "Validation loss: 1.0137815855779486....\n",
      "-----------------------------------\n",
      "Training loss: 1.043435146707333....\n",
      "Validation loss: 1.0136403204870352....\n",
      "-----------------------------------\n",
      "Training loss: 1.0431503927258914....\n",
      "Validation loss: 1.0134991780339238....\n",
      "-----------------------------------\n",
      "Training loss: 1.0428659998082448....\n",
      "Validation loss: 1.013358634084981....\n",
      "-----------------------------------\n",
      "Training loss: 1.042581915624522....\n",
      "Validation loss: 1.0132182211127718....\n",
      "-----------------------------------\n",
      "Training loss: 1.042298175745656....\n",
      "Validation loss: 1.013077614545873....\n",
      "-----------------------------------\n",
      "Training loss: 1.0420148275032335....\n",
      "Validation loss: 1.012937700532516....\n",
      "-----------------------------------\n",
      "Training loss: 1.0417317564106698....\n",
      "Validation loss: 1.0127981140803641....\n",
      "-----------------------------------\n",
      "Training loss: 1.0414488783804232....\n",
      "Validation loss: 1.0126584381621424....\n",
      "-----------------------------------\n",
      "Training loss: 1.0411661584112881....\n",
      "Validation loss: 1.012519426471545....\n",
      "-----------------------------------\n",
      "Training loss: 1.0408836867895648....\n",
      "Validation loss: 1.012380307648139....\n",
      "-----------------------------------\n",
      "Training loss: 1.0406015321650894....\n",
      "Validation loss: 1.0122416944700263....\n",
      "-----------------------------------\n",
      "Training loss: 1.0403195005756904....\n",
      "Validation loss: 1.012103248054018....\n",
      "-----------------------------------\n",
      "Training loss: 1.0400375637999768....\n",
      "Validation loss: 1.0119645362525216....\n",
      "-----------------------------------\n",
      "Training loss: 1.039755965544817....\n",
      "Validation loss: 1.0118267174518756....\n",
      "-----------------------------------\n",
      "Training loss: 1.0394746835237487....\n",
      "Validation loss: 1.0116887391276665....\n",
      "-----------------------------------\n",
      "Training loss: 1.0391937777367544....\n",
      "Validation loss: 1.0115509138336078....\n",
      "-----------------------------------\n",
      "Training loss: 1.038913207547....\n",
      "Validation loss: 1.0114134162534167....\n",
      "-----------------------------------\n",
      "Training loss: 1.0386328822106021....\n",
      "Validation loss: 1.011276141507442....\n",
      "-----------------------------------\n",
      "Training loss: 1.0383526603678985....\n",
      "Validation loss: 1.011138858536916....\n",
      "-----------------------------------\n",
      "Training loss: 1.0380727922501816....\n",
      "Validation loss: 1.0110022269792616....\n",
      "-----------------------------------\n",
      "Training loss: 1.0377932520321802....\n",
      "Validation loss: 1.0108651059591125....\n",
      "-----------------------------------\n",
      "Training loss: 1.0375139195332308....\n",
      "Validation loss: 1.0107287384728023....\n",
      "-----------------------------------\n",
      "Training loss: 1.0372347690346237....\n",
      "Validation loss: 1.0105922573043984....\n",
      "-----------------------------------\n",
      "Training loss: 1.0369558930250233....\n",
      "Validation loss: 1.010455958394571....\n",
      "-----------------------------------\n",
      "Training loss: 1.0366772693640982....\n",
      "Validation loss: 1.0103201208317678....\n",
      "-----------------------------------\n",
      "Training loss: 1.036398859504628....\n",
      "Validation loss: 1.0101842620751322....\n",
      "-----------------------------------\n",
      "Training loss: 1.036120571413052....\n",
      "Validation loss: 1.0100487958086313....\n",
      "-----------------------------------\n",
      "Training loss: 1.0358425672372933....\n",
      "Validation loss: 1.0099135375705874....\n",
      "-----------------------------------\n",
      "Training loss: 1.0355649764868833....\n",
      "Validation loss: 1.0097781073522258....\n",
      "-----------------------------------\n",
      "Training loss: 1.0352877378837022....\n",
      "Validation loss: 1.0096432721843278....\n",
      "-----------------------------------\n",
      "Training loss: 1.0350107729990898....\n",
      "Validation loss: 1.009508456423981....\n",
      "-----------------------------------\n",
      "Training loss: 1.034734187339132....\n",
      "Validation loss: 1.0093741627291233....\n",
      "-----------------------------------\n",
      "Training loss: 1.0344579302586079....\n",
      "Validation loss: 1.00924000393389....\n",
      "-----------------------------------\n",
      "Training loss: 1.0341819544661595....\n",
      "Validation loss: 1.0091060925363047....\n",
      "-----------------------------------\n",
      "Training loss: 1.0339063057712548....\n",
      "Validation loss: 1.008972471257488....\n",
      "-----------------------------------\n",
      "Training loss: 1.033630920077381....\n",
      "Validation loss: 1.008838924158044....\n",
      "-----------------------------------\n",
      "Training loss: 1.0333558776472511....\n",
      "Validation loss: 1.0087058391577282....\n",
      "-----------------------------------\n",
      "Training loss: 1.0330811228558996....\n",
      "Validation loss: 1.0085724929007776....\n",
      "-----------------------------------\n",
      "Training loss: 1.032806609910904....\n",
      "Validation loss: 1.0084397584514966....\n",
      "-----------------------------------\n",
      "Training loss: 1.0325323414841823....\n",
      "Validation loss: 1.0083069664992723....\n",
      "-----------------------------------\n",
      "Training loss: 1.0322583052880117....\n",
      "Validation loss: 1.0081743905949654....\n",
      "-----------------------------------\n",
      "Training loss: 1.031984594953284....\n",
      "Validation loss: 1.0080419367126279....\n",
      "-----------------------------------\n",
      "Training loss: 1.0317111850360152....\n",
      "Validation loss: 1.0079096036793145....\n",
      "-----------------------------------\n",
      "Training loss: 1.0314380602610382....\n",
      "Validation loss: 1.0077773912740013....\n",
      "-----------------------------------\n",
      "Training loss: 1.0311652022043278....\n",
      "Validation loss: 1.0076456617856717....\n",
      "-----------------------------------\n",
      "Training loss: 1.0308925662492285....\n",
      "Validation loss: 1.007514020177829....\n",
      "-----------------------------------\n",
      "Training loss: 1.0306201547307765....\n",
      "Validation loss: 1.0073822184941725....\n",
      "-----------------------------------\n",
      "Training loss: 1.0303480453634761....\n",
      "Validation loss: 1.0072511958230015....\n",
      "-----------------------------------\n",
      "Training loss: 1.030076245486081....\n",
      "Validation loss: 1.0071199226236516....\n",
      "-----------------------------------\n",
      "Training loss: 1.0298047112060067....\n",
      "Validation loss: 1.0069889460853616....\n",
      "-----------------------------------\n",
      "Training loss: 1.0295333268333764....\n",
      "Validation loss: 1.0068581960078156....\n",
      "-----------------------------------\n",
      "Training loss: 1.0292622526283721....\n",
      "Validation loss: 1.0067274912582695....\n",
      "-----------------------------------\n",
      "Training loss: 1.028991483345432....\n",
      "Validation loss: 1.0065969919218793....\n",
      "-----------------------------------\n",
      "Training loss: 1.028721026882778....\n",
      "Validation loss: 1.006466889612245....\n",
      "-----------------------------------\n",
      "Training loss: 1.0284509350198283....\n",
      "Validation loss: 1.0063367965909669....\n",
      "-----------------------------------\n",
      "Training loss: 1.0281811448392733....\n",
      "Validation loss: 1.0062074773522374....\n",
      "-----------------------------------\n",
      "Training loss: 1.0279115889302983....\n",
      "Validation loss: 1.0060778503029988....\n",
      "-----------------------------------\n",
      "Training loss: 1.0276422138716443....\n",
      "Validation loss: 1.0059488887799337....\n",
      "-----------------------------------\n",
      "Training loss: 1.0273730431432544....\n",
      "Validation loss: 1.005819952617493....\n",
      "-----------------------------------\n",
      "Training loss: 1.0271041733683437....\n",
      "Validation loss: 1.0056914520610265....\n",
      "-----------------------------------\n",
      "Training loss: 1.0268355832325584....\n",
      "Validation loss: 1.0055629822484502....\n",
      "-----------------------------------\n",
      "Training loss: 1.0265672913899913....\n",
      "Validation loss: 1.0054347485319834....\n",
      "-----------------------------------\n",
      "Training loss: 1.0262992787871832....\n",
      "Validation loss: 1.0053068449921962....\n",
      "-----------------------------------\n",
      "Training loss: 1.02603153177469....\n",
      "Validation loss: 1.0051788113513036....\n",
      "-----------------------------------\n",
      "Training loss: 1.0257641111318037....\n",
      "Validation loss: 1.0050512651994257....\n",
      "-----------------------------------\n",
      "Training loss: 1.0254969368009725....\n",
      "Validation loss: 1.0049238044382176....\n",
      "-----------------------------------\n",
      "Training loss: 1.0252300005025563....\n",
      "Validation loss: 1.0047964846286923....\n",
      "-----------------------------------\n",
      "Training loss: 1.0249634329318462....\n",
      "Validation loss: 1.0046693023463136....\n",
      "-----------------------------------\n",
      "Training loss: 1.0246971880819213....\n",
      "Validation loss: 1.0045423967659974....\n",
      "-----------------------------------\n",
      "Training loss: 1.0244312843317185....\n",
      "Validation loss: 1.0044154862467223....\n",
      "-----------------------------------\n",
      "Training loss: 1.0241656213843424....\n",
      "Validation loss: 1.0042891853213909....\n",
      "-----------------------------------\n",
      "Training loss: 1.0239001279621855....\n",
      "Validation loss: 1.0041629558518201....\n",
      "-----------------------------------\n",
      "Training loss: 1.0236349651961436....\n",
      "Validation loss: 1.0040369987288205....\n",
      "-----------------------------------\n",
      "Training loss: 1.0233701098290893....\n",
      "Validation loss: 1.0039109781051645....\n",
      "-----------------------------------\n",
      "Training loss: 1.0231055520258945....\n",
      "Validation loss: 1.0037851062413026....\n",
      "-----------------------------------\n",
      "Training loss: 1.0228412762985886....\n",
      "Validation loss: 1.003659813062753....\n",
      "-----------------------------------\n",
      "Training loss: 1.0225772200586551....\n",
      "Validation loss: 1.0035344804657....\n",
      "-----------------------------------\n",
      "Training loss: 1.0223134729550476....\n",
      "Validation loss: 1.0034093071006656....\n",
      "-----------------------------------\n",
      "Training loss: 1.022049957135558....\n",
      "Validation loss: 1.0032845055795307....\n",
      "-----------------------------------\n",
      "Training loss: 1.0217867685284825....\n",
      "Validation loss: 1.0031592380242775....\n",
      "-----------------------------------\n",
      "Training loss: 1.02152390808301....\n",
      "Validation loss: 1.0030347947153917....\n",
      "-----------------------------------\n",
      "Training loss: 1.0212613310686787....\n",
      "Validation loss: 1.002910484916963....\n",
      "-----------------------------------\n",
      "Training loss: 1.0209991107934389....\n",
      "Validation loss: 1.0027862247217236....\n",
      "-----------------------------------\n",
      "Training loss: 1.0207371716593323....\n",
      "Validation loss: 1.0026621613815192....\n",
      "-----------------------------------\n",
      "Training loss: 1.0204755230969607....\n",
      "Validation loss: 1.0025383919225461....\n",
      "-----------------------------------\n",
      "Training loss: 1.0202141070901578....\n",
      "Validation loss: 1.0024145852948392....\n",
      "-----------------------------------\n",
      "Training loss: 1.0199528883653382....\n",
      "Validation loss: 1.002291364505138....\n",
      "-----------------------------------\n",
      "Training loss: 1.019691896284895....\n",
      "Validation loss: 1.0021678052554....\n",
      "-----------------------------------\n",
      "Training loss: 1.019431014454993....\n",
      "Validation loss: 1.0020447504508272....\n",
      "-----------------------------------\n",
      "Training loss: 1.0191702725283474....\n",
      "Validation loss: 1.0019214128010678....\n",
      "-----------------------------------\n",
      "Training loss: 1.018909902771424....\n",
      "Validation loss: 1.0017987225757308....\n",
      "-----------------------------------\n",
      "Training loss: 1.018649788231847....\n",
      "Validation loss: 1.001676167936141....\n",
      "-----------------------------------\n",
      "Training loss: 1.0183899222807935....\n",
      "Validation loss: 1.001553802524828....\n",
      "-----------------------------------\n",
      "Training loss: 1.0181303300616809....\n",
      "Validation loss: 1.0014317397616583....\n",
      "-----------------------------------\n",
      "Training loss: 1.0178709934898937....\n",
      "Validation loss: 1.0013096105610406....\n",
      "-----------------------------------\n",
      "Training loss: 1.0176119245700828....\n",
      "Validation loss: 1.001187946715224....\n",
      "-----------------------------------\n",
      "Training loss: 1.0173531978625003....\n",
      "Validation loss: 1.0010662127088528....\n",
      "-----------------------------------\n",
      "Training loss: 1.017094715027082....\n",
      "Validation loss: 1.00094490174395....\n",
      "-----------------------------------\n",
      "Training loss: 1.0168364286386171....\n",
      "Validation loss: 1.0008233227956886....\n",
      "-----------------------------------\n",
      "Training loss: 1.0165784214520304....\n",
      "Validation loss: 1.0007022940807275....\n",
      "-----------------------------------\n",
      "Training loss: 1.0163206439785968....\n",
      "Validation loss: 1.0005813079828416....\n",
      "-----------------------------------\n",
      "Training loss: 1.0160632232018973....\n",
      "Validation loss: 1.0004601862637361....\n",
      "-----------------------------------\n",
      "Training loss: 1.0158061027366867....\n",
      "Validation loss: 1.0003393709078487....\n",
      "-----------------------------------\n",
      "Training loss: 1.015549262140315....\n",
      "Validation loss: 1.0002187979877672....\n",
      "-----------------------------------\n",
      "Training loss: 1.015292746219462....\n",
      "Validation loss: 1.0000985218093634....\n",
      "-----------------------------------\n",
      "Training loss: 1.0150365271242772....\n",
      "Validation loss: 0.9999780089319835....\n",
      "-----------------------------------\n",
      "Training loss: 1.0147806116591496....\n",
      "Validation loss: 0.9998582906899632....\n",
      "-----------------------------------\n",
      "Training loss: 1.014524958449693....\n",
      "Validation loss: 0.9997384927383808....\n",
      "-----------------------------------\n",
      "Training loss: 1.0142695166568205....\n",
      "Validation loss: 0.9996190118961681....\n",
      "-----------------------------------\n",
      "Training loss: 1.0140142711991973....\n",
      "Validation loss: 0.9994996270118225....\n",
      "-----------------------------------\n",
      "Training loss: 1.0137592848038621....\n",
      "Validation loss: 0.9993803922379003....\n",
      "-----------------------------------\n",
      "Training loss: 1.013504535293528....\n",
      "Validation loss: 0.9992616160805108....\n",
      "-----------------------------------\n",
      "Training loss: 1.013250021159133....\n",
      "Validation loss: 0.99914290692388....\n",
      "-----------------------------------\n",
      "Training loss: 1.0129957865394204....\n",
      "Validation loss: 0.999024419812781....\n",
      "-----------------------------------\n",
      "Training loss: 1.0127418448967878....\n",
      "Validation loss: 0.9989060930208191....\n",
      "-----------------------------------\n",
      "Training loss: 1.0124881228933593....\n",
      "Validation loss: 0.9987879193093331....\n",
      "-----------------------------------\n",
      "Training loss: 1.0122346114288017....\n",
      "Validation loss: 0.9986700282104883....\n",
      "-----------------------------------\n",
      "Training loss: 1.0119813722836206....\n",
      "Validation loss: 0.9985518538124833....\n",
      "-----------------------------------\n",
      "Training loss: 1.011728393190504....\n",
      "Validation loss: 0.9984345089625143....\n",
      "-----------------------------------\n",
      "Training loss: 1.011475677097858....\n",
      "Validation loss: 0.9983172206556582....\n",
      "-----------------------------------\n",
      "Training loss: 1.011223205935363....\n",
      "Validation loss: 0.9982003945268799....\n",
      "-----------------------------------\n",
      "Training loss: 1.0109709391493327....\n",
      "Validation loss: 0.9980835239030814....\n",
      "-----------------------------------\n",
      "Training loss: 1.010718999454276....\n",
      "Validation loss: 0.9979668534724778....\n",
      "-----------------------------------\n",
      "Training loss: 1.010467365049443....\n",
      "Validation loss: 0.9978503424542419....\n",
      "-----------------------------------\n",
      "Training loss: 1.0102160095545394....\n",
      "Validation loss: 0.9977337525154942....\n",
      "-----------------------------------\n",
      "Training loss: 1.009964911131496....\n",
      "Validation loss: 0.9976176034631913....\n",
      "-----------------------------------\n",
      "Training loss: 1.009714116805673....\n",
      "Validation loss: 0.997501298404401....\n",
      "-----------------------------------\n",
      "Training loss: 1.0094635711061428....\n",
      "Validation loss: 0.9973855976602369....\n",
      "-----------------------------------\n",
      "Training loss: 1.009213254418318....\n",
      "Validation loss: 0.9972699356087495....\n",
      "-----------------------------------\n",
      "Training loss: 1.0089631474684158....\n",
      "Validation loss: 0.9971546411065793....\n",
      "-----------------------------------\n",
      "Training loss: 1.008713283639272....\n",
      "Validation loss: 0.9970394729594839....\n",
      "-----------------------------------\n",
      "Training loss: 1.008463659107283....\n",
      "Validation loss: 0.9969243632121346....\n",
      "-----------------------------------\n",
      "Training loss: 1.0082142685989675....\n",
      "Validation loss: 0.996809560474045....\n",
      "-----------------------------------\n",
      "Training loss: 1.007965011089991....\n",
      "Validation loss: 0.9966948561724132....\n",
      "-----------------------------------\n",
      "Training loss: 1.0077160525341433....\n",
      "Validation loss: 0.9965804854379933....\n",
      "-----------------------------------\n",
      "Training loss: 1.0074674530588323....\n",
      "Validation loss: 0.9964663166467842....\n",
      "-----------------------------------\n",
      "Training loss: 1.0072191185032116....\n",
      "Validation loss: 0.9963520374790924....\n",
      "-----------------------------------\n",
      "Training loss: 1.0069709916766967....\n",
      "Validation loss: 0.9962381150116147....\n",
      "-----------------------------------\n",
      "Training loss: 1.0067231125708238....\n",
      "Validation loss: 0.9961244050837148....\n",
      "-----------------------------------\n",
      "Training loss: 1.006475500190592....\n",
      "Validation loss: 0.9960110565008018....\n",
      "-----------------------------------\n",
      "Training loss: 1.0062281203940593....\n",
      "Validation loss: 0.9958978038852518....\n",
      "-----------------------------------\n",
      "Training loss: 1.0059809757619198....\n",
      "Validation loss: 0.9957846990723027....\n",
      "-----------------------------------\n",
      "Training loss: 1.005734076184007....\n",
      "Validation loss: 0.9956715669448073....\n",
      "-----------------------------------\n",
      "Training loss: 1.0054874171978616....\n",
      "Validation loss: 0.9955587333678771....\n",
      "-----------------------------------\n",
      "Training loss: 1.0052410095933029....\n",
      "Validation loss: 0.995446044649384....\n",
      "-----------------------------------\n",
      "Training loss: 1.0049947663493428....\n",
      "Validation loss: 0.9953338471676492....\n",
      "-----------------------------------\n",
      "Training loss: 1.004748810444159....\n",
      "Validation loss: 0.9952214056919014....\n",
      "-----------------------------------\n",
      "Training loss: 1.0045030179999166....\n",
      "Validation loss: 0.9951092472907939....\n",
      "-----------------------------------\n",
      "Training loss: 1.004257402889124....\n",
      "Validation loss: 0.9949971398104858....\n",
      "-----------------------------------\n",
      "Training loss: 1.004011971697359....\n",
      "Validation loss: 0.9948846193347395....\n",
      "-----------------------------------\n",
      "Training loss: 1.0037666803842302....\n",
      "Validation loss: 0.9947730788592162....\n",
      "-----------------------------------\n",
      "Training loss: 1.0035216025137306....\n",
      "Validation loss: 0.9946616025355276....\n",
      "-----------------------------------\n",
      "Training loss: 1.0032767655537251....\n",
      "Validation loss: 0.9945498611394591....\n",
      "-----------------------------------\n",
      "Training loss: 1.003032098073235....\n",
      "Validation loss: 0.9944384224115065....\n",
      "-----------------------------------\n",
      "Training loss: 1.0027876340308823....\n",
      "Validation loss: 0.9943272727379547....\n",
      "-----------------------------------\n",
      "Training loss: 1.0025434145738967....\n",
      "Validation loss: 0.9942163720963282....\n",
      "-----------------------------------\n",
      "Training loss: 1.002299436510342....\n",
      "Validation loss: 0.9941054444912124....\n",
      "-----------------------------------\n",
      "Training loss: 1.002055739280404....\n",
      "Validation loss: 0.9939950214360795....\n",
      "-----------------------------------\n",
      "Training loss: 1.0018122914911163....\n",
      "Validation loss: 0.9938843761618859....\n",
      "-----------------------------------\n",
      "Training loss: 1.001569102055591....\n",
      "Validation loss: 0.9937744069113161....\n",
      "-----------------------------------\n",
      "Training loss: 1.0013261389408137....\n",
      "Validation loss: 0.9936644890636984....\n",
      "-----------------------------------\n",
      "Training loss: 1.0010834136748166....\n",
      "Validation loss: 0.9935545890793179....\n",
      "-----------------------------------\n",
      "Training loss: 1.0008409562625202....\n",
      "Validation loss: 0.9934443095367201....\n",
      "-----------------------------------\n",
      "Training loss: 1.0005987015332423....\n",
      "Validation loss: 0.9933352810256387....\n",
      "-----------------------------------\n",
      "Training loss: 1.0003566557444206....\n",
      "Validation loss: 0.9932260602982427....\n",
      "-----------------------------------\n",
      "Training loss: 1.0001148173562318....\n",
      "Validation loss: 0.9931169352429324....\n",
      "-----------------------------------\n",
      "Training loss: 0.9998732432121429....\n",
      "Validation loss: 0.9930078796684408....\n",
      "-----------------------------------\n",
      "Training loss: 0.999631878237993....\n",
      "Validation loss: 0.9928991699009....\n",
      "-----------------------------------\n",
      "Training loss: 0.9993907090829602....\n",
      "Validation loss: 0.992790584463019....\n",
      "-----------------------------------\n",
      "Training loss: 0.9991497428286537....\n",
      "Validation loss: 0.9926820498950925....\n",
      "-----------------------------------\n",
      "Training loss: 0.9989090214413384....\n",
      "Validation loss: 0.992573942791281....\n",
      "-----------------------------------\n",
      "Training loss: 0.9986684654591959....\n",
      "Validation loss: 0.9924659333148584....\n",
      "-----------------------------------\n",
      "Training loss: 0.9984281609669481....\n",
      "Validation loss: 0.9923576236483028....\n",
      "-----------------------------------\n",
      "Training loss: 0.9981880848511961....\n",
      "Validation loss: 0.9922500329414506....\n",
      "-----------------------------------\n",
      "Training loss: 0.997948113772315....\n",
      "Validation loss: 0.9921423438283274....\n",
      "-----------------------------------\n",
      "Training loss: 0.9977083606777649....\n",
      "Validation loss: 0.9920351530194603....\n",
      "-----------------------------------\n",
      "Training loss: 0.997468777493794....\n",
      "Validation loss: 0.9919275200907988....\n",
      "-----------------------------------\n",
      "Training loss: 0.997229381998908....\n",
      "Validation loss: 0.9918207471852155....\n",
      "-----------------------------------\n",
      "Training loss: 0.9969902171338165....\n",
      "Validation loss: 0.9917135331326128....\n",
      "-----------------------------------\n",
      "Training loss: 0.9967512797247491....\n",
      "Validation loss: 0.991607093098834....\n",
      "-----------------------------------\n",
      "Training loss: 0.9965125445186555....\n",
      "Validation loss: 0.9915000066124131....\n",
      "-----------------------------------\n",
      "Training loss: 0.9962740920374477....\n",
      "Validation loss: 0.9913938296772232....\n",
      "-----------------------------------\n",
      "Training loss: 0.996035864074098....\n",
      "Validation loss: 0.9912872676984292....\n",
      "-----------------------------------\n",
      "Training loss: 0.9957978486046234....\n",
      "Validation loss: 0.9911809847217964....\n",
      "-----------------------------------\n",
      "Training loss: 0.9955600543539591....\n",
      "Validation loss: 0.9910748918328919....\n",
      "-----------------------------------\n",
      "Training loss: 0.995322443722751....\n",
      "Validation loss: 0.9909693525439294....\n",
      "-----------------------------------\n",
      "Training loss: 0.9950850341110239....\n",
      "Validation loss: 0.9908631547921197....\n",
      "-----------------------------------\n",
      "Training loss: 0.9948478297937773....\n",
      "Validation loss: 0.9907576936671635....\n",
      "-----------------------------------\n",
      "Training loss: 0.9946107229057318....\n",
      "Validation loss: 0.9906520503829634....\n",
      "-----------------------------------\n",
      "Training loss: 0.9943738141997909....\n",
      "Validation loss: 0.9905463849432173....\n",
      "-----------------------------------\n",
      "Training loss: 0.9941371566542967....\n",
      "Validation loss: 0.9904411312408741....\n",
      "-----------------------------------\n",
      "Training loss: 0.9939007059944618....\n",
      "Validation loss: 0.9903361664739971....\n",
      "-----------------------------------\n",
      "Training loss: 0.993664458168841....\n",
      "Validation loss: 0.9902311328033682....\n",
      "-----------------------------------\n",
      "Training loss: 0.9934284474873178....\n",
      "Validation loss: 0.9901267797519359....\n",
      "-----------------------------------\n",
      "Training loss: 0.993192672354782....\n",
      "Validation loss: 0.99002183862497....\n",
      "-----------------------------------\n",
      "Training loss: 0.992957148163187....\n",
      "Validation loss: 0.989917496575142....\n",
      "-----------------------------------\n",
      "Training loss: 0.9927218164107826....\n",
      "Validation loss: 0.9898133813918716....\n",
      "-----------------------------------\n",
      "Training loss: 0.9924867515664197....\n",
      "Validation loss: 0.9897089829392209....\n",
      "-----------------------------------\n",
      "Training loss: 0.9922518873479462....\n",
      "Validation loss: 0.9896054011304858....\n",
      "-----------------------------------\n",
      "Training loss: 0.9920172283508708....\n",
      "Validation loss: 0.9895017320095235....\n",
      "-----------------------------------\n",
      "Training loss: 0.9917827487295535....\n",
      "Validation loss: 0.9893981335836263....\n",
      "-----------------------------------\n",
      "Training loss: 0.9915484930762648....\n",
      "Validation loss: 0.989294978917339....\n",
      "-----------------------------------\n",
      "Training loss: 0.9913145078678295....\n",
      "Validation loss: 0.9891916540143288....\n",
      "-----------------------------------\n",
      "Training loss: 0.9910807417570401....\n",
      "Validation loss: 0.9890888447781363....\n",
      "-----------------------------------\n",
      "Training loss: 0.990847199661347....\n",
      "Validation loss: 0.9889859682969504....\n",
      "-----------------------------------\n",
      "Training loss: 0.9906139293863219....\n",
      "Validation loss: 0.9888831290127376....\n",
      "-----------------------------------\n",
      "Training loss: 0.9903808336427501....\n",
      "Validation loss: 0.9887808119117864....\n",
      "-----------------------------------\n",
      "Training loss: 0.9901479512783447....\n",
      "Validation loss: 0.9886784247431091....\n",
      "-----------------------------------\n",
      "Training loss: 0.9899153545154836....\n",
      "Validation loss: 0.9885761761754813....\n",
      "-----------------------------------\n",
      "Training loss: 0.9896830012452723....\n",
      "Validation loss: 0.988474363770479....\n",
      "-----------------------------------\n",
      "Training loss: 0.9894508871325729....\n",
      "Validation loss: 0.9883728494381824....\n",
      "-----------------------------------\n",
      "Training loss: 0.9892190220781681....\n",
      "Validation loss: 0.988270776029982....\n",
      "-----------------------------------\n",
      "Training loss: 0.9889873785072221....\n",
      "Validation loss: 0.9881698071196969....\n",
      "-----------------------------------\n",
      "Training loss: 0.9887559702784232....\n",
      "Validation loss: 0.9880681827896758....\n",
      "-----------------------------------\n",
      "Training loss: 0.988524786444259....\n",
      "Validation loss: 0.9879674454854923....\n",
      "-----------------------------------\n",
      "Training loss: 0.9882938036435466....\n",
      "Validation loss: 0.9878664138874448....\n",
      "-----------------------------------\n",
      "Training loss: 0.9880630908954148....\n",
      "Validation loss: 0.9877659777881003....\n",
      "-----------------------------------\n",
      "Training loss: 0.9878325821585202....\n",
      "Validation loss: 0.9876653102160435....\n",
      "-----------------------------------\n",
      "Training loss: 0.9876023199433869....\n",
      "Validation loss: 0.9875650177147878....\n",
      "-----------------------------------\n",
      "Training loss: 0.987372307986326....\n",
      "Validation loss: 0.9874647054384976....\n",
      "-----------------------------------\n",
      "Training loss: 0.9871425282039209....\n",
      "Validation loss: 0.9873648173455565....\n",
      "-----------------------------------\n",
      "Training loss: 0.9869129524668617....\n",
      "Validation loss: 0.9872649904449151....\n",
      "-----------------------------------\n",
      "Training loss: 0.9866836297109922....\n",
      "Validation loss: 0.9871659105885499....\n",
      "-----------------------------------\n",
      "Training loss: 0.9864544897351682....\n",
      "Validation loss: 0.9870663119061295....\n",
      "-----------------------------------\n",
      "Training loss: 0.9862255195650573....\n",
      "Validation loss: 0.986967529857937....\n",
      "-----------------------------------\n",
      "Training loss: 0.985996689824422....\n",
      "Validation loss: 0.9868682114224179....\n",
      "-----------------------------------\n",
      "Training loss: 0.9857680314039813....\n",
      "Validation loss: 0.9867697369189421....\n",
      "-----------------------------------\n",
      "Training loss: 0.9855395706202658....\n",
      "Validation loss: 0.9866709876194443....\n",
      "-----------------------------------\n",
      "Training loss: 0.9853113406398366....\n",
      "Validation loss: 0.9865726090690707....\n",
      "-----------------------------------\n",
      "Training loss: 0.9850832704348904....\n",
      "Validation loss: 0.9864747897201708....\n",
      "-----------------------------------\n",
      "Training loss: 0.9848553988697245....\n",
      "Validation loss: 0.9863767757372606....\n",
      "-----------------------------------\n",
      "Training loss: 0.9846277868100908....\n",
      "Validation loss: 0.9862790063046933....\n",
      "-----------------------------------\n",
      "Training loss: 0.9844004540277156....\n",
      "Validation loss: 0.9861814077824247....\n",
      "-----------------------------------\n",
      "Training loss: 0.9841733214968744....\n",
      "Validation loss: 0.9860837555198827....\n",
      "-----------------------------------\n",
      "Training loss: 0.9839463819594081....\n",
      "Validation loss: 0.9859867091825198....\n",
      "-----------------------------------\n",
      "Training loss: 0.9837196366304859....\n",
      "Validation loss: 0.9858892008281658....\n",
      "-----------------------------------\n",
      "Training loss: 0.9834930915037186....\n",
      "Validation loss: 0.9857923300728557....\n",
      "-----------------------------------\n",
      "Training loss: 0.9832667818487939....\n",
      "Validation loss: 0.9856952615472783....\n",
      "-----------------------------------\n",
      "Training loss: 0.9830406908045536....\n",
      "Validation loss: 0.9855986871368027....\n",
      "-----------------------------------\n",
      "Training loss: 0.9828147752027928....\n",
      "Validation loss: 0.9855018868886012....\n",
      "-----------------------------------\n",
      "Training loss: 0.9825890737031553....\n",
      "Validation loss: 0.985405652912968....\n",
      "-----------------------------------\n",
      "Training loss: 0.982363587397213....\n",
      "Validation loss: 0.985308906723289....\n",
      "-----------------------------------\n",
      "Training loss: 0.9821382695788216....\n",
      "Validation loss: 0.9852128849269833....\n",
      "-----------------------------------\n",
      "Training loss: 0.9819131384312075....\n",
      "Validation loss: 0.9851165700139514....\n",
      "-----------------------------------\n",
      "Training loss: 0.9816882014886492....\n",
      "Validation loss: 0.9850207748351961....\n",
      "-----------------------------------\n",
      "Training loss: 0.9814634723897355....\n",
      "Validation loss: 0.984924774328522....\n",
      "-----------------------------------\n",
      "Training loss: 0.9812389624607057....\n",
      "Validation loss: 0.9848293699630603....\n",
      "-----------------------------------\n",
      "Training loss: 0.9810146418169025....\n",
      "Validation loss: 0.9847339442823593....\n",
      "-----------------------------------\n",
      "Training loss: 0.9807905403012626....\n",
      "Validation loss: 0.9846389254186556....\n",
      "-----------------------------------\n",
      "Training loss: 0.9805666262309919....\n",
      "Validation loss: 0.9845436240409963....\n",
      "-----------------------------------\n",
      "Training loss: 0.9803429587422532....\n",
      "Validation loss: 0.9844492193788612....\n",
      "-----------------------------------\n",
      "Training loss: 0.9801194839573009....\n",
      "Validation loss: 0.9843542230386634....\n",
      "-----------------------------------\n",
      "Training loss: 0.9798962351576139....\n",
      "Validation loss: 0.9842595071692098....\n",
      "-----------------------------------\n",
      "Training loss: 0.9796732044242292....\n",
      "Validation loss: 0.984165294907966....\n",
      "-----------------------------------\n",
      "Training loss: 0.9794503780393767....\n",
      "Validation loss: 0.9840706970148128....\n",
      "-----------------------------------\n",
      "Training loss: 0.9792277343602888....\n",
      "Validation loss: 0.9839765394515766....\n",
      "-----------------------------------\n",
      "Training loss: 0.9790052999969371....\n",
      "Validation loss: 0.9838824463053777....\n",
      "-----------------------------------\n",
      "Training loss: 0.9787830951757609....\n",
      "Validation loss: 0.9837889086575754....\n",
      "-----------------------------------\n",
      "Training loss: 0.9785611258585794....\n",
      "Validation loss: 0.9836948843470739....\n",
      "-----------------------------------\n",
      "Training loss: 0.9783393602064764....\n",
      "Validation loss: 0.9836017598924068....\n",
      "-----------------------------------\n",
      "Training loss: 0.9781177887162619....\n",
      "Validation loss: 0.98350824665432....\n",
      "-----------------------------------\n",
      "Training loss: 0.9778963328208978....\n",
      "Validation loss: 0.9834149711471596....\n",
      "-----------------------------------\n",
      "Training loss: 0.9776749844186761....\n",
      "Validation loss: 0.9833221020381577....\n",
      "-----------------------------------\n",
      "Training loss: 0.9774539240853417....\n",
      "Validation loss: 0.9832288603719398....\n",
      "-----------------------------------\n",
      "Training loss: 0.9772329942643302....\n",
      "Validation loss: 0.9831362295606136....\n",
      "-----------------------------------\n",
      "Training loss: 0.9770122866414983....\n",
      "Validation loss: 0.983043693399666....\n",
      "-----------------------------------\n",
      "Training loss: 0.9767917905021702....\n",
      "Validation loss: 0.9829510299382591....\n",
      "-----------------------------------\n",
      "Training loss: 0.9765714385206916....\n",
      "Validation loss: 0.9828585305173253....\n",
      "-----------------------------------\n",
      "Training loss: 0.9763512346137304....\n",
      "Validation loss: 0.9827663202954422....\n",
      "-----------------------------------\n",
      "Training loss: 0.9761312212563471....\n",
      "Validation loss: 0.9826742590634225....\n",
      "-----------------------------------\n",
      "Training loss: 0.9759114032031014....\n",
      "Validation loss: 0.9825819503265503....\n",
      "-----------------------------------\n",
      "Training loss: 0.9756917736546022....\n",
      "Validation loss: 0.982490485733569....\n",
      "-----------------------------------\n",
      "Training loss: 0.9754723314334557....\n",
      "Validation loss: 0.9823986022080506....\n",
      "-----------------------------------\n",
      "Training loss: 0.9752530783738648....\n",
      "Validation loss: 0.9823073799113573....\n",
      "-----------------------------------\n",
      "Training loss: 0.975034009131354....\n",
      "Validation loss: 0.9822158475537492....\n",
      "-----------------------------------\n",
      "Training loss: 0.9748151377445252....\n",
      "Validation loss: 0.9821244764279983....\n",
      "-----------------------------------\n",
      "Training loss: 0.974596434753736....\n",
      "Validation loss: 0.9820334052920032....\n",
      "-----------------------------------\n",
      "Training loss: 0.9743779087605415....\n",
      "Validation loss: 0.9819424763804417....\n",
      "-----------------------------------\n",
      "Training loss: 0.9741595972826905....\n",
      "Validation loss: 0.9818513066549746....\n",
      "-----------------------------------\n",
      "Training loss: 0.9739414403108586....\n",
      "Validation loss: 0.9817605561286785....\n",
      "-----------------------------------\n",
      "Training loss: 0.9737234655840701....\n",
      "Validation loss: 0.9816700380074946....\n",
      "-----------------------------------\n",
      "Training loss: 0.9735057264069921....\n",
      "Validation loss: 0.981579418261196....\n",
      "-----------------------------------\n",
      "Training loss: 0.9732882373866653....\n",
      "Validation loss: 0.9814897658030796....\n",
      "-----------------------------------\n",
      "Training loss: 0.9730709851082133....\n",
      "Validation loss: 0.9813995212909392....\n",
      "-----------------------------------\n",
      "Training loss: 0.9728539183795639....\n",
      "Validation loss: 0.9813095666751827....\n",
      "-----------------------------------\n",
      "Training loss: 0.9726370579590379....\n",
      "Validation loss: 0.9812201861934093....\n",
      "-----------------------------------\n",
      "Training loss: 0.9724203976196293....\n",
      "Validation loss: 0.9811305623974113....\n",
      "-----------------------------------\n",
      "Training loss: 0.9722039050924718....\n",
      "Validation loss: 0.9810408253776611....\n",
      "-----------------------------------\n",
      "Training loss: 0.9719876221329625....\n",
      "Validation loss: 0.9809516062198801....\n",
      "-----------------------------------\n",
      "Training loss: 0.9717715580044355....\n",
      "Validation loss: 0.9808627299073087....\n",
      "-----------------------------------\n",
      "Training loss: 0.9715556968213714....\n",
      "Validation loss: 0.9807735832055766....\n",
      "-----------------------------------\n",
      "Training loss: 0.9713399934750242....\n",
      "Validation loss: 0.9806844676569929....\n",
      "-----------------------------------\n",
      "Training loss: 0.9711244207348618....\n",
      "Validation loss: 0.9805955792164115....\n",
      "-----------------------------------\n",
      "Training loss: 0.9709090837041237....\n",
      "Validation loss: 0.980506742662453....\n",
      "-----------------------------------\n",
      "Training loss: 0.9706939468324961....\n",
      "Validation loss: 0.9804181796668465....\n",
      "-----------------------------------\n",
      "Training loss: 0.9704790630368193....\n",
      "Validation loss: 0.9803297505500826....\n",
      "-----------------------------------\n",
      "Training loss: 0.9702643988905258....\n",
      "Validation loss: 0.9802413015262389....\n",
      "-----------------------------------\n",
      "Training loss: 0.9700499670188735....\n",
      "Validation loss: 0.9801535825114017....\n",
      "-----------------------------------\n",
      "Training loss: 0.969835728226792....\n",
      "Validation loss: 0.980065408076561....\n",
      "-----------------------------------\n",
      "Training loss: 0.9696216748700177....\n",
      "Validation loss: 0.9799776976533439....\n",
      "-----------------------------------\n",
      "Training loss: 0.9694078464865613....\n",
      "Validation loss: 0.9798900271495269....\n",
      "-----------------------------------\n",
      "Training loss: 0.9691942141865404....\n",
      "Validation loss: 0.979802876737241....\n",
      "-----------------------------------\n",
      "Training loss: 0.9689807388711111....\n",
      "Validation loss: 0.9797152787324998....\n",
      "-----------------------------------\n",
      "Training loss: 0.9687674415138003....\n",
      "Validation loss: 0.9796281314522667....\n",
      "-----------------------------------\n",
      "Training loss: 0.968554305600933....\n",
      "Validation loss: 0.9795412334390915....\n",
      "-----------------------------------\n",
      "Training loss: 0.9683413337646557....\n",
      "Validation loss: 0.9794544121786395....\n",
      "-----------------------------------\n",
      "Training loss: 0.9681284881078728....\n",
      "Validation loss: 0.9793680946580037....\n",
      "-----------------------------------\n",
      "Training loss: 0.9679157677959417....\n",
      "Validation loss: 0.9792812725531306....\n",
      "-----------------------------------\n",
      "Training loss: 0.9677032007790296....\n",
      "Validation loss: 0.9791949190244374....\n",
      "-----------------------------------\n",
      "Training loss: 0.9674908351421384....\n",
      "Validation loss: 0.9791087417209263....\n",
      "-----------------------------------\n",
      "Training loss: 0.967278681581979....\n",
      "Validation loss: 0.9790229738767958....\n",
      "-----------------------------------\n",
      "Training loss: 0.9670667013249622....\n",
      "Validation loss: 0.9789371437041225....\n",
      "-----------------------------------\n",
      "Training loss: 0.9668549217875507....\n",
      "Validation loss: 0.9788515429331983....\n",
      "-----------------------------------\n",
      "Training loss: 0.9666433207832454....\n",
      "Validation loss: 0.978766061337223....\n",
      "-----------------------------------\n",
      "Training loss: 0.9664319234094773....\n",
      "Validation loss: 0.978680220930308....\n",
      "-----------------------------------\n",
      "Training loss: 0.9662207061808938....\n",
      "Validation loss: 0.9785953966798067....\n",
      "-----------------------------------\n",
      "Training loss: 0.9660096588546623....\n",
      "Validation loss: 0.9785103272718116....\n",
      "-----------------------------------\n",
      "Training loss: 0.9657988061872288....\n",
      "Validation loss: 0.9784253999047632....\n",
      "-----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.72      0.66       499\n",
      "           2       0.63      0.59      0.61       500\n",
      "           3       0.62      0.57      0.60       500\n",
      "           4       0.61      0.62      0.61       500\n",
      "           5       0.60      0.53      0.57       500\n",
      "           6       0.66      0.67      0.67       500\n",
      "           7       0.60      0.62      0.61       500\n",
      "           8       0.51      0.51      0.51       500\n",
      "           9       0.55      0.52      0.53       500\n",
      "          10       0.69      0.75      0.72       500\n",
      "\n",
      "    accuracy                           0.61      4999\n",
      "   macro avg       0.61      0.61      0.61      4999\n",
      "weighted avg       0.61      0.61      0.61      4999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distortion 0.5\n",
    "xtr2= distortion(0.5,X_train_arr,X_train_arr.shape[0])\n",
    "xtest2=distortion(0.5,X_test_arr,X_test_arr.shape[0])\n",
    "C2 = NN(0.01)\n",
    "C2.add_layer(50,256,'relu')\n",
    "C2.add_layer(10,50,'softmax')\n",
    "t_loss,v_loss= train_and_validate(C2,xtr2,Y_train_arr,n=3)\n",
    "report2 = test(C2,xtest2)\n",
    "print(report2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e5b8f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.6380931014088485....\n",
      "Validation loss: 2.603681316139521....\n",
      "-----------------------------------\n",
      "Training loss: 2.613104860718649....\n",
      "Validation loss: 2.582746703027099....\n",
      "-----------------------------------\n",
      "Training loss: 2.5921795203520004....\n",
      "Validation loss: 2.565037816584401....\n",
      "-----------------------------------\n",
      "Training loss: 2.5744183080323033....\n",
      "Validation loss: 2.5497655532099115....\n",
      "-----------------------------------\n",
      "Training loss: 2.559138158879592....\n",
      "Validation loss: 2.5365873311102862....\n",
      "-----------------------------------\n",
      "Training loss: 2.545947885553299....\n",
      "Validation loss: 2.525100028474145....\n",
      "-----------------------------------\n",
      "Training loss: 2.53440665071078....\n",
      "Validation loss: 2.5149813394005585....\n",
      "-----------------------------------\n",
      "Training loss: 2.524189379294166....\n",
      "Validation loss: 2.506058891808003....\n",
      "-----------------------------------\n",
      "Training loss: 2.515158955653528....\n",
      "Validation loss: 2.498099498035612....\n",
      "-----------------------------------\n",
      "Training loss: 2.5071065006641295....\n",
      "Validation loss: 2.4909893461603945....\n",
      "-----------------------------------\n",
      "Training loss: 2.4998916198521757....\n",
      "Validation loss: 2.4846100589388436....\n",
      "-----------------------------------\n",
      "Training loss: 2.493410628577357....\n",
      "Validation loss: 2.4789024571600335....\n",
      "-----------------------------------\n",
      "Training loss: 2.487581731657128....\n",
      "Validation loss: 2.4737485704704314....\n",
      "-----------------------------------\n",
      "Training loss: 2.4822829973171943....\n",
      "Validation loss: 2.469069278992486....\n",
      "-----------------------------------\n",
      "Training loss: 2.4774554163838953....\n",
      "Validation loss: 2.4648006117603627....\n",
      "-----------------------------------\n",
      "Training loss: 2.473037966288877....\n",
      "Validation loss: 2.460912493285024....\n",
      "-----------------------------------\n",
      "Training loss: 2.4689893521728106....\n",
      "Validation loss: 2.45738199237021....\n",
      "-----------------------------------\n",
      "Training loss: 2.4652706372414976....\n",
      "Validation loss: 2.454176105136194....\n",
      "-----------------------------------\n",
      "Training loss: 2.4618452006357847....\n",
      "Validation loss: 2.451241929241792....\n",
      "-----------------------------------\n",
      "Training loss: 2.458680466489434....\n",
      "Validation loss: 2.448545121948047....\n",
      "-----------------------------------\n",
      "Training loss: 2.4557584743269634....\n",
      "Validation loss: 2.44606736739353....\n",
      "-----------------------------------\n",
      "Training loss: 2.4530501739211887....\n",
      "Validation loss: 2.4437578716598742....\n",
      "-----------------------------------\n",
      "Training loss: 2.4505128295443583....\n",
      "Validation loss: 2.4416182526540333....\n",
      "-----------------------------------\n",
      "Training loss: 2.448146992673042....\n",
      "Validation loss: 2.4396214099035696....\n",
      "-----------------------------------\n",
      "Training loss: 2.445926687579639....\n",
      "Validation loss: 2.4377635515317797....\n",
      "-----------------------------------\n",
      "Training loss: 2.443861224434214....\n",
      "Validation loss: 2.436040356566405....\n",
      "-----------------------------------\n",
      "Training loss: 2.44191502177111....\n",
      "Validation loss: 2.4344127493899856....\n",
      "-----------------------------------\n",
      "Training loss: 2.4400579244272045....\n",
      "Validation loss: 2.432881042093454....\n",
      "-----------------------------------\n",
      "Training loss: 2.438300562749369....\n",
      "Validation loss: 2.4314408029543446....\n",
      "-----------------------------------\n",
      "Training loss: 2.4366352945038288....\n",
      "Validation loss: 2.430080667229169....\n",
      "-----------------------------------\n",
      "Training loss: 2.4350468149488984....\n",
      "Validation loss: 2.4288017326337283....\n",
      "-----------------------------------\n",
      "Training loss: 2.433532764112287....\n",
      "Validation loss: 2.4275939983171515....\n",
      "-----------------------------------\n",
      "Training loss: 2.4320843912616197....\n",
      "Validation loss: 2.4264484434260734....\n",
      "-----------------------------------\n",
      "Training loss: 2.430697990195686....\n",
      "Validation loss: 2.4253571211322287....\n",
      "-----------------------------------\n",
      "Training loss: 2.429371292342614....\n",
      "Validation loss: 2.424319499273515....\n",
      "-----------------------------------\n",
      "Training loss: 2.428096583607129....\n",
      "Validation loss: 2.4233314838471096....\n",
      "-----------------------------------\n",
      "Training loss: 2.426870187667101....\n",
      "Validation loss: 2.42239513809085....\n",
      "-----------------------------------\n",
      "Training loss: 2.425693268228539....\n",
      "Validation loss: 2.4214944149162556....\n",
      "-----------------------------------\n",
      "Training loss: 2.424560316914336....\n",
      "Validation loss: 2.4206277892798225....\n",
      "-----------------------------------\n",
      "Training loss: 2.423463369815955....\n",
      "Validation loss: 2.4197993965847058....\n",
      "-----------------------------------\n",
      "Training loss: 2.422402388772591....\n",
      "Validation loss: 2.4190072781006857....\n",
      "-----------------------------------\n",
      "Training loss: 2.4213749022777185....\n",
      "Validation loss: 2.4182389101947788....\n",
      "-----------------------------------\n",
      "Training loss: 2.4203716916256335....\n",
      "Validation loss: 2.417491425024456....\n",
      "-----------------------------------\n",
      "Training loss: 2.419391918175243....\n",
      "Validation loss: 2.416771888376804....\n",
      "-----------------------------------\n",
      "Training loss: 2.418437028168481....\n",
      "Validation loss: 2.416073718777733....\n",
      "-----------------------------------\n",
      "Training loss: 2.41750739856321....\n",
      "Validation loss: 2.415397807213117....\n",
      "-----------------------------------\n",
      "Training loss: 2.416600833517186....\n",
      "Validation loss: 2.414735923049262....\n",
      "-----------------------------------\n",
      "Training loss: 2.4157112331335284....\n",
      "Validation loss: 2.4140943425828354....\n",
      "-----------------------------------\n",
      "Training loss: 2.4148452686197426....\n",
      "Validation loss: 2.4134737600493206....\n",
      "-----------------------------------\n",
      "Training loss: 2.413999427299152....\n",
      "Validation loss: 2.4128729221517227....\n",
      "-----------------------------------\n",
      "Training loss: 2.413173860926934....\n",
      "Validation loss: 2.412287421480744....\n",
      "-----------------------------------\n",
      "Training loss: 2.412366000690633....\n",
      "Validation loss: 2.411716642659553....\n",
      "-----------------------------------\n",
      "Training loss: 2.4115736634678813....\n",
      "Validation loss: 2.411157255019891....\n",
      "-----------------------------------\n",
      "Training loss: 2.410794977975954....\n",
      "Validation loss: 2.4106108187967186....\n",
      "-----------------------------------\n",
      "Training loss: 2.4100339720853454....\n",
      "Validation loss: 2.410071701629777....\n",
      "-----------------------------------\n",
      "Training loss: 2.409282992214536....\n",
      "Validation loss: 2.409542518349213....\n",
      "-----------------------------------\n",
      "Training loss: 2.4085439444825707....\n",
      "Validation loss: 2.4090228574768098....\n",
      "-----------------------------------\n",
      "Training loss: 2.4078182528583243....\n",
      "Validation loss: 2.4085129410363373....\n",
      "-----------------------------------\n",
      "Training loss: 2.4071038858549065....\n",
      "Validation loss: 2.408012095671805....\n",
      "-----------------------------------\n",
      "Training loss: 2.406399666529718....\n",
      "Validation loss: 2.407517356502192....\n",
      "-----------------------------------\n",
      "Training loss: 2.405701298440118....\n",
      "Validation loss: 2.407029503359965....\n",
      "-----------------------------------\n",
      "Training loss: 2.4050104325640222....\n",
      "Validation loss: 2.406549099445849....\n",
      "-----------------------------------\n",
      "Training loss: 2.4043292919059214....\n",
      "Validation loss: 2.40607239814645....\n",
      "-----------------------------------\n",
      "Training loss: 2.4036509796754046....\n",
      "Validation loss: 2.4056006089650634....\n",
      "-----------------------------------\n",
      "Training loss: 2.4029777087790345....\n",
      "Validation loss: 2.4051326505315664....\n",
      "-----------------------------------\n",
      "Training loss: 2.4023104071159653....\n",
      "Validation loss: 2.4046694790968957....\n",
      "-----------------------------------\n",
      "Training loss: 2.401649551522072....\n",
      "Validation loss: 2.4042151767633335....\n",
      "-----------------------------------\n",
      "Training loss: 2.400995385571999....\n",
      "Validation loss: 2.4037710086470714....\n",
      "-----------------------------------\n",
      "Training loss: 2.4003469271348656....\n",
      "Validation loss: 2.4033339361556028....\n",
      "-----------------------------------\n",
      "Training loss: 2.3997042039817353....\n",
      "Validation loss: 2.4029032851820364....\n",
      "-----------------------------------\n",
      "Training loss: 2.3990691061502676....\n",
      "Validation loss: 2.4024758767907457....\n",
      "-----------------------------------\n",
      "Training loss: 2.3984391280211717....\n",
      "Validation loss: 2.4020557840360754....\n",
      "-----------------------------------\n",
      "Training loss: 2.3978196000689778....\n",
      "Validation loss: 2.4016423700719174....\n",
      "-----------------------------------\n",
      "Training loss: 2.3972056101165804....\n",
      "Validation loss: 2.401232459661008....\n",
      "-----------------------------------\n",
      "Training loss: 2.396598173164156....\n",
      "Validation loss: 2.4008274866883523....\n",
      "-----------------------------------\n",
      "Training loss: 2.3959987861071648....\n",
      "Validation loss: 2.4004277332594737....\n",
      "-----------------------------------\n",
      "Training loss: 2.395404926535771....\n",
      "Validation loss: 2.400031054669517....\n",
      "-----------------------------------\n",
      "Training loss: 2.3948186201403514....\n",
      "Validation loss: 2.3996380061523865....\n",
      "-----------------------------------\n",
      "Training loss: 2.3942384344959713....\n",
      "Validation loss: 2.3992452786096563....\n",
      "-----------------------------------\n",
      "Training loss: 2.393660062616644....\n",
      "Validation loss: 2.398851477261375....\n",
      "-----------------------------------\n",
      "Training loss: 2.393083003236947....\n",
      "Validation loss: 2.3984596505758287....\n",
      "-----------------------------------\n",
      "Training loss: 2.3925117269503895....\n",
      "Validation loss: 2.3980737441206976....\n",
      "-----------------------------------\n",
      "Training loss: 2.3919477429993066....\n",
      "Validation loss: 2.3976904043253757....\n",
      "-----------------------------------\n",
      "Training loss: 2.391385570533565....\n",
      "Validation loss: 2.3973086950745586....\n",
      "-----------------------------------\n",
      "Training loss: 2.3908248615655467....\n",
      "Validation loss: 2.396928897471662....\n",
      "-----------------------------------\n",
      "Training loss: 2.3902664701864875....\n",
      "Validation loss: 2.3965497402131333....\n",
      "-----------------------------------\n",
      "Training loss: 2.389712838727346....\n",
      "Validation loss: 2.3961748869317288....\n",
      "-----------------------------------\n",
      "Training loss: 2.389162337254292....\n",
      "Validation loss: 2.3958000780642292....\n",
      "-----------------------------------\n",
      "Training loss: 2.3886136073267674....\n",
      "Validation loss: 2.395424893566146....\n",
      "-----------------------------------\n",
      "Training loss: 2.3880684695971652....\n",
      "Validation loss: 2.3950503304773685....\n",
      "-----------------------------------\n",
      "Training loss: 2.3875267203942014....\n",
      "Validation loss: 2.3946761990685292....\n",
      "-----------------------------------\n",
      "Training loss: 2.3869876916650457....\n",
      "Validation loss: 2.3943053692527902....\n",
      "-----------------------------------\n",
      "Training loss: 2.386452480342212....\n",
      "Validation loss: 2.3939366901568024....\n",
      "-----------------------------------\n",
      "Training loss: 2.3859200791883386....\n",
      "Validation loss: 2.393573343346143....\n",
      "-----------------------------------\n",
      "Training loss: 2.385391971623267....\n",
      "Validation loss: 2.393213528479....\n",
      "-----------------------------------\n",
      "Training loss: 2.38486742408501....\n",
      "Validation loss: 2.392855039117384....\n",
      "-----------------------------------\n",
      "Training loss: 2.384347088856614....\n",
      "Validation loss: 2.39249755062127....\n",
      "-----------------------------------\n",
      "Training loss: 2.383830066824362....\n",
      "Validation loss: 2.3921440529892943....\n",
      "-----------------------------------\n",
      "Training loss: 2.3833160608581045....\n",
      "Validation loss: 2.3917953130543794....\n",
      "-----------------------------------\n",
      "Training loss: 2.382806510963816....\n",
      "Validation loss: 2.391448854344404....\n",
      "-----------------------------------\n",
      "Training loss: 2.382299280934381....\n",
      "Validation loss: 2.391104579256002....\n",
      "-----------------------------------\n",
      "Training loss: 2.3817946648330937....\n",
      "Validation loss: 2.3907636639548095....\n",
      "-----------------------------------\n",
      "Training loss: 2.3812944271542653....\n",
      "Validation loss: 2.390423629609593....\n",
      "-----------------------------------\n",
      "Training loss: 2.3807959913947303....\n",
      "Validation loss: 2.390085248002794....\n",
      "-----------------------------------\n",
      "Training loss: 2.38030253571106....\n",
      "Validation loss: 2.3897520271480057....\n",
      "-----------------------------------\n",
      "Training loss: 2.3798122286257106....\n",
      "Validation loss: 2.3894213981123964....\n",
      "-----------------------------------\n",
      "Training loss: 2.37932316995697....\n",
      "Validation loss: 2.3890944655941415....\n",
      "-----------------------------------\n",
      "Training loss: 2.3788385340783416....\n",
      "Validation loss: 2.388770651784175....\n",
      "-----------------------------------\n",
      "Training loss: 2.378359641367939....\n",
      "Validation loss: 2.3884517341659675....\n",
      "-----------------------------------\n",
      "Training loss: 2.3778852860651916....\n",
      "Validation loss: 2.3881363814289402....\n",
      "-----------------------------------\n",
      "Training loss: 2.377413925075416....\n",
      "Validation loss: 2.387823638810127....\n",
      "-----------------------------------\n",
      "Training loss: 2.3769437180393305....\n",
      "Validation loss: 2.3875130365847053....\n",
      "-----------------------------------\n",
      "Training loss: 2.3764754740630427....\n",
      "Validation loss: 2.3872065642344977....\n",
      "-----------------------------------\n",
      "Training loss: 2.376008407078409....\n",
      "Validation loss: 2.3869015314546043....\n",
      "-----------------------------------\n",
      "Training loss: 2.3755414380795403....\n",
      "Validation loss: 2.386599738585961....\n",
      "-----------------------------------\n",
      "Training loss: 2.3750770894438054....\n",
      "Validation loss: 2.386299417838131....\n",
      "-----------------------------------\n",
      "Training loss: 2.3746139128450516....\n",
      "Validation loss: 2.3860000913580537....\n",
      "-----------------------------------\n",
      "Training loss: 2.374150229355821....\n",
      "Validation loss: 2.3857015591652817....\n",
      "-----------------------------------\n",
      "Training loss: 2.37368780572895....\n",
      "Validation loss: 2.3854036615514618....\n",
      "-----------------------------------\n",
      "Training loss: 2.3732271092967077....\n",
      "Validation loss: 2.385106323090486....\n",
      "-----------------------------------\n",
      "Training loss: 2.3727698926271463....\n",
      "Validation loss: 2.3848109773124526....\n",
      "-----------------------------------\n",
      "Training loss: 2.372315532524524....\n",
      "Validation loss: 2.3845174978789645....\n",
      "-----------------------------------\n",
      "Training loss: 2.3718644258385204....\n",
      "Validation loss: 2.384224927839722....\n",
      "-----------------------------------\n",
      "Training loss: 2.3714161080443033....\n",
      "Validation loss: 2.3839337264792086....\n",
      "-----------------------------------\n",
      "Training loss: 2.3709688534506044....\n",
      "Validation loss: 2.3836471199400164....\n",
      "-----------------------------------\n",
      "Training loss: 2.370524935058005....\n",
      "Validation loss: 2.383362230955747....\n",
      "-----------------------------------\n",
      "Training loss: 2.370083238075433....\n",
      "Validation loss: 2.3830770433450237....\n",
      "-----------------------------------\n",
      "Training loss: 2.3696445843907923....\n",
      "Validation loss: 2.3827910721928443....\n",
      "-----------------------------------\n",
      "Training loss: 2.3692088829770084....\n",
      "Validation loss: 2.382508032706091....\n",
      "-----------------------------------\n",
      "Training loss: 2.3687777631562206....\n",
      "Validation loss: 2.3822291209363105....\n",
      "-----------------------------------\n",
      "Training loss: 2.3683489312931636....\n",
      "Validation loss: 2.3819519126381756....\n",
      "-----------------------------------\n",
      "Training loss: 2.367922200993416....\n",
      "Validation loss: 2.381675035827946....\n",
      "-----------------------------------\n",
      "Training loss: 2.367497435390049....\n",
      "Validation loss: 2.3813987962260854....\n",
      "-----------------------------------\n",
      "Training loss: 2.367073827589185....\n",
      "Validation loss: 2.3811236612250295....\n",
      "-----------------------------------\n",
      "Training loss: 2.366653203942587....\n",
      "Validation loss: 2.380850065131535....\n",
      "-----------------------------------\n",
      "Training loss: 2.3662341951495365....\n",
      "Validation loss: 2.380577835279777....\n",
      "-----------------------------------\n",
      "Training loss: 2.365817900951583....\n",
      "Validation loss: 2.380307430595899....\n",
      "-----------------------------------\n",
      "Training loss: 2.3654024977359915....\n",
      "Validation loss: 2.38003892686587....\n",
      "-----------------------------------\n",
      "Training loss: 2.364989103198895....\n",
      "Validation loss: 2.3797741351150195....\n",
      "-----------------------------------\n",
      "Training loss: 2.3645776825459763....\n",
      "Validation loss: 2.379511461259497....\n",
      "-----------------------------------\n",
      "Training loss: 2.3641683087624594....\n",
      "Validation loss: 2.379250199638427....\n",
      "-----------------------------------\n",
      "Training loss: 2.363761634806281....\n",
      "Validation loss: 2.378989217854195....\n",
      "-----------------------------------\n",
      "Training loss: 2.363355223145306....\n",
      "Validation loss: 2.378728865800681....\n",
      "-----------------------------------\n",
      "Training loss: 2.3629508969769457....\n",
      "Validation loss: 2.3784709602063403....\n",
      "-----------------------------------\n",
      "Training loss: 2.3625491226707176....\n",
      "Validation loss: 2.378213560060033....\n",
      "-----------------------------------\n",
      "Training loss: 2.3621479747360423....\n",
      "Validation loss: 2.377957792475858....\n",
      "-----------------------------------\n",
      "Training loss: 2.3617494421561718....\n",
      "Validation loss: 2.3777018535235483....\n",
      "-----------------------------------\n",
      "Training loss: 2.361351827286262....\n",
      "Validation loss: 2.3774476738309103....\n",
      "-----------------------------------\n",
      "Training loss: 2.3609575451219835....\n",
      "Validation loss: 2.3771942848223655....\n",
      "-----------------------------------\n",
      "Training loss: 2.3605653664137134....\n",
      "Validation loss: 2.376942313671695....\n",
      "-----------------------------------\n",
      "Training loss: 2.3601733210045714....\n",
      "Validation loss: 2.3766925661366476....\n",
      "-----------------------------------\n",
      "Training loss: 2.3597832108561088....\n",
      "Validation loss: 2.376443468839866....\n",
      "-----------------------------------\n",
      "Training loss: 2.3593941718027676....\n",
      "Validation loss: 2.3761948899587266....\n",
      "-----------------------------------\n",
      "Training loss: 2.3590075653151663....\n",
      "Validation loss: 2.375948719830678....\n",
      "-----------------------------------\n",
      "Training loss: 2.3586243856970848....\n",
      "Validation loss: 2.375704831293563....\n",
      "-----------------------------------\n",
      "Training loss: 2.3582436161897498....\n",
      "Validation loss: 2.3754618691951452....\n",
      "-----------------------------------\n",
      "Training loss: 2.357864273328747....\n",
      "Validation loss: 2.3752209016387695....\n",
      "-----------------------------------\n",
      "Training loss: 2.3574871818835623....\n",
      "Validation loss: 2.3749812304977107....\n",
      "-----------------------------------\n",
      "Training loss: 2.3571115908792213....\n",
      "Validation loss: 2.3747414524812585....\n",
      "-----------------------------------\n",
      "Training loss: 2.3567375528048986....\n",
      "Validation loss: 2.374501811263481....\n",
      "-----------------------------------\n",
      "Training loss: 2.3563656049389534....\n",
      "Validation loss: 2.3742644999104763....\n",
      "-----------------------------------\n",
      "Training loss: 2.355995181703895....\n",
      "Validation loss: 2.374027065734773....\n",
      "-----------------------------------\n",
      "Training loss: 2.3556256371255655....\n",
      "Validation loss: 2.373790251300401....\n",
      "-----------------------------------\n",
      "Training loss: 2.3552565622536674....\n",
      "Validation loss: 2.373554291492203....\n",
      "-----------------------------------\n",
      "Training loss: 2.3548890015299957....\n",
      "Validation loss: 2.3733207412303203....\n",
      "-----------------------------------\n",
      "Training loss: 2.3545244927791624....\n",
      "Validation loss: 2.373091404331033....\n",
      "-----------------------------------\n",
      "Training loss: 2.3541632092659146....\n",
      "Validation loss: 2.372863165759368....\n",
      "-----------------------------------\n",
      "Training loss: 2.3538047754255675....\n",
      "Validation loss: 2.372635973457301....\n",
      "-----------------------------------\n",
      "Training loss: 2.353449741801195....\n",
      "Validation loss: 2.372410616440213....\n",
      "-----------------------------------\n",
      "Training loss: 2.3530978957072124....\n",
      "Validation loss: 2.3721861593989013....\n",
      "-----------------------------------\n",
      "Training loss: 2.352747465598623....\n",
      "Validation loss: 2.371963410306308....\n",
      "-----------------------------------\n",
      "Training loss: 2.352398374415334....\n",
      "Validation loss: 2.3717424365217155....\n",
      "-----------------------------------\n",
      "Training loss: 2.3520513841373067....\n",
      "Validation loss: 2.3715219808639882....\n",
      "-----------------------------------\n",
      "Training loss: 2.3517059498470982....\n",
      "Validation loss: 2.371303201310519....\n",
      "-----------------------------------\n",
      "Training loss: 2.3513610746943057....\n",
      "Validation loss: 2.3710846080238768....\n",
      "-----------------------------------\n",
      "Training loss: 2.3510161936256315....\n",
      "Validation loss: 2.3708650344948654....\n",
      "-----------------------------------\n",
      "Training loss: 2.350670818145459....\n",
      "Validation loss: 2.370646704667705....\n",
      "-----------------------------------\n",
      "Training loss: 2.3503263525569604....\n",
      "Validation loss: 2.37043005734098....\n",
      "-----------------------------------\n",
      "Training loss: 2.3499835459471528....\n",
      "Validation loss: 2.3702150601029484....\n",
      "-----------------------------------\n",
      "Training loss: 2.349642017463143....\n",
      "Validation loss: 2.3700013625579706....\n",
      "-----------------------------------\n",
      "Training loss: 2.3493009965429423....\n",
      "Validation loss: 2.369788723892625....\n",
      "-----------------------------------\n",
      "Training loss: 2.348962585090861....\n",
      "Validation loss: 2.3695782283061906....\n",
      "-----------------------------------\n",
      "Training loss: 2.3486259629290296....\n",
      "Validation loss: 2.369369443689204....\n",
      "-----------------------------------\n",
      "Training loss: 2.3482899052680137....\n",
      "Validation loss: 2.369161616070402....\n",
      "-----------------------------------\n",
      "Training loss: 2.347953858625928....\n",
      "Validation loss: 2.36895428926138....\n",
      "-----------------------------------\n",
      "Training loss: 2.347619428176406....\n",
      "Validation loss: 2.3687484922478648....\n",
      "-----------------------------------\n",
      "Training loss: 2.3472862623461372....\n",
      "Validation loss: 2.3685415845417808....\n",
      "-----------------------------------\n",
      "Training loss: 2.34695338591547....\n",
      "Validation loss: 2.36833647017868....\n",
      "-----------------------------------\n",
      "Training loss: 2.346622452427489....\n",
      "Validation loss: 2.368131510241903....\n",
      "-----------------------------------\n",
      "Training loss: 2.3462922278686182....\n",
      "Validation loss: 2.367927777357542....\n",
      "-----------------------------------\n",
      "Training loss: 2.3459648623042226....\n",
      "Validation loss: 2.367724308276046....\n",
      "-----------------------------------\n",
      "Training loss: 2.345638172741392....\n",
      "Validation loss: 2.367521103409823....\n",
      "-----------------------------------\n",
      "Training loss: 2.345313785869881....\n",
      "Validation loss: 2.3673188873852906....\n",
      "-----------------------------------\n",
      "Training loss: 2.344991728352631....\n",
      "Validation loss: 2.3671186299162854....\n",
      "-----------------------------------\n",
      "Training loss: 2.3446712110573737....\n",
      "Validation loss: 2.3669194388132526....\n",
      "-----------------------------------\n",
      "Training loss: 2.344352330743219....\n",
      "Validation loss: 2.3667206518267796....\n",
      "-----------------------------------\n",
      "Training loss: 2.34403583496032....\n",
      "Validation loss: 2.366523347295277....\n",
      "-----------------------------------\n",
      "Training loss: 2.343719844075623....\n",
      "Validation loss: 2.366326886035577....\n",
      "-----------------------------------\n",
      "Training loss: 2.3434041973302473....\n",
      "Validation loss: 2.36613077507846....\n",
      "-----------------------------------\n",
      "Training loss: 2.3430888654693547....\n",
      "Validation loss: 2.365935049764875....\n",
      "-----------------------------------\n",
      "Training loss: 2.3427745566315332....\n",
      "Validation loss: 2.3657403051286146....\n",
      "-----------------------------------\n",
      "Training loss: 2.3424614900753853....\n",
      "Validation loss: 2.3655447835637062....\n",
      "-----------------------------------\n",
      "Training loss: 2.342148430659805....\n",
      "Validation loss: 2.3653489184907066....\n",
      "-----------------------------------\n",
      "Training loss: 2.341835972570848....\n",
      "Validation loss: 2.365154434987665....\n",
      "-----------------------------------\n",
      "Training loss: 2.3415242045212077....\n",
      "Validation loss: 2.3649609483495295....\n",
      "-----------------------------------\n",
      "Training loss: 2.3412125283590703....\n",
      "Validation loss: 2.3647675938691064....\n",
      "-----------------------------------\n",
      "Training loss: 2.3409012223573815....\n",
      "Validation loss: 2.3645749876043336....\n",
      "-----------------------------------\n",
      "Training loss: 2.34059038836393....\n",
      "Validation loss: 2.3643824212633207....\n",
      "-----------------------------------\n",
      "Training loss: 2.3402801387515346....\n",
      "Validation loss: 2.364190366416383....\n",
      "-----------------------------------\n",
      "Training loss: 2.3399707310914186....\n",
      "Validation loss: 2.3639996721474543....\n",
      "-----------------------------------\n",
      "Training loss: 2.339662535579013....\n",
      "Validation loss: 2.3638098016902704....\n",
      "-----------------------------------\n",
      "Training loss: 2.3393566759263296....\n",
      "Validation loss: 2.3636204163041747....\n",
      "-----------------------------------\n",
      "Training loss: 2.3390522548698462....\n",
      "Validation loss: 2.363432206362234....\n",
      "-----------------------------------\n",
      "Training loss: 2.3387497738670757....\n",
      "Validation loss: 2.363244744186678....\n",
      "-----------------------------------\n",
      "Training loss: 2.3384483009020607....\n",
      "Validation loss: 2.3630576747719143....\n",
      "-----------------------------------\n",
      "Training loss: 2.338147240846389....\n",
      "Validation loss: 2.3628706068945013....\n",
      "-----------------------------------\n",
      "Training loss: 2.3378473811264446....\n",
      "Validation loss: 2.3626836728107996....\n",
      "-----------------------------------\n",
      "Training loss: 2.3375479202628107....\n",
      "Validation loss: 2.3624973410786434....\n",
      "-----------------------------------\n",
      "Training loss: 2.337249775450794....\n",
      "Validation loss: 2.3623119182551746....\n",
      "-----------------------------------\n",
      "Training loss: 2.3369535733420053....\n",
      "Validation loss: 2.3621279704190083....\n",
      "-----------------------------------\n",
      "Training loss: 2.336659023593948....\n",
      "Validation loss: 2.361944901052386....\n",
      "-----------------------------------\n",
      "Training loss: 2.3363666915165933....\n",
      "Validation loss: 2.3617625342356963....\n",
      "-----------------------------------\n",
      "Training loss: 2.3360759217699956....\n",
      "Validation loss: 2.3615805316358185....\n",
      "-----------------------------------\n",
      "Training loss: 2.3357859988954393....\n",
      "Validation loss: 2.3613989432086773....\n",
      "-----------------------------------\n",
      "Training loss: 2.3354974386153633....\n",
      "Validation loss: 2.3612192887818813....\n",
      "-----------------------------------\n",
      "Training loss: 2.335210372246783....\n",
      "Validation loss: 2.3610397531981797....\n",
      "-----------------------------------\n",
      "Training loss: 2.3349235053536943....\n",
      "Validation loss: 2.360860776751416....\n",
      "-----------------------------------\n",
      "Training loss: 2.3346372053247673....\n",
      "Validation loss: 2.3606823472613376....\n",
      "-----------------------------------\n",
      "Training loss: 2.3343509444818373....\n",
      "Validation loss: 2.360505141734682....\n",
      "-----------------------------------\n",
      "Training loss: 2.3340654498732025....\n",
      "Validation loss: 2.3603284346313167....\n",
      "-----------------------------------\n",
      "Training loss: 2.333781195253672....\n",
      "Validation loss: 2.3601529022397....\n",
      "-----------------------------------\n",
      "Training loss: 2.3334975695601....\n",
      "Validation loss: 2.3599771064759008....\n",
      "-----------------------------------\n",
      "Training loss: 2.333214713827666....\n",
      "Validation loss: 2.3598019737747764....\n",
      "-----------------------------------\n",
      "Training loss: 2.3329340877712847....\n",
      "Validation loss: 2.3596271757577005....\n",
      "-----------------------------------\n",
      "Training loss: 2.332655249632855....\n",
      "Validation loss: 2.359453520121779....\n",
      "-----------------------------------\n",
      "Training loss: 2.332378024836388....\n",
      "Validation loss: 2.3592812014829323....\n",
      "-----------------------------------\n",
      "Training loss: 2.332101477160689....\n",
      "Validation loss: 2.3591100031838357....\n",
      "-----------------------------------\n",
      "Training loss: 2.3318258922454036....\n",
      "Validation loss: 2.3589397968710206....\n",
      "-----------------------------------\n",
      "Training loss: 2.3315517091286506....\n",
      "Validation loss: 2.3587699567681986....\n",
      "-----------------------------------\n",
      "Training loss: 2.3312777330363055....\n",
      "Validation loss: 2.3586011253808854....\n",
      "-----------------------------------\n",
      "Training loss: 2.331004379545055....\n",
      "Validation loss: 2.3584328201619975....\n",
      "-----------------------------------\n",
      "Training loss: 2.3307327796680894....\n",
      "Validation loss: 2.358265889598159....\n",
      "-----------------------------------\n",
      "Training loss: 2.330461947993688....\n",
      "Validation loss: 2.358099174403657....\n",
      "-----------------------------------\n",
      "Training loss: 2.330191477001086....\n",
      "Validation loss: 2.357932859570572....\n",
      "-----------------------------------\n",
      "Training loss: 2.3299213757083637....\n",
      "Validation loss: 2.3577674755561446....\n",
      "-----------------------------------\n",
      "Training loss: 2.3296512114107943....\n",
      "Validation loss: 2.357602105503614....\n",
      "-----------------------------------\n",
      "Training loss: 2.3293815779805236....\n",
      "Validation loss: 2.357437226819655....\n",
      "-----------------------------------\n",
      "Training loss: 2.329113425596208....\n",
      "Validation loss: 2.357272948237773....\n",
      "-----------------------------------\n",
      "Training loss: 2.328845190128584....\n",
      "Validation loss: 2.357108729481416....\n",
      "-----------------------------------\n",
      "Training loss: 2.328578278800358....\n",
      "Validation loss: 2.356945394705369....\n",
      "-----------------------------------\n",
      "Training loss: 2.3283124576834124....\n",
      "Validation loss: 2.3567836717975625....\n",
      "-----------------------------------\n",
      "Training loss: 2.32804842941512....\n",
      "Validation loss: 2.35662278194206....\n",
      "-----------------------------------\n",
      "Training loss: 2.3277855460647654....\n",
      "Validation loss: 2.356462511610914....\n",
      "-----------------------------------\n",
      "Training loss: 2.3275228559762575....\n",
      "Validation loss: 2.3563034698928536....\n",
      "-----------------------------------\n",
      "Training loss: 2.3272613400105366....\n",
      "Validation loss: 2.356144929104973....\n",
      "-----------------------------------\n",
      "Training loss: 2.3270009186007026....\n",
      "Validation loss: 2.3559864417557748....\n",
      "-----------------------------------\n",
      "Training loss: 2.3267417683706384....\n",
      "Validation loss: 2.355829069304191....\n",
      "-----------------------------------\n",
      "Training loss: 2.326484026642029....\n",
      "Validation loss: 2.3556721845525077....\n",
      "-----------------------------------\n",
      "Training loss: 2.326227084837197....\n",
      "Validation loss: 2.3555167724570247....\n",
      "-----------------------------------\n",
      "Training loss: 2.3259705900873193....\n",
      "Validation loss: 2.355361321020857....\n",
      "-----------------------------------\n",
      "Training loss: 2.3257151031224135....\n",
      "Validation loss: 2.3552055635226807....\n",
      "-----------------------------------\n",
      "Training loss: 2.3254602266123388....\n",
      "Validation loss: 2.3550508553232605....\n",
      "-----------------------------------\n",
      "Training loss: 2.3252055798284252....\n",
      "Validation loss: 2.354896897628613....\n",
      "-----------------------------------\n",
      "Training loss: 2.3249516527901433....\n",
      "Validation loss: 2.354743492771658....\n",
      "-----------------------------------\n",
      "Training loss: 2.3246981833343656....\n",
      "Validation loss: 2.354590177049097....\n",
      "-----------------------------------\n",
      "Training loss: 2.3244454560232715....\n",
      "Validation loss: 2.3544381327717256....\n",
      "-----------------------------------\n",
      "Training loss: 2.324193926552428....\n",
      "Validation loss: 2.3542871701904944....\n",
      "-----------------------------------\n",
      "Training loss: 2.323942695363901....\n",
      "Validation loss: 2.3541366946257227....\n",
      "-----------------------------------\n",
      "Training loss: 2.3236918631504975....\n",
      "Validation loss: 2.3539858990151408....\n",
      "-----------------------------------\n",
      "Training loss: 2.3234412450052173....\n",
      "Validation loss: 2.353836125294465....\n",
      "-----------------------------------\n",
      "Training loss: 2.323191857682926....\n",
      "Validation loss: 2.3536866321458474....\n",
      "-----------------------------------\n",
      "Training loss: 2.32294302232915....\n",
      "Validation loss: 2.3535370315114217....\n",
      "-----------------------------------\n",
      "Training loss: 2.322694470457817....\n",
      "Validation loss: 2.3533877765827587....\n",
      "-----------------------------------\n",
      "Training loss: 2.322446400154771....\n",
      "Validation loss: 2.353238758891627....\n",
      "-----------------------------------\n",
      "Training loss: 2.322197695123348....\n",
      "Validation loss: 2.353088953580815....\n",
      "-----------------------------------\n",
      "Training loss: 2.321948644914872....\n",
      "Validation loss: 2.3529395525540053....\n",
      "-----------------------------------\n",
      "Training loss: 2.3217004181881054....\n",
      "Validation loss: 2.3527906347536716....\n",
      "-----------------------------------\n",
      "Training loss: 2.3214522005036407....\n",
      "Validation loss: 2.352641470996462....\n",
      "-----------------------------------\n",
      "Training loss: 2.321203596224826....\n",
      "Validation loss: 2.3524929895948055....\n",
      "-----------------------------------\n",
      "Training loss: 2.320955877807499....\n",
      "Validation loss: 2.352344647990093....\n",
      "-----------------------------------\n",
      "Training loss: 2.320708091666425....\n",
      "Validation loss: 2.3521978257083394....\n",
      "-----------------------------------\n",
      "Training loss: 2.3204614608373872....\n",
      "Validation loss: 2.3520529692454204....\n",
      "-----------------------------------\n",
      "Training loss: 2.3202150104658648....\n",
      "Validation loss: 2.3519082632789328....\n",
      "-----------------------------------\n",
      "Training loss: 2.319969647724503....\n",
      "Validation loss: 2.3517651136726463....\n",
      "-----------------------------------\n",
      "Training loss: 2.3197257281757295....\n",
      "Validation loss: 2.3516225059774585....\n",
      "-----------------------------------\n",
      "Training loss: 2.319482531231182....\n",
      "Validation loss: 2.351480520848462....\n",
      "-----------------------------------\n",
      "Training loss: 2.3192397161735903....\n",
      "Validation loss: 2.3513386072038296....\n",
      "-----------------------------------\n",
      "Training loss: 2.318997364669268....\n",
      "Validation loss: 2.351197667803028....\n",
      "-----------------------------------\n",
      "Training loss: 2.318755403246506....\n",
      "Validation loss: 2.3510568537742125....\n",
      "-----------------------------------\n",
      "Training loss: 2.3185139465988276....\n",
      "Validation loss: 2.350916486248646....\n",
      "-----------------------------------\n",
      "Training loss: 2.3182727753449637....\n",
      "Validation loss: 2.350777464443208....\n",
      "-----------------------------------\n",
      "Training loss: 2.318032551016824....\n",
      "Validation loss: 2.3506381814796806....\n",
      "-----------------------------------\n",
      "Training loss: 2.3177927030882133....\n",
      "Validation loss: 2.3505003278162104....\n",
      "-----------------------------------\n",
      "Training loss: 2.3175538381233336....\n",
      "Validation loss: 2.3503627491996024....\n",
      "-----------------------------------\n",
      "Training loss: 2.317316042793771....\n",
      "Validation loss: 2.350226140633944....\n",
      "-----------------------------------\n",
      "Training loss: 2.3170790300148987....\n",
      "Validation loss: 2.350089560901129....\n",
      "-----------------------------------\n",
      "Training loss: 2.3168426627945835....\n",
      "Validation loss: 2.3499542539081943....\n",
      "-----------------------------------\n",
      "Training loss: 2.3166072765854198....\n",
      "Validation loss: 2.349818404971825....\n",
      "-----------------------------------\n",
      "Training loss: 2.3163719954935362....\n",
      "Validation loss: 2.3496830876391206....\n",
      "-----------------------------------\n",
      "Training loss: 2.3161372171083996....\n",
      "Validation loss: 2.3495481554849627....\n",
      "-----------------------------------\n",
      "Training loss: 2.315903000063535....\n",
      "Validation loss: 2.349414461426517....\n",
      "-----------------------------------\n",
      "Training loss: 2.3156694747925144....\n",
      "Validation loss: 2.3492803918946086....\n",
      "-----------------------------------\n",
      "Training loss: 2.3154365445644225....\n",
      "Validation loss: 2.3491468770240944....\n",
      "-----------------------------------\n",
      "Training loss: 2.315204803235436....\n",
      "Validation loss: 2.3490137539438782....\n",
      "-----------------------------------\n",
      "Training loss: 2.3149736415401208....\n",
      "Validation loss: 2.3488810241952978....\n",
      "-----------------------------------\n",
      "Training loss: 2.3147432952697087....\n",
      "Validation loss: 2.3487492856869596....\n",
      "-----------------------------------\n",
      "Training loss: 2.3145142689933658....\n",
      "Validation loss: 2.348618932030142....\n",
      "-----------------------------------\n",
      "Training loss: 2.314286935825661....\n",
      "Validation loss: 2.348488789985264....\n",
      "-----------------------------------\n",
      "Training loss: 2.3140595527454533....\n",
      "Validation loss: 2.3483594834804515....\n",
      "-----------------------------------\n",
      "Training loss: 2.313832547392616....\n",
      "Validation loss: 2.3482300717433833....\n",
      "-----------------------------------\n",
      "Training loss: 2.3136054402608064....\n",
      "Validation loss: 2.3480993185338845....\n",
      "-----------------------------------\n",
      "Training loss: 2.313378608693161....\n",
      "Validation loss: 2.347970050150303....\n",
      "-----------------------------------\n",
      "Training loss: 2.3131522011783257....\n",
      "Validation loss: 2.3478414037848405....\n",
      "-----------------------------------\n",
      "Training loss: 2.3129260060641696....\n",
      "Validation loss: 2.347712741165614....\n",
      "-----------------------------------\n",
      "Training loss: 2.3127003028489574....\n",
      "Validation loss: 2.347584704670956....\n",
      "-----------------------------------\n",
      "Training loss: 2.312475199211109....\n",
      "Validation loss: 2.3474578126444228....\n",
      "-----------------------------------\n",
      "Training loss: 2.312250618446906....\n",
      "Validation loss: 2.3473315426518244....\n",
      "-----------------------------------\n",
      "Training loss: 2.312026367219189....\n",
      "Validation loss: 2.3472053065509018....\n",
      "-----------------------------------\n",
      "Training loss: 2.3118019879318177....\n",
      "Validation loss: 2.3470791293433995....\n",
      "-----------------------------------\n",
      "Training loss: 2.3115779940144057....\n",
      "Validation loss: 2.3469536630123744....\n",
      "-----------------------------------\n",
      "Training loss: 2.311354114950499....\n",
      "Validation loss: 2.346829043027984....\n",
      "-----------------------------------\n",
      "Training loss: 2.3111308233852585....\n",
      "Validation loss: 2.3467047678246353....\n",
      "-----------------------------------\n",
      "Training loss: 2.3109083022190915....\n",
      "Validation loss: 2.346580544536093....\n",
      "-----------------------------------\n",
      "Training loss: 2.3106867525767587....\n",
      "Validation loss: 2.3464568544241784....\n",
      "-----------------------------------\n",
      "Training loss: 2.310465523730637....\n",
      "Validation loss: 2.3463335027629277....\n",
      "-----------------------------------\n",
      "Training loss: 2.3102444649372886....\n",
      "Validation loss: 2.3462100125565617....\n",
      "-----------------------------------\n",
      "Training loss: 2.3100241107716877....\n",
      "Validation loss: 2.346087623435462....\n",
      "-----------------------------------\n",
      "Training loss: 2.3098040531841777....\n",
      "Validation loss: 2.3459666785564934....\n",
      "-----------------------------------\n",
      "Training loss: 2.3095845128834553....\n",
      "Validation loss: 2.345845626072671....\n",
      "-----------------------------------\n",
      "Training loss: 2.3093656277212564....\n",
      "Validation loss: 2.3457258174778444....\n",
      "-----------------------------------\n",
      "Training loss: 2.3091473882975917....\n",
      "Validation loss: 2.3456057643363164....\n",
      "-----------------------------------\n",
      "Training loss: 2.308929261450565....\n",
      "Validation loss: 2.345486620285481....\n",
      "-----------------------------------\n",
      "Training loss: 2.30871145860926....\n",
      "Validation loss: 2.345368220848775....\n",
      "-----------------------------------\n",
      "Training loss: 2.3084942458788125....\n",
      "Validation loss: 2.3452502165452263....\n",
      "-----------------------------------\n",
      "Training loss: 2.3082777543593664....\n",
      "Validation loss: 2.345132012832319....\n",
      "-----------------------------------\n",
      "Training loss: 2.308061506161809....\n",
      "Validation loss: 2.34501457480504....\n",
      "-----------------------------------\n",
      "Training loss: 2.3078452724052574....\n",
      "Validation loss: 2.344897539256357....\n",
      "-----------------------------------\n",
      "Training loss: 2.307629493595769....\n",
      "Validation loss: 2.3447811142908446....\n",
      "-----------------------------------\n",
      "Training loss: 2.307413992983613....\n",
      "Validation loss: 2.3446652095973044....\n",
      "-----------------------------------\n",
      "Training loss: 2.3071984833369434....\n",
      "Validation loss: 2.344548932386647....\n",
      "-----------------------------------\n",
      "Training loss: 2.30698301123305....\n",
      "Validation loss: 2.344432966554224....\n",
      "-----------------------------------\n",
      "Training loss: 2.306767843889783....\n",
      "Validation loss: 2.3443168279147613....\n",
      "-----------------------------------\n",
      "Training loss: 2.306553192387587....\n",
      "Validation loss: 2.3442013384207505....\n",
      "-----------------------------------\n",
      "Training loss: 2.3063389620087817....\n",
      "Validation loss: 2.3440860363191915....\n",
      "-----------------------------------\n",
      "Training loss: 2.3061251722801757....\n",
      "Validation loss: 2.3439710581822766....\n",
      "-----------------------------------\n",
      "Training loss: 2.3059126519279207....\n",
      "Validation loss: 2.3438569718923175....\n",
      "-----------------------------------\n",
      "Training loss: 2.3057009475546995....\n",
      "Validation loss: 2.343743277906635....\n",
      "-----------------------------------\n",
      "Training loss: 2.305489753158793....\n",
      "Validation loss: 2.3436298415606642....\n",
      "-----------------------------------\n",
      "Training loss: 2.305279048075674....\n",
      "Validation loss: 2.343516499626547....\n",
      "-----------------------------------\n",
      "Training loss: 2.3050683549251607....\n",
      "Validation loss: 2.3434038319601895....\n",
      "-----------------------------------\n",
      "Training loss: 2.304858124752288....\n",
      "Validation loss: 2.343291081783358....\n",
      "-----------------------------------\n",
      "Training loss: 2.3046483638668587....\n",
      "Validation loss: 2.343179052382948....\n",
      "-----------------------------------\n",
      "Training loss: 2.304438667862938....\n",
      "Validation loss: 2.3430667824053475....\n",
      "-----------------------------------\n",
      "Training loss: 2.304228837037057....\n",
      "Validation loss: 2.342954754721478....\n",
      "-----------------------------------\n",
      "Training loss: 2.304019222931621....\n",
      "Validation loss: 2.3428432152234953....\n",
      "-----------------------------------\n",
      "Training loss: 2.3038095541225005....\n",
      "Validation loss: 2.342730605800448....\n",
      "-----------------------------------\n",
      "Training loss: 2.3035999408717753....\n",
      "Validation loss: 2.342618740574372....\n",
      "-----------------------------------\n",
      "Training loss: 2.3033906746011996....\n",
      "Validation loss: 2.3425070859926946....\n",
      "-----------------------------------\n",
      "Training loss: 2.303181266517936....\n",
      "Validation loss: 2.3423959951621933....\n",
      "-----------------------------------\n",
      "Training loss: 2.3029726419122873....\n",
      "Validation loss: 2.3422858647393934....\n",
      "-----------------------------------\n",
      "Training loss: 2.302765364501223....\n",
      "Validation loss: 2.3421765681867184....\n",
      "-----------------------------------\n",
      "Training loss: 2.3025590000619007....\n",
      "Validation loss: 2.3420677812496358....\n",
      "-----------------------------------\n",
      "Training loss: 2.3023529863693364....\n",
      "Validation loss: 2.341958824608382....\n",
      "-----------------------------------\n",
      "Training loss: 2.3021470892674034....\n",
      "Validation loss: 2.341850588844926....\n",
      "-----------------------------------\n",
      "Training loss: 2.3019418595731356....\n",
      "Validation loss: 2.3417422593847377....\n",
      "-----------------------------------\n",
      "Training loss: 2.3017366654638103....\n",
      "Validation loss: 2.3416335055580584....\n",
      "-----------------------------------\n",
      "Training loss: 2.30153160139979....\n",
      "Validation loss: 2.341525456164082....\n",
      "-----------------------------------\n",
      "Training loss: 2.3013261002105545....\n",
      "Validation loss: 2.3414184060628593....\n",
      "-----------------------------------\n",
      "Training loss: 2.301120900925227....\n",
      "Validation loss: 2.341311292885891....\n",
      "-----------------------------------\n",
      "Training loss: 2.300915968014136....\n",
      "Validation loss: 2.341204685218125....\n",
      "-----------------------------------\n",
      "Training loss: 2.300711569358922....\n",
      "Validation loss: 2.3410982748861433....\n",
      "-----------------------------------\n",
      "Training loss: 2.300507104436719....\n",
      "Validation loss: 2.3409923493386486....\n",
      "-----------------------------------\n",
      "Training loss: 2.3003032536440884....\n",
      "Validation loss: 2.340886762601353....\n",
      "-----------------------------------\n",
      "Training loss: 2.300099826818265....\n",
      "Validation loss: 2.340782465675034....\n",
      "-----------------------------------\n",
      "Training loss: 2.2998968922612586....\n",
      "Validation loss: 2.3406779744634907....\n",
      "-----------------------------------\n",
      "Training loss: 2.2996945960036212....\n",
      "Validation loss: 2.3405743799199126....\n",
      "-----------------------------------\n",
      "Training loss: 2.2994926118943586....\n",
      "Validation loss: 2.340471610171241....\n",
      "-----------------------------------\n",
      "Training loss: 2.2992909200763534....\n",
      "Validation loss: 2.3403688352098238....\n",
      "-----------------------------------\n",
      "Training loss: 2.2990896388839506....\n",
      "Validation loss: 2.3402664088494305....\n",
      "-----------------------------------\n",
      "Training loss: 2.2988887706387127....\n",
      "Validation loss: 2.3401640889326654....\n",
      "-----------------------------------\n",
      "Training loss: 2.2986880494517763....\n",
      "Validation loss: 2.340061866426772....\n",
      "-----------------------------------\n",
      "Training loss: 2.2984877526636316....\n",
      "Validation loss: 2.3399599904690183....\n",
      "-----------------------------------\n",
      "Training loss: 2.298287797719993....\n",
      "Validation loss: 2.3398591091539913....\n",
      "-----------------------------------\n",
      "Training loss: 2.298088397561043....\n",
      "Validation loss: 2.3397585786643513....\n",
      "-----------------------------------\n",
      "Training loss: 2.2978895781585456....\n",
      "Validation loss: 2.3396591467634615....\n",
      "-----------------------------------\n",
      "Training loss: 2.2976915517253915....\n",
      "Validation loss: 2.3395600858953793....\n",
      "-----------------------------------\n",
      "Training loss: 2.2974945162650147....\n",
      "Validation loss: 2.3394620071250403....\n",
      "-----------------------------------\n",
      "Training loss: 2.2972978892863996....\n",
      "Validation loss: 2.33936442891579....\n",
      "-----------------------------------\n",
      "Training loss: 2.2971015208945205....\n",
      "Validation loss: 2.33926713612805....\n",
      "-----------------------------------\n",
      "Training loss: 2.296905664037793....\n",
      "Validation loss: 2.3391702083782944....\n",
      "-----------------------------------\n",
      "Training loss: 2.2967102067411753....\n",
      "Validation loss: 2.3390742261194726....\n",
      "-----------------------------------\n",
      "Training loss: 2.296515077301262....\n",
      "Validation loss: 2.3389784645263756....\n",
      "-----------------------------------\n",
      "Training loss: 2.296320468858518....\n",
      "Validation loss: 2.3388822270832987....\n",
      "-----------------------------------\n",
      "Training loss: 2.2961260582276157....\n",
      "Validation loss: 2.33878682846009....\n",
      "-----------------------------------\n",
      "Training loss: 2.2959315298756304....\n",
      "Validation loss: 2.338691684416271....\n",
      "-----------------------------------\n",
      "Training loss: 2.2957373661452807....\n",
      "Validation loss: 2.3385967707540822....\n",
      "-----------------------------------\n",
      "Training loss: 2.2955435572668663....\n",
      "Validation loss: 2.338502698926953....\n",
      "-----------------------------------\n",
      "Training loss: 2.2953501079082996....\n",
      "Validation loss: 2.3384077900593057....\n",
      "-----------------------------------\n",
      "Training loss: 2.295156880318029....\n",
      "Validation loss: 2.3383132775191373....\n",
      "-----------------------------------\n",
      "Training loss: 2.29496392616155....\n",
      "Validation loss: 2.338219373890053....\n",
      "-----------------------------------\n",
      "Training loss: 2.2947713263180196....\n",
      "Validation loss: 2.3381258296313154....\n",
      "-----------------------------------\n",
      "Training loss: 2.2945792706007677....\n",
      "Validation loss: 2.3380323461533568....\n",
      "-----------------------------------\n",
      "Training loss: 2.29438748152582....\n",
      "Validation loss: 2.3379389410541607....\n",
      "-----------------------------------\n",
      "Training loss: 2.2941957491899823....\n",
      "Validation loss: 2.3378463975110315....\n",
      "-----------------------------------\n",
      "Training loss: 2.2940041069703....\n",
      "Validation loss: 2.337753587034052....\n",
      "-----------------------------------\n",
      "Training loss: 2.293812775029603....\n",
      "Validation loss: 2.3376619356581916....\n",
      "-----------------------------------\n",
      "Training loss: 2.2936223666563884....\n",
      "Validation loss: 2.3375699986773553....\n",
      "-----------------------------------\n",
      "Training loss: 2.29343246284502....\n",
      "Validation loss: 2.3374785940881....\n",
      "-----------------------------------\n",
      "Training loss: 2.2932428113824694....\n",
      "Validation loss: 2.3373875952780367....\n",
      "-----------------------------------\n",
      "Training loss: 2.293053738942771....\n",
      "Validation loss: 2.3372967367834843....\n",
      "-----------------------------------\n",
      "Training loss: 2.292864924353769....\n",
      "Validation loss: 2.337206031255404....\n",
      "-----------------------------------\n",
      "Training loss: 2.2926762625271224....\n",
      "Validation loss: 2.3371148615781276....\n",
      "-----------------------------------\n",
      "Training loss: 2.2924879585641964....\n",
      "Validation loss: 2.337024108352105....\n",
      "-----------------------------------\n",
      "Training loss: 2.292299615724548....\n",
      "Validation loss: 2.336933732465474....\n",
      "-----------------------------------\n",
      "Training loss: 2.2921117038603547....\n",
      "Validation loss: 2.336844260493673....\n",
      "-----------------------------------\n",
      "Training loss: 2.2919246145541323....\n",
      "Validation loss: 2.336754365773423....\n",
      "-----------------------------------\n",
      "Training loss: 2.2917374201188783....\n",
      "Validation loss: 2.33666520991237....\n",
      "-----------------------------------\n",
      "Training loss: 2.291550248700263....\n",
      "Validation loss: 2.3365765007764665....\n",
      "-----------------------------------\n",
      "Training loss: 2.291363388066896....\n",
      "Validation loss: 2.3364879862818677....\n",
      "-----------------------------------\n",
      "Training loss: 2.2911765606616394....\n",
      "Validation loss: 2.336398838406461....\n",
      "-----------------------------------\n",
      "Training loss: 2.2909897506396515....\n",
      "Validation loss: 2.3363104373376564....\n",
      "-----------------------------------\n",
      "Training loss: 2.2908029664409693....\n",
      "Validation loss: 2.3362224010854953....\n",
      "-----------------------------------\n",
      "Training loss: 2.290616698598583....\n",
      "Validation loss: 2.3361342004359216....\n",
      "-----------------------------------\n",
      "Training loss: 2.2904307031552116....\n",
      "Validation loss: 2.3360461515321362....\n",
      "-----------------------------------\n",
      "Training loss: 2.2902447013712934....\n",
      "Validation loss: 2.3359586360382245....\n",
      "-----------------------------------\n",
      "Training loss: 2.290058292603399....\n",
      "Validation loss: 2.3358712518073563....\n",
      "-----------------------------------\n",
      "Training loss: 2.2898723499753624....\n",
      "Validation loss: 2.335783594105782....\n",
      "-----------------------------------\n",
      "Training loss: 2.2896866027667597....\n",
      "Validation loss: 2.335696624635017....\n",
      "-----------------------------------\n",
      "Training loss: 2.2895006220519556....\n",
      "Validation loss: 2.3356101121158828....\n",
      "-----------------------------------\n",
      "Training loss: 2.289314888527528....\n",
      "Validation loss: 2.335523536710926....\n",
      "-----------------------------------\n",
      "Training loss: 2.2891291589056233....\n",
      "Validation loss: 2.3354376000988433....\n",
      "-----------------------------------\n",
      "Training loss: 2.288943164426311....\n",
      "Validation loss: 2.3353521220103683....\n",
      "-----------------------------------\n",
      "Training loss: 2.2887572879674147....\n",
      "Validation loss: 2.3352665170088023....\n",
      "-----------------------------------\n",
      "Training loss: 2.2885714808446265....\n",
      "Validation loss: 2.3351815080562903....\n",
      "-----------------------------------\n",
      "Training loss: 2.288386084299541....\n",
      "Validation loss: 2.3350959671578813....\n",
      "-----------------------------------\n",
      "Training loss: 2.2882011466726775....\n",
      "Validation loss: 2.3350106015329537....\n",
      "-----------------------------------\n",
      "Training loss: 2.288015892709195....\n",
      "Validation loss: 2.334924903596617....\n",
      "-----------------------------------\n",
      "Training loss: 2.2878311225437122....\n",
      "Validation loss: 2.3348395561187463....\n",
      "-----------------------------------\n",
      "Training loss: 2.2876463755271903....\n",
      "Validation loss: 2.334753884201624....\n",
      "-----------------------------------\n",
      "Training loss: 2.287461852873314....\n",
      "Validation loss: 2.3346686774408485....\n",
      "-----------------------------------\n",
      "Training loss: 2.287277407804695....\n",
      "Validation loss: 2.334582915413098....\n",
      "-----------------------------------\n",
      "Training loss: 2.2870936245178326....\n",
      "Validation loss: 2.334497715351876....\n",
      "-----------------------------------\n",
      "Training loss: 2.2869103680433462....\n",
      "Validation loss: 2.334413245691525....\n",
      "-----------------------------------\n",
      "Training loss: 2.286727602645992....\n",
      "Validation loss: 2.334328326040545....\n",
      "-----------------------------------\n",
      "Training loss: 2.2865450320773006....\n",
      "Validation loss: 2.3342440948206824....\n",
      "-----------------------------------\n",
      "Training loss: 2.2863629025483125....\n",
      "Validation loss: 2.3341604955298694....\n",
      "-----------------------------------\n",
      "Training loss: 2.286180976575289....\n",
      "Validation loss: 2.3340768983778366....\n",
      "-----------------------------------\n",
      "Training loss: 2.285999224900156....\n",
      "Validation loss: 2.333993077404536....\n",
      "-----------------------------------\n",
      "Training loss: 2.285817805855947....\n",
      "Validation loss: 2.333909867761855....\n",
      "-----------------------------------\n",
      "Training loss: 2.2856367596939506....\n",
      "Validation loss: 2.3338264700610725....\n",
      "-----------------------------------\n",
      "Training loss: 2.285455910098459....\n",
      "Validation loss: 2.3337428004740626....\n",
      "-----------------------------------\n",
      "Training loss: 2.2852750594143405....\n",
      "Validation loss: 2.333659120155152....\n",
      "-----------------------------------\n",
      "Training loss: 2.2850948687996255....\n",
      "Validation loss: 2.333575834768755....\n",
      "-----------------------------------\n",
      "Training loss: 2.2849149270396665....\n",
      "Validation loss: 2.333491648842717....\n",
      "-----------------------------------\n",
      "Training loss: 2.2847356096366735....\n",
      "Validation loss: 2.333407842349192....\n",
      "-----------------------------------\n",
      "Training loss: 2.284556557474993....\n",
      "Validation loss: 2.333323879485566....\n",
      "-----------------------------------\n",
      "Training loss: 2.284378026352617....\n",
      "Validation loss: 2.3332406441855302....\n",
      "-----------------------------------\n",
      "Training loss: 2.2841995299985918....\n",
      "Validation loss: 2.333157196958048....\n",
      "-----------------------------------\n",
      "Training loss: 2.2840208988504....\n",
      "Validation loss: 2.333073910947158....\n",
      "-----------------------------------\n",
      "Training loss: 2.283842587730161....\n",
      "Validation loss: 2.332990733069345....\n",
      "-----------------------------------\n",
      "Training loss: 2.283664686969385....\n",
      "Validation loss: 2.3329085962323597....\n",
      "-----------------------------------\n",
      "Training loss: 2.2834869998471894....\n",
      "Validation loss: 2.3328263088817365....\n",
      "-----------------------------------\n",
      "Training loss: 2.2833091498180096....\n",
      "Validation loss: 2.332744878836401....\n",
      "-----------------------------------\n",
      "Training loss: 2.283131501000913....\n",
      "Validation loss: 2.3326624828795346....\n",
      "-----------------------------------\n",
      "Training loss: 2.2829545448783515....\n",
      "Validation loss: 2.3325807733124337....\n",
      "-----------------------------------\n",
      "Training loss: 2.2827776697214173....\n",
      "Validation loss: 2.33249830696299....\n",
      "-----------------------------------\n",
      "Training loss: 2.2826007041410117....\n",
      "Validation loss: 2.3324162378405844....\n",
      "-----------------------------------\n",
      "Training loss: 2.282423741583398....\n",
      "Validation loss: 2.332333987334185....\n",
      "-----------------------------------\n",
      "Training loss: 2.28224654278318....\n",
      "Validation loss: 2.332252821371058....\n",
      "-----------------------------------\n",
      "Training loss: 2.2820696002132657....\n",
      "Validation loss: 2.3321717928718653....\n",
      "-----------------------------------\n",
      "Training loss: 2.281892942869375....\n",
      "Validation loss: 2.332090902465958....\n",
      "-----------------------------------\n",
      "Training loss: 2.2817163276185863....\n",
      "Validation loss: 2.332010158765094....\n",
      "-----------------------------------\n",
      "Training loss: 2.281539857157725....\n",
      "Validation loss: 2.331930306262466....\n",
      "-----------------------------------\n",
      "Training loss: 2.2813629959292108....\n",
      "Validation loss: 2.3318501087772443....\n",
      "-----------------------------------\n",
      "Training loss: 2.2811858573905672....\n",
      "Validation loss: 2.331770659115086....\n",
      "-----------------------------------\n",
      "Training loss: 2.2810087029475175....\n",
      "Validation loss: 2.331690450611779....\n",
      "-----------------------------------\n",
      "Training loss: 2.280831897091592....\n",
      "Validation loss: 2.3316115391623815....\n",
      "-----------------------------------\n",
      "Training loss: 2.2806552348196862....\n",
      "Validation loss: 2.331532318265898....\n",
      "-----------------------------------\n",
      "Training loss: 2.280478816508319....\n",
      "Validation loss: 2.3314535839320287....\n",
      "-----------------------------------\n",
      "Training loss: 2.2803022581778793....\n",
      "Validation loss: 2.3313755062447443....\n",
      "-----------------------------------\n",
      "Training loss: 2.280125273199699....\n",
      "Validation loss: 2.3312981899197958....\n",
      "-----------------------------------\n",
      "Training loss: 2.27994777904262....\n",
      "Validation loss: 2.3312208311451172....\n",
      "-----------------------------------\n",
      "Training loss: 2.2797704104247787....\n",
      "Validation loss: 2.331143559718476....\n",
      "-----------------------------------\n",
      "Training loss: 2.279593379846504....\n",
      "Validation loss: 2.3310670621894225....\n",
      "-----------------------------------\n",
      "Training loss: 2.2794166607087565....\n",
      "Validation loss: 2.3309908898094474....\n",
      "-----------------------------------\n",
      "Training loss: 2.2792400792953664....\n",
      "Validation loss: 2.330914874804624....\n",
      "-----------------------------------\n",
      "Training loss: 2.2790634799037996....\n",
      "Validation loss: 2.3308394095539597....\n",
      "-----------------------------------\n",
      "Training loss: 2.2788865921086883....\n",
      "Validation loss: 2.330762053515947....\n",
      "-----------------------------------\n",
      "Training loss: 2.278710164057291....\n",
      "Validation loss: 2.3306848809258067....\n",
      "-----------------------------------\n",
      "Training loss: 2.278534419196926....\n",
      "Validation loss: 2.3306061610863127....\n",
      "-----------------------------------\n",
      "Training loss: 2.2783593689607775....\n",
      "Validation loss: 2.3305285880955093....\n",
      "-----------------------------------\n",
      "Training loss: 2.2781846298265735....\n",
      "Validation loss: 2.3304502033502223....\n",
      "-----------------------------------\n",
      "Training loss: 2.278009701183487....\n",
      "Validation loss: 2.3303709881836316....\n",
      "-----------------------------------\n",
      "Training loss: 2.2778350043483657....\n",
      "Validation loss: 2.330291836587352....\n",
      "-----------------------------------\n",
      "Training loss: 2.2776604492418784....\n",
      "Validation loss: 2.330212713093216....\n",
      "-----------------------------------\n",
      "Training loss: 2.277485920933066....\n",
      "Validation loss: 2.330133574168115....\n",
      "-----------------------------------\n",
      "Training loss: 2.277311349904515....\n",
      "Validation loss: 2.3300542286018833....\n",
      "-----------------------------------\n",
      "Training loss: 2.2771368640947474....\n",
      "Validation loss: 2.3299756912016307....\n",
      "-----------------------------------\n",
      "Training loss: 2.2769623745284413....\n",
      "Validation loss: 2.329896624203352....\n",
      "-----------------------------------\n",
      "Training loss: 2.276787743966276....\n",
      "Validation loss: 2.3298171044111236....\n",
      "-----------------------------------\n",
      "Training loss: 2.27661342854327....\n",
      "Validation loss: 2.329737952947668....\n",
      "-----------------------------------\n",
      "Training loss: 2.276439320276812....\n",
      "Validation loss: 2.3296590618827624....\n",
      "-----------------------------------\n",
      "Training loss: 2.2762654696802107....\n",
      "Validation loss: 2.329580025795445....\n",
      "-----------------------------------\n",
      "Training loss: 2.276091941541023....\n",
      "Validation loss: 2.329501919920779....\n",
      "-----------------------------------\n",
      "Training loss: 2.2759184158550676....\n",
      "Validation loss: 2.3294231975501933....\n",
      "-----------------------------------\n",
      "Training loss: 2.275745107709915....\n",
      "Validation loss: 2.329344579532987....\n",
      "-----------------------------------\n",
      "Training loss: 2.275572134129795....\n",
      "Validation loss: 2.329266726104372....\n",
      "-----------------------------------\n",
      "Training loss: 2.275399432473879....\n",
      "Validation loss: 2.329188513932071....\n",
      "-----------------------------------\n",
      "Training loss: 2.275227117485485....\n",
      "Validation loss: 2.329110757973765....\n",
      "-----------------------------------\n",
      "Training loss: 2.275054981389041....\n",
      "Validation loss: 2.3290329278744837....\n",
      "-----------------------------------\n",
      "Training loss: 2.2748829516344315....\n",
      "Validation loss: 2.3289550547258115....\n",
      "-----------------------------------\n",
      "Training loss: 2.274710771224776....\n",
      "Validation loss: 2.3288768289427297....\n",
      "-----------------------------------\n",
      "Training loss: 2.274538621700336....\n",
      "Validation loss: 2.328799725644033....\n",
      "-----------------------------------\n",
      "Training loss: 2.2743669153136774....\n",
      "Validation loss: 2.328721805930193....\n",
      "-----------------------------------\n",
      "Training loss: 2.2741952064594733....\n",
      "Validation loss: 2.3286448412005396....\n",
      "-----------------------------------\n",
      "Training loss: 2.2740237025947514....\n",
      "Validation loss: 2.3285666939294214....\n",
      "-----------------------------------\n",
      "Training loss: 2.2738525235933023....\n",
      "Validation loss: 2.3284894553869258....\n",
      "-----------------------------------\n",
      "Training loss: 2.2736814907626925....\n",
      "Validation loss: 2.3284109920478615....\n",
      "-----------------------------------\n",
      "Training loss: 2.273510619589518....\n",
      "Validation loss: 2.328333746245684....\n",
      "-----------------------------------\n",
      "Training loss: 2.2733398387273636....\n",
      "Validation loss: 2.328256260780341....\n",
      "-----------------------------------\n",
      "Training loss: 2.2731692322215755....\n",
      "Validation loss: 2.3281780569971193....\n",
      "-----------------------------------\n",
      "Training loss: 2.2729987987040032....\n",
      "Validation loss: 2.3281007500954654....\n",
      "-----------------------------------\n",
      "Training loss: 2.2728286694365747....\n",
      "Validation loss: 2.3280226054589996....\n",
      "-----------------------------------\n",
      "Training loss: 2.272658754486001....\n",
      "Validation loss: 2.327945770071629....\n",
      "-----------------------------------\n",
      "Training loss: 2.2724891094814628....\n",
      "Validation loss: 2.327868586511199....\n",
      "-----------------------------------\n",
      "Training loss: 2.2723199676942123....\n",
      "Validation loss: 2.3277912696027605....\n",
      "-----------------------------------\n",
      "Training loss: 2.272150915659891....\n",
      "Validation loss: 2.327715195524797....\n",
      "-----------------------------------\n",
      "Training loss: 2.271982185183504....\n",
      "Validation loss: 2.327638035652243....\n",
      "-----------------------------------\n",
      "Training loss: 2.2718136829364086....\n",
      "Validation loss: 2.3275613557368917....\n",
      "-----------------------------------\n",
      "Training loss: 2.2716453056210466....\n",
      "Validation loss: 2.3274839924849253....\n",
      "-----------------------------------\n",
      "Training loss: 2.271477043331042....\n",
      "Validation loss: 2.3274070365896584....\n",
      "-----------------------------------\n",
      "Training loss: 2.271309085003063....\n",
      "Validation loss: 2.3273300734285587....\n",
      "-----------------------------------\n",
      "Training loss: 2.2711413657013417....\n",
      "Validation loss: 2.3272535757785424....\n",
      "-----------------------------------\n",
      "Training loss: 2.270973600262027....\n",
      "Validation loss: 2.327176932379978....\n",
      "-----------------------------------\n",
      "Training loss: 2.270806076302047....\n",
      "Validation loss: 2.327100413564547....\n",
      "-----------------------------------\n",
      "Training loss: 2.2706385524323505....\n",
      "Validation loss: 2.3270245885217777....\n",
      "-----------------------------------\n",
      "Training loss: 2.270471514207598....\n",
      "Validation loss: 2.326947551380113....\n",
      "-----------------------------------\n",
      "Training loss: 2.2703048614986177....\n",
      "Validation loss: 2.3268719823459723....\n",
      "-----------------------------------\n",
      "Training loss: 2.270138328366843....\n",
      "Validation loss: 2.3267960199595836....\n",
      "-----------------------------------\n",
      "Training loss: 2.269971941496643....\n",
      "Validation loss: 2.3267203289826477....\n",
      "-----------------------------------\n",
      "Training loss: 2.2698057527882667....\n",
      "Validation loss: 2.32664532875204....\n",
      "-----------------------------------\n",
      "Training loss: 2.2696396948311515....\n",
      "Validation loss: 2.3265693130278935....\n",
      "-----------------------------------\n",
      "Training loss: 2.269473681851934....\n",
      "Validation loss: 2.326494397418694....\n",
      "-----------------------------------\n",
      "Training loss: 2.2693078009223826....\n",
      "Validation loss: 2.3264193566640703....\n",
      "-----------------------------------\n",
      "Training loss: 2.269141997225554....\n",
      "Validation loss: 2.3263439379852375....\n",
      "-----------------------------------\n",
      "Training loss: 2.268976141777751....\n",
      "Validation loss: 2.3262696025631593....\n",
      "-----------------------------------\n",
      "Training loss: 2.268810701130633....\n",
      "Validation loss: 2.326195124101471....\n",
      "-----------------------------------\n",
      "Training loss: 2.2686456173608787....\n",
      "Validation loss: 2.3261200226480816....\n",
      "-----------------------------------\n",
      "Training loss: 2.2684806701234628....\n",
      "Validation loss: 2.3260454818099414....\n",
      "-----------------------------------\n",
      "Training loss: 2.268315995626433....\n",
      "Validation loss: 2.3259715013622277....\n",
      "-----------------------------------\n",
      "Training loss: 2.2681513237491724....\n",
      "Validation loss: 2.325897034496995....\n",
      "-----------------------------------\n",
      "Training loss: 2.2679868919855215....\n",
      "Validation loss: 2.3258228756862693....\n",
      "-----------------------------------\n",
      "Training loss: 2.267822349553981....\n",
      "Validation loss: 2.3257485266676388....\n",
      "-----------------------------------\n",
      "Training loss: 2.267657892013578....\n",
      "Validation loss: 2.325674121707405....\n",
      "-----------------------------------\n",
      "Training loss: 2.267493362668092....\n",
      "Validation loss: 2.325599254475243....\n",
      "-----------------------------------\n",
      "Training loss: 2.2673287817864156....\n",
      "Validation loss: 2.325524483156458....\n",
      "-----------------------------------\n",
      "Training loss: 2.2671641642620943....\n",
      "Validation loss: 2.3254487556794206....\n",
      "-----------------------------------\n",
      "Training loss: 2.266999485275644....\n",
      "Validation loss: 2.3253739771214015....\n",
      "-----------------------------------\n",
      "Training loss: 2.2668349133990926....\n",
      "Validation loss: 2.3252986016739596....\n",
      "-----------------------------------\n",
      "Training loss: 2.2666705753591265....\n",
      "Validation loss: 2.3252235561455055....\n",
      "-----------------------------------\n",
      "Training loss: 2.26650632508135....\n",
      "Validation loss: 2.325147934259192....\n",
      "-----------------------------------\n",
      "Training loss: 2.2663421014300678....\n",
      "Validation loss: 2.325072934487735....\n",
      "-----------------------------------\n",
      "Training loss: 2.266177937665382....\n",
      "Validation loss: 2.3249971254679846....\n",
      "-----------------------------------\n",
      "Training loss: 2.266013801265984....\n",
      "Validation loss: 2.3249227046197927....\n",
      "-----------------------------------\n",
      "Training loss: 2.2658497414987506....\n",
      "Validation loss: 2.3248478374572055....\n",
      "-----------------------------------\n",
      "Training loss: 2.265685565231287....\n",
      "Validation loss: 2.324774117354532....\n",
      "-----------------------------------\n",
      "Training loss: 2.2655213152468408....\n",
      "Validation loss: 2.324699921038805....\n",
      "-----------------------------------\n",
      "Training loss: 2.2653572608070665....\n",
      "Validation loss: 2.3246265137390285....\n",
      "-----------------------------------\n",
      "Training loss: 2.2651932058423636....\n",
      "Validation loss: 2.3245525300060974....\n",
      "-----------------------------------\n",
      "Training loss: 2.265029246687137....\n",
      "Validation loss: 2.3244796278094317....\n",
      "-----------------------------------\n",
      "Training loss: 2.264865403963013....\n",
      "Validation loss: 2.3244062784417663....\n",
      "-----------------------------------\n",
      "Training loss: 2.264701630791612....\n",
      "Validation loss: 2.324333422443749....\n",
      "-----------------------------------\n",
      "Training loss: 2.264537874726508....\n",
      "Validation loss: 2.3242607413474685....\n",
      "-----------------------------------\n",
      "Training loss: 2.264374210634631....\n",
      "Validation loss: 2.3241882781164978....\n",
      "-----------------------------------\n",
      "Training loss: 2.2642106732536162....\n",
      "Validation loss: 2.3241159110119685....\n",
      "-----------------------------------\n",
      "Training loss: 2.264047242163092....\n",
      "Validation loss: 2.324043028454626....\n",
      "-----------------------------------\n",
      "Training loss: 2.263883693440241....\n",
      "Validation loss: 2.323970455053911....\n",
      "-----------------------------------\n",
      "Training loss: 2.2637201582940443....\n",
      "Validation loss: 2.323898029673926....\n",
      "-----------------------------------\n",
      "Training loss: 2.263556723410238....\n",
      "Validation loss: 2.323825178855811....\n",
      "-----------------------------------\n",
      "Training loss: 2.2633933331217264....\n",
      "Validation loss: 2.32375298601962....\n",
      "-----------------------------------\n",
      "Training loss: 2.263230193843597....\n",
      "Validation loss: 2.3236795768059566....\n",
      "-----------------------------------\n",
      "Training loss: 2.2630671010738674....\n",
      "Validation loss: 2.323607099603597....\n",
      "-----------------------------------\n",
      "Training loss: 2.2629041639391487....\n",
      "Validation loss: 2.32353473776389....\n",
      "-----------------------------------\n",
      "Training loss: 2.2627410695575616....\n",
      "Validation loss: 2.323462360770639....\n",
      "-----------------------------------\n",
      "Training loss: 2.2625781538583....\n",
      "Validation loss: 2.3233898687398717....\n",
      "-----------------------------------\n",
      "Training loss: 2.2624153906741995....\n",
      "Validation loss: 2.323318071521553....\n",
      "-----------------------------------\n",
      "Training loss: 2.2622525359948242....\n",
      "Validation loss: 2.32324596503639....\n",
      "-----------------------------------\n",
      "Training loss: 2.2620892924063467....\n",
      "Validation loss: 2.3231736796614166....\n",
      "-----------------------------------\n",
      "Training loss: 2.2619262036897845....\n",
      "Validation loss: 2.323101161407992....\n",
      "-----------------------------------\n",
      "Training loss: 2.261763227254902....\n",
      "Validation loss: 2.32302967941909....\n",
      "-----------------------------------\n",
      "Training loss: 2.2616002842778653....\n",
      "Validation loss: 2.322957525377714....\n",
      "-----------------------------------\n",
      "Training loss: 2.2614371683478796....\n",
      "Validation loss: 2.3228858569810127....\n",
      "-----------------------------------\n",
      "Training loss: 2.2612742995567077....\n",
      "Validation loss: 2.3228141371370534....\n",
      "-----------------------------------\n",
      "Training loss: 2.261111607419374....\n",
      "Validation loss: 2.322743189210416....\n",
      "-----------------------------------\n",
      "Training loss: 2.260949112967157....\n",
      "Validation loss: 2.3226721207364953....\n",
      "-----------------------------------\n",
      "Training loss: 2.2607867442951832....\n",
      "Validation loss: 2.3226009502475207....\n",
      "-----------------------------------\n",
      "Training loss: 2.2606245157610285....\n",
      "Validation loss: 2.322530330541803....\n",
      "-----------------------------------\n",
      "Training loss: 2.2604625129387053....\n",
      "Validation loss: 2.3224591612720213....\n",
      "-----------------------------------\n",
      "Training loss: 2.26030052133312....\n",
      "Validation loss: 2.322388695803469....\n",
      "-----------------------------------\n",
      "Training loss: 2.260138503364851....\n",
      "Validation loss: 2.322318027794042....\n",
      "-----------------------------------\n",
      "Training loss: 2.25997650158786....\n",
      "Validation loss: 2.3222480839932027....\n",
      "-----------------------------------\n",
      "Training loss: 2.259814503398087....\n",
      "Validation loss: 2.322177308372929....\n",
      "-----------------------------------\n",
      "Training loss: 2.2596524584070004....\n",
      "Validation loss: 2.3221066046592975....\n",
      "-----------------------------------\n",
      "Training loss: 2.2594903460874645....\n",
      "Validation loss: 2.3220363326816216....\n",
      "-----------------------------------\n",
      "Training loss: 2.259328323259805....\n",
      "Validation loss: 2.321965535792611....\n",
      "-----------------------------------\n",
      "Training loss: 2.259166253297245....\n",
      "Validation loss: 2.3218949395868864....\n",
      "-----------------------------------\n",
      "Training loss: 2.2590042597955704....\n",
      "Validation loss: 2.321824987133625....\n",
      "-----------------------------------\n",
      "Training loss: 2.2588424712393316....\n",
      "Validation loss: 2.3217553229594365....\n",
      "-----------------------------------\n",
      "Training loss: 2.2586807269912086....\n",
      "Validation loss: 2.3216848768205653....\n",
      "-----------------------------------\n",
      "Training loss: 2.2585186766611276....\n",
      "Validation loss: 2.321615150715832....\n",
      "-----------------------------------\n",
      "Training loss: 2.25835660337338....\n",
      "Validation loss: 2.3215443760451144....\n",
      "-----------------------------------\n",
      "Training loss: 2.258194666785993....\n",
      "Validation loss: 2.3214740428218983....\n",
      "-----------------------------------\n",
      "Training loss: 2.2580327887886975....\n",
      "Validation loss: 2.321403687981373....\n",
      "-----------------------------------\n",
      "Training loss: 2.257870836776201....\n",
      "Validation loss: 2.321333656260711....\n",
      "-----------------------------------\n",
      "Training loss: 2.2577089740660345....\n",
      "Validation loss: 2.3212637624850365....\n",
      "-----------------------------------\n",
      "Training loss: 2.2575472371179512....\n",
      "Validation loss: 2.3211938870584485....\n",
      "-----------------------------------\n",
      "Training loss: 2.257385443326021....\n",
      "Validation loss: 2.3211242560540075....\n",
      "-----------------------------------\n",
      "Training loss: 2.257223756660846....\n",
      "Validation loss: 2.3210535005305237....\n",
      "-----------------------------------\n",
      "Training loss: 2.2570622296673695....\n",
      "Validation loss: 2.3209835356876347....\n",
      "-----------------------------------\n",
      "Training loss: 2.25690101333935....\n",
      "Validation loss: 2.3209144500324967....\n",
      "-----------------------------------\n",
      "Training loss: 2.2567396947205793....\n",
      "Validation loss: 2.3208438711228903....\n",
      "-----------------------------------\n",
      "Training loss: 2.256578495128556....\n",
      "Validation loss: 2.320774388269252....\n",
      "-----------------------------------\n",
      "Training loss: 2.2564174146814793....\n",
      "Validation loss: 2.320703708214598....\n",
      "-----------------------------------\n",
      "Training loss: 2.2562563834543585....\n",
      "Validation loss: 2.3206347859079295....\n",
      "-----------------------------------\n",
      "Training loss: 2.256095534289553....\n",
      "Validation loss: 2.320565388740154....\n",
      "-----------------------------------\n",
      "Training loss: 2.255934840514127....\n",
      "Validation loss: 2.320497148716309....\n",
      "-----------------------------------\n",
      "Training loss: 2.255774156444136....\n",
      "Validation loss: 2.320427870917224....\n",
      "-----------------------------------\n",
      "Training loss: 2.2556135681756846....\n",
      "Validation loss: 2.3203588544558387....\n",
      "-----------------------------------\n",
      "Training loss: 2.2554530159155384....\n",
      "Validation loss: 2.320290275501453....\n",
      "-----------------------------------\n",
      "Training loss: 2.255292337273718....\n",
      "Validation loss: 2.3202222596517013....\n",
      "-----------------------------------\n",
      "Training loss: 2.255131587287976....\n",
      "Validation loss: 2.3201538354002498....\n",
      "-----------------------------------\n",
      "Training loss: 2.2549707529094185....\n",
      "Validation loss: 2.320085641847638....\n",
      "-----------------------------------\n",
      "Training loss: 2.2548100133928877....\n",
      "Validation loss: 2.320018074529461....\n",
      "-----------------------------------\n",
      "Training loss: 2.2546493366958233....\n",
      "Validation loss: 2.3199496904719217....\n",
      "-----------------------------------\n",
      "Training loss: 2.2544887043774047....\n",
      "Validation loss: 2.3198812090794654....\n",
      "-----------------------------------\n",
      "Training loss: 2.254328131982002....\n",
      "Validation loss: 2.3198129987465648....\n",
      "-----------------------------------\n",
      "Training loss: 2.2541676409260063....\n",
      "Validation loss: 2.3197460612714775....\n",
      "-----------------------------------\n",
      "Training loss: 2.25400721084057....\n",
      "Validation loss: 2.319677987360903....\n",
      "-----------------------------------\n",
      "Training loss: 2.2538467493918475....\n",
      "Validation loss: 2.3196107948234377....\n",
      "-----------------------------------\n",
      "Training loss: 2.253686410127168....\n",
      "Validation loss: 2.3195429024833567....\n",
      "-----------------------------------\n",
      "Training loss: 2.253526164417962....\n",
      "Validation loss: 2.3194761358310196....\n",
      "-----------------------------------\n",
      "Training loss: 2.2533659348514665....\n",
      "Validation loss: 2.31940846218738....\n",
      "-----------------------------------\n",
      "Training loss: 2.2532056363229565....\n",
      "Validation loss: 2.3193413086204013....\n",
      "-----------------------------------\n",
      "Training loss: 2.2530451534379337....\n",
      "Validation loss: 2.319273552367029....\n",
      "-----------------------------------\n",
      "Training loss: 2.2528847068009505....\n",
      "Validation loss: 2.3192058350437708....\n",
      "-----------------------------------\n",
      "Training loss: 2.252724395712062....\n",
      "Validation loss: 2.3191387942115353....\n",
      "-----------------------------------\n",
      "Training loss: 2.252564077056515....\n",
      "Validation loss: 2.319070564590912....\n",
      "-----------------------------------\n",
      "Training loss: 2.252403736673096....\n",
      "Validation loss: 2.3190037088398934....\n",
      "-----------------------------------\n",
      "Training loss: 2.2522431828070535....\n",
      "Validation loss: 2.31893569239271....\n",
      "-----------------------------------\n",
      "Training loss: 2.2520825637972997....\n",
      "Validation loss: 2.318869012359784....\n",
      "-----------------------------------\n",
      "Training loss: 2.2519215751192934....\n",
      "Validation loss: 2.318800790465608....\n",
      "-----------------------------------\n",
      "Training loss: 2.251760726144772....\n",
      "Validation loss: 2.318733593522973....\n",
      "-----------------------------------\n",
      "Training loss: 2.251600036998438....\n",
      "Validation loss: 2.31866582885108....\n",
      "-----------------------------------\n",
      "Training loss: 2.2514396722168666....\n",
      "Validation loss: 2.318599167548758....\n",
      "-----------------------------------\n",
      "Training loss: 2.2512794826602383....\n",
      "Validation loss: 2.3185310098812644....\n",
      "-----------------------------------\n",
      "Training loss: 2.251119622176669....\n",
      "Validation loss: 2.31846489967142....\n",
      "-----------------------------------\n",
      "Training loss: 2.250959916951303....\n",
      "Validation loss: 2.318397494989811....\n",
      "-----------------------------------\n",
      "Training loss: 2.2508002127459266....\n",
      "Validation loss: 2.318331262053121....\n",
      "-----------------------------------\n",
      "Training loss: 2.2506406442982634....\n",
      "Validation loss: 2.3182642107913973....\n",
      "-----------------------------------\n",
      "Training loss: 2.2504810745901147....\n",
      "Validation loss: 2.318198047610278....\n",
      "-----------------------------------\n",
      "Training loss: 2.2503215828568908....\n",
      "Validation loss: 2.318130868444936....\n",
      "-----------------------------------\n",
      "Training loss: 2.250162143116945....\n",
      "Validation loss: 2.3180648921919875....\n",
      "-----------------------------------\n",
      "Training loss: 2.2500029102249797....\n",
      "Validation loss: 2.3179974995404344....\n",
      "-----------------------------------\n",
      "Training loss: 2.2498440678791205....\n",
      "Validation loss: 2.3179312103507135....\n",
      "-----------------------------------\n",
      "Training loss: 2.2496852355796535....\n",
      "Validation loss: 2.317864310812938....\n",
      "-----------------------------------\n",
      "Training loss: 2.2495263048090726....\n",
      "Validation loss: 2.3177973532953695....\n",
      "-----------------------------------\n",
      "Training loss: 2.2493673412045543....\n",
      "Validation loss: 2.3177310665576614....\n",
      "-----------------------------------\n",
      "Training loss: 2.249208383525687....\n",
      "Validation loss: 2.317664197987937....\n",
      "-----------------------------------\n",
      "Training loss: 2.249049345299429....\n",
      "Validation loss: 2.317597041354478....\n",
      "-----------------------------------\n",
      "Training loss: 2.248890280592016....\n",
      "Validation loss: 2.3175311425841465....\n",
      "-----------------------------------\n",
      "Training loss: 2.2487312279892993....\n",
      "Validation loss: 2.3174636381277263....\n",
      "-----------------------------------\n",
      "Training loss: 2.2485721088088897....\n",
      "Validation loss: 2.317397087686183....\n",
      "-----------------------------------\n",
      "Training loss: 2.2484130582958417....\n",
      "Validation loss: 2.3173298794640167....\n",
      "-----------------------------------\n",
      "Training loss: 2.248253938565902....\n",
      "Validation loss: 2.3172629080283005....\n",
      "-----------------------------------\n",
      "Training loss: 2.2480948746838587....\n",
      "Validation loss: 2.317195890251116....\n",
      "-----------------------------------\n",
      "Training loss: 2.247935916836401....\n",
      "Validation loss: 2.317128322897111....\n",
      "-----------------------------------\n",
      "Training loss: 2.2477769500224034....\n",
      "Validation loss: 2.3170615474721354....\n",
      "-----------------------------------\n",
      "Training loss: 2.2476180024041548....\n",
      "Validation loss: 2.3169941517407007....\n",
      "-----------------------------------\n",
      "Training loss: 2.2474589677435444....\n",
      "Validation loss: 2.316926859279235....\n",
      "-----------------------------------\n",
      "Training loss: 2.2472999380453977....\n",
      "Validation loss: 2.316859609209545....\n",
      "-----------------------------------\n",
      "Training loss: 2.2471410226221784....\n",
      "Validation loss: 2.316791908109293....\n",
      "-----------------------------------\n",
      "Training loss: 2.246982050813802....\n",
      "Validation loss: 2.3167249919609185....\n",
      "-----------------------------------\n",
      "Training loss: 2.2468231226934274....\n",
      "Validation loss: 2.316656892687447....\n",
      "-----------------------------------\n",
      "Training loss: 2.246664327306225....\n",
      "Validation loss: 2.3165893791505767....\n",
      "-----------------------------------\n",
      "Training loss: 2.246505507505503....\n",
      "Validation loss: 2.3165219674371738....\n",
      "-----------------------------------\n",
      "Training loss: 2.2463466348043437....\n",
      "Validation loss: 2.3164538727794324....\n",
      "-----------------------------------\n",
      "Training loss: 2.2461877818427682....\n",
      "Validation loss: 2.3163872771865055....\n",
      "-----------------------------------\n",
      "Training loss: 2.2460290039299675....\n",
      "Validation loss: 2.3163197687943176....\n",
      "-----------------------------------\n",
      "Training loss: 2.2458702633225625....\n",
      "Validation loss: 2.3162521960050015....\n",
      "-----------------------------------\n",
      "Training loss: 2.2457115982587927....\n",
      "Validation loss: 2.3161850467293315....\n",
      "-----------------------------------\n",
      "Training loss: 2.2455527852094566....\n",
      "Validation loss: 2.3161173408120805....\n",
      "-----------------------------------\n",
      "Training loss: 2.2453940024682084....\n",
      "Validation loss: 2.3160503706615283....\n",
      "-----------------------------------\n",
      "Training loss: 2.245235276955121....\n",
      "Validation loss: 2.315983195672544....\n",
      "-----------------------------------\n",
      "Training loss: 2.245076439990775....\n",
      "Validation loss: 2.3159156154435614....\n",
      "-----------------------------------\n",
      "Training loss: 2.2449175874447773....\n",
      "Validation loss: 2.31584852299062....\n",
      "-----------------------------------\n",
      "Training loss: 2.2447588094187974....\n",
      "Validation loss: 2.315780823194606....\n",
      "-----------------------------------\n",
      "Training loss: 2.2446000025664077....\n",
      "Validation loss: 2.3157136148533883....\n",
      "-----------------------------------\n",
      "Training loss: 2.2444412021466187....\n",
      "Validation loss: 2.3156452263178062....\n",
      "-----------------------------------\n",
      "Training loss: 2.244282490175267....\n",
      "Validation loss: 2.3155774241080023....\n",
      "-----------------------------------\n",
      "Training loss: 2.244123744789407....\n",
      "Validation loss: 2.3155100908301516....\n",
      "-----------------------------------\n",
      "Training loss: 2.2439649367476378....\n",
      "Validation loss: 2.3154422650937856....\n",
      "-----------------------------------\n",
      "Training loss: 2.243806202809629....\n",
      "Validation loss: 2.315374343281077....\n",
      "-----------------------------------\n",
      "Training loss: 2.2436477667967836....\n",
      "Validation loss: 2.3153069034998524....\n",
      "-----------------------------------\n",
      "Training loss: 2.243489473679113....\n",
      "Validation loss: 2.3152389569133365....\n",
      "-----------------------------------\n",
      "Training loss: 2.243331158318537....\n",
      "Validation loss: 2.3151716406289595....\n",
      "-----------------------------------\n",
      "Training loss: 2.243172815954573....\n",
      "Validation loss: 2.3151040697968495....\n",
      "-----------------------------------\n",
      "Training loss: 2.243014486505875....\n",
      "Validation loss: 2.3150369153989145....\n",
      "-----------------------------------\n",
      "Training loss: 2.2428562439717363....\n",
      "Validation loss: 2.3149703212823907....\n",
      "-----------------------------------\n",
      "Training loss: 2.242698238840782....\n",
      "Validation loss: 2.314902665627578....\n",
      "-----------------------------------\n",
      "Training loss: 2.24254040108992....\n",
      "Validation loss: 2.3148370017899187....\n",
      "-----------------------------------\n",
      "Training loss: 2.2423825914075626....\n",
      "Validation loss: 2.3147701406770995....\n",
      "-----------------------------------\n",
      "Training loss: 2.2422246751161565....\n",
      "Validation loss: 2.3147030319567943....\n",
      "-----------------------------------\n",
      "Training loss: 2.242066997577657....\n",
      "Validation loss: 2.3146369917971987....\n",
      "-----------------------------------\n",
      "Training loss: 2.2419093351944626....\n",
      "Validation loss: 2.314569920953132....\n",
      "-----------------------------------\n",
      "Training loss: 2.2417516611272434....\n",
      "Validation loss: 2.314503268784282....\n",
      "-----------------------------------\n",
      "Training loss: 2.2415940689184506....\n",
      "Validation loss: 2.314436949938218....\n",
      "-----------------------------------\n",
      "Training loss: 2.241436696846657....\n",
      "Validation loss: 2.3143707163031575....\n",
      "-----------------------------------\n",
      "Training loss: 2.241279436674108....\n",
      "Validation loss: 2.3143050855307448....\n",
      "-----------------------------------\n",
      "Training loss: 2.2411221873227944....\n",
      "Validation loss: 2.314239326073829....\n",
      "-----------------------------------\n",
      "Training loss: 2.240964801631993....\n",
      "Validation loss: 2.3141732000242263....\n",
      "-----------------------------------\n",
      "Training loss: 2.2408073146391247....\n",
      "Validation loss: 2.3141070738666834....\n",
      "-----------------------------------\n",
      "Training loss: 2.240649726170904....\n",
      "Validation loss: 2.314040516940431....\n",
      "-----------------------------------\n",
      "Training loss: 2.2404922468430577....\n",
      "Validation loss: 2.3139741303300005....\n",
      "-----------------------------------\n",
      "Training loss: 2.240334767272511....\n",
      "Validation loss: 2.3139077882670684....\n",
      "-----------------------------------\n",
      "Training loss: 2.2401772511900666....\n",
      "Validation loss: 2.313841555211534....\n",
      "-----------------------------------\n",
      "Training loss: 2.2400198186640115....\n",
      "Validation loss: 2.3137756183628007....\n",
      "-----------------------------------\n",
      "Training loss: 2.239862491240576....\n",
      "Validation loss: 2.3137093865082163....\n",
      "-----------------------------------\n",
      "Training loss: 2.2397053096698873....\n",
      "Validation loss: 2.3136429509161793....\n",
      "-----------------------------------\n",
      "Training loss: 2.239548096735364....\n",
      "Validation loss: 2.3135773221088605....\n",
      "-----------------------------------\n",
      "Training loss: 2.2393909058109065....\n",
      "Validation loss: 2.3135113838797867....\n",
      "-----------------------------------\n",
      "Training loss: 2.239233606421437....\n",
      "Validation loss: 2.3134452976439435....\n",
      "-----------------------------------\n",
      "Training loss: 2.2390762521060625....\n",
      "Validation loss: 2.3133798044344593....\n",
      "-----------------------------------\n",
      "Training loss: 2.2389188698248392....\n",
      "Validation loss: 2.313314227174931....\n",
      "-----------------------------------\n",
      "Training loss: 2.238761474827853....\n",
      "Validation loss: 2.3132486784961874....\n",
      "-----------------------------------\n",
      "Training loss: 2.2386042063256806....\n",
      "Validation loss: 2.3131822519268534....\n",
      "-----------------------------------\n",
      "Training loss: 2.238447054765077....\n",
      "Validation loss: 2.3131163984538206....\n",
      "-----------------------------------\n",
      "Training loss: 2.238289809572143....\n",
      "Validation loss: 2.313050447427436....\n",
      "-----------------------------------\n",
      "Training loss: 2.2381326055102835....\n",
      "Validation loss: 2.3129845065976036....\n",
      "-----------------------------------\n",
      "Training loss: 2.2379754324796033....\n",
      "Validation loss: 2.3129185067623177....\n",
      "-----------------------------------\n",
      "Training loss: 2.2378182193640828....\n",
      "Validation loss: 2.3128530320495204....\n",
      "-----------------------------------\n",
      "Training loss: 2.2376608285099278....\n",
      "Validation loss: 2.3127868747442446....\n",
      "-----------------------------------\n",
      "Training loss: 2.23750356782717....\n",
      "Validation loss: 2.3127210699054....\n",
      "-----------------------------------\n",
      "Training loss: 2.2373464107521754....\n",
      "Validation loss: 2.312655974124448....\n",
      "-----------------------------------\n",
      "Training loss: 2.237189056679161....\n",
      "Validation loss: 2.3125904166295523....\n",
      "-----------------------------------\n",
      "Training loss: 2.237031758382608....\n",
      "Validation loss: 2.312525833471241....\n",
      "-----------------------------------\n",
      "Training loss: 2.236874468734414....\n",
      "Validation loss: 2.3124607327712665....\n",
      "-----------------------------------\n",
      "Training loss: 2.23671717376727....\n",
      "Validation loss: 2.3123955123362676....\n",
      "-----------------------------------\n",
      "Training loss: 2.2365600053845536....\n",
      "Validation loss: 2.312331051641967....\n",
      "-----------------------------------\n",
      "Training loss: 2.2364028641041056....\n",
      "Validation loss: 2.312266747378826....\n",
      "-----------------------------------\n",
      "Training loss: 2.2362456687881296....\n",
      "Validation loss: 2.312202341588114....\n",
      "-----------------------------------\n",
      "Training loss: 2.2732059910800984....\n",
      "Validation loss: 2.23794953743384....\n",
      "-----------------------------------\n",
      "Training loss: 2.272997817780815....\n",
      "Validation loss: 2.237913848519065....\n",
      "-----------------------------------\n",
      "Training loss: 2.272792137248023....\n",
      "Validation loss: 2.237878602707885....\n",
      "-----------------------------------\n",
      "Training loss: 2.272587819917438....\n",
      "Validation loss: 2.237843171564684....\n",
      "-----------------------------------\n",
      "Training loss: 2.2723855578161563....\n",
      "Validation loss: 2.2378086910315993....\n",
      "-----------------------------------\n",
      "Training loss: 2.272183662396213....\n",
      "Validation loss: 2.2377747147246634....\n",
      "-----------------------------------\n",
      "Training loss: 2.2719825592044365....\n",
      "Validation loss: 2.237741649049712....\n",
      "-----------------------------------\n",
      "Training loss: 2.271782103139428....\n",
      "Validation loss: 2.237709042091686....\n",
      "-----------------------------------\n",
      "Training loss: 2.271582191323466....\n",
      "Validation loss: 2.237676849791885....\n",
      "-----------------------------------\n",
      "Training loss: 2.2713832824176445....\n",
      "Validation loss: 2.2376445876350712....\n",
      "-----------------------------------\n",
      "Training loss: 2.2711852000575643....\n",
      "Validation loss: 2.2376119139915174....\n",
      "-----------------------------------\n",
      "Training loss: 2.2709886033926296....\n",
      "Validation loss: 2.2375798024299764....\n",
      "-----------------------------------\n",
      "Training loss: 2.270792574212837....\n",
      "Validation loss: 2.237548066629127....\n",
      "-----------------------------------\n",
      "Training loss: 2.270596470835836....\n",
      "Validation loss: 2.2375166996394156....\n",
      "-----------------------------------\n",
      "Training loss: 2.270401265162898....\n",
      "Validation loss: 2.237484860138202....\n",
      "-----------------------------------\n",
      "Training loss: 2.270207114995472....\n",
      "Validation loss: 2.237452146843883....\n",
      "-----------------------------------\n",
      "Training loss: 2.2700140955829915....\n",
      "Validation loss: 2.2374198786803525....\n",
      "-----------------------------------\n",
      "Training loss: 2.2698212662951787....\n",
      "Validation loss: 2.2373879717239333....\n",
      "-----------------------------------\n",
      "Training loss: 2.2696292317678513....\n",
      "Validation loss: 2.2373564109585375....\n",
      "-----------------------------------\n",
      "Training loss: 2.269438146664335....\n",
      "Validation loss: 2.2373246665115074....\n",
      "-----------------------------------\n",
      "Training loss: 2.2692474275389536....\n",
      "Validation loss: 2.2372922666191304....\n",
      "-----------------------------------\n",
      "Training loss: 2.2690573970779577....\n",
      "Validation loss: 2.237259649396841....\n",
      "-----------------------------------\n",
      "Training loss: 2.2688680106160453....\n",
      "Validation loss: 2.2372269187304608....\n",
      "-----------------------------------\n",
      "Training loss: 2.268679286186937....\n",
      "Validation loss: 2.237193320855037....\n",
      "-----------------------------------\n",
      "Training loss: 2.2684915267195778....\n",
      "Validation loss: 2.237159710207766....\n",
      "-----------------------------------\n",
      "Training loss: 2.2683043586322973....\n",
      "Validation loss: 2.23712654043214....\n",
      "-----------------------------------\n",
      "Training loss: 2.268117752742133....\n",
      "Validation loss: 2.23709289250274....\n",
      "-----------------------------------\n",
      "Training loss: 2.2679320524357207....\n",
      "Validation loss: 2.2370595361551056....\n",
      "-----------------------------------\n",
      "Training loss: 2.267746027795439....\n",
      "Validation loss: 2.23702613603764....\n",
      "-----------------------------------\n",
      "Training loss: 2.2675600207136286....\n",
      "Validation loss: 2.236992789802983....\n",
      "-----------------------------------\n",
      "Training loss: 2.2673738170872357....\n",
      "Validation loss: 2.236960264782885....\n",
      "-----------------------------------\n",
      "Training loss: 2.2671875215285704....\n",
      "Validation loss: 2.2369276429093925....\n",
      "-----------------------------------\n",
      "Training loss: 2.2670018070834193....\n",
      "Validation loss: 2.2368947900636957....\n",
      "-----------------------------------\n",
      "Training loss: 2.2668171235900494....\n",
      "Validation loss: 2.2368610973842773....\n",
      "-----------------------------------\n",
      "Training loss: 2.266633075051233....\n",
      "Validation loss: 2.236827184067781....\n",
      "-----------------------------------\n",
      "Training loss: 2.266448673949316....\n",
      "Validation loss: 2.2367936151880166....\n",
      "-----------------------------------\n",
      "Training loss: 2.2662643898224437....\n",
      "Validation loss: 2.2367593531639214....\n",
      "-----------------------------------\n",
      "Training loss: 2.2660809023316553....\n",
      "Validation loss: 2.2367244842914866....\n",
      "-----------------------------------\n",
      "Training loss: 2.2658976597436817....\n",
      "Validation loss: 2.23668930671407....\n",
      "-----------------------------------\n",
      "Training loss: 2.2657146069639165....\n",
      "Validation loss: 2.236654219992664....\n",
      "-----------------------------------\n",
      "Training loss: 2.2655319723321288....\n",
      "Validation loss: 2.236618626829488....\n",
      "-----------------------------------\n",
      "Training loss: 2.26535004334862....\n",
      "Validation loss: 2.236583781736657....\n",
      "-----------------------------------\n",
      "Training loss: 2.2651677639677588....\n",
      "Validation loss: 2.2365492007682204....\n",
      "-----------------------------------\n",
      "Training loss: 2.2649853223111944....\n",
      "Validation loss: 2.2365144998506192....\n",
      "-----------------------------------\n",
      "Training loss: 2.2648029566021535....\n",
      "Validation loss: 2.236479849881886....\n",
      "-----------------------------------\n",
      "Training loss: 2.264620642480949....\n",
      "Validation loss: 2.2364447692814675....\n",
      "-----------------------------------\n",
      "Training loss: 2.2644384194862117....\n",
      "Validation loss: 2.2364096986059114....\n",
      "-----------------------------------\n",
      "Training loss: 2.264256009637451....\n",
      "Validation loss: 2.236374618137411....\n",
      "-----------------------------------\n",
      "Training loss: 2.2640732151975533....\n",
      "Validation loss: 2.2363400038484404....\n",
      "-----------------------------------\n",
      "Training loss: 2.263890120275122....\n",
      "Validation loss: 2.236304890977729....\n",
      "-----------------------------------\n",
      "Training loss: 2.263707011734507....\n",
      "Validation loss: 2.2362699720172503....\n",
      "-----------------------------------\n",
      "Training loss: 2.263524261647295....\n",
      "Validation loss: 2.2362350223710687....\n",
      "-----------------------------------\n",
      "Training loss: 2.263341211638736....\n",
      "Validation loss: 2.236200104570823....\n",
      "-----------------------------------\n",
      "Training loss: 2.263157895419309....\n",
      "Validation loss: 2.2361656005370754....\n",
      "-----------------------------------\n",
      "Training loss: 2.2629745417928424....\n",
      "Validation loss: 2.2361312652390275....\n",
      "-----------------------------------\n",
      "Training loss: 2.2627916829240338....\n",
      "Validation loss: 2.2360971374787346....\n",
      "-----------------------------------\n",
      "Training loss: 2.2626085795425093....\n",
      "Validation loss: 2.2360627716780845....\n",
      "-----------------------------------\n",
      "Training loss: 2.262425407663561....\n",
      "Validation loss: 2.2360281794921493....\n",
      "-----------------------------------\n",
      "Training loss: 2.2622420243987036....\n",
      "Validation loss: 2.23599329482827....\n",
      "-----------------------------------\n",
      "Training loss: 2.262058596823084....\n",
      "Validation loss: 2.2359587781810215....\n",
      "-----------------------------------\n",
      "Training loss: 2.2618752126621624....\n",
      "Validation loss: 2.235923150091746....\n",
      "-----------------------------------\n",
      "Training loss: 2.261692335010954....\n",
      "Validation loss: 2.2358875543355627....\n",
      "-----------------------------------\n",
      "Training loss: 2.2615094113445044....\n",
      "Validation loss: 2.235851691876099....\n",
      "-----------------------------------\n",
      "Training loss: 2.261326611753745....\n",
      "Validation loss: 2.235815808799459....\n",
      "-----------------------------------\n",
      "Training loss: 2.261143817930139....\n",
      "Validation loss: 2.2357781357756608....\n",
      "-----------------------------------\n",
      "Training loss: 2.260961289664072....\n",
      "Validation loss: 2.2357405303389766....\n",
      "-----------------------------------\n",
      "Training loss: 2.2607785002304555....\n",
      "Validation loss: 2.235702875397853....\n",
      "-----------------------------------\n",
      "Training loss: 2.2605958845340135....\n",
      "Validation loss: 2.2356653911204534....\n",
      "-----------------------------------\n",
      "Training loss: 2.2604136551138634....\n",
      "Validation loss: 2.2356280726788946....\n",
      "-----------------------------------\n",
      "Training loss: 2.2602309052268965....\n",
      "Validation loss: 2.2355904929201813....\n",
      "-----------------------------------\n",
      "Training loss: 2.2600484100561986....\n",
      "Validation loss: 2.2355528582284685....\n",
      "-----------------------------------\n",
      "Training loss: 2.259865810581774....\n",
      "Validation loss: 2.2355147900289274....\n",
      "-----------------------------------\n",
      "Training loss: 2.259683439860288....\n",
      "Validation loss: 2.235476499608321....\n",
      "-----------------------------------\n",
      "Training loss: 2.2595012559878485....\n",
      "Validation loss: 2.235437987897329....\n",
      "-----------------------------------\n",
      "Training loss: 2.25931927390522....\n",
      "Validation loss: 2.2353997530339305....\n",
      "-----------------------------------\n",
      "Training loss: 2.2591369477653007....\n",
      "Validation loss: 2.2353619471768114....\n",
      "-----------------------------------\n",
      "Training loss: 2.258954031607955....\n",
      "Validation loss: 2.2353237502030274....\n",
      "-----------------------------------\n",
      "Training loss: 2.258771333974903....\n",
      "Validation loss: 2.235285385800805....\n",
      "-----------------------------------\n",
      "Training loss: 2.2585887545014525....\n",
      "Validation loss: 2.2352469454760286....\n",
      "-----------------------------------\n",
      "Training loss: 2.258406032244309....\n",
      "Validation loss: 2.2352082718957647....\n",
      "-----------------------------------\n",
      "Training loss: 2.258223217205562....\n",
      "Validation loss: 2.2351690064013594....\n",
      "-----------------------------------\n",
      "Training loss: 2.2580404594136994....\n",
      "Validation loss: 2.235130447774994....\n",
      "-----------------------------------\n",
      "Training loss: 2.257857488309252....\n",
      "Validation loss: 2.2350922177440116....\n",
      "-----------------------------------\n",
      "Training loss: 2.2576745308064785....\n",
      "Validation loss: 2.2350537646898245....\n",
      "-----------------------------------\n",
      "Training loss: 2.257491732727509....\n",
      "Validation loss: 2.2350149041437684....\n",
      "-----------------------------------\n",
      "Training loss: 2.2573092893937607....\n",
      "Validation loss: 2.2349760217684005....\n",
      "-----------------------------------\n",
      "Training loss: 2.257126566692589....\n",
      "Validation loss: 2.2349369214271446....\n",
      "-----------------------------------\n",
      "Training loss: 2.256943686729243....\n",
      "Validation loss: 2.23489761111722....\n",
      "-----------------------------------\n",
      "Training loss: 2.256760669428003....\n",
      "Validation loss: 2.2348580594177543....\n",
      "-----------------------------------\n",
      "Training loss: 2.2565779338951795....\n",
      "Validation loss: 2.234818207109738....\n",
      "-----------------------------------\n",
      "Training loss: 2.2563951846603243....\n",
      "Validation loss: 2.2347788624916647....\n",
      "-----------------------------------\n",
      "Training loss: 2.256212714622627....\n",
      "Validation loss: 2.234739567423799....\n",
      "-----------------------------------\n",
      "Training loss: 2.2560308015811446....\n",
      "Validation loss: 2.2346997854269013....\n",
      "-----------------------------------\n",
      "Training loss: 2.2558490306888834....\n",
      "Validation loss: 2.2346599533914278....\n",
      "-----------------------------------\n",
      "Training loss: 2.25566744114568....\n",
      "Validation loss: 2.2346200726618015....\n",
      "-----------------------------------\n",
      "Training loss: 2.255486197917584....\n",
      "Validation loss: 2.234580190639181....\n",
      "-----------------------------------\n",
      "Training loss: 2.2553050339571423....\n",
      "Validation loss: 2.234540162147293....\n",
      "-----------------------------------\n",
      "Training loss: 2.2551240282262643....\n",
      "Validation loss: 2.2345004652607403....\n",
      "-----------------------------------\n",
      "Training loss: 2.2549431724762186....\n",
      "Validation loss: 2.234460338647992....\n",
      "-----------------------------------\n",
      "Training loss: 2.2547630424889102....\n",
      "Validation loss: 2.234419813188917....\n",
      "-----------------------------------\n",
      "Training loss: 2.254583176614368....\n",
      "Validation loss: 2.234378704592569....\n",
      "-----------------------------------\n",
      "Training loss: 2.254403524806826....\n",
      "Validation loss: 2.2343367640751857....\n",
      "-----------------------------------\n",
      "Training loss: 2.2542244169449996....\n",
      "Validation loss: 2.2342947710083254....\n",
      "-----------------------------------\n",
      "Training loss: 2.2540453060452608....\n",
      "Validation loss: 2.234252532544356....\n",
      "-----------------------------------\n",
      "Training loss: 2.253866411093739....\n",
      "Validation loss: 2.2342100136662646....\n",
      "-----------------------------------\n",
      "Training loss: 2.253687250744035....\n",
      "Validation loss: 2.2341675128374483....\n",
      "-----------------------------------\n",
      "Training loss: 2.2535083381689494....\n",
      "Validation loss: 2.2341243293300086....\n",
      "-----------------------------------\n",
      "Training loss: 2.253329879318526....\n",
      "Validation loss: 2.234080662259379....\n",
      "-----------------------------------\n",
      "Training loss: 2.253151288431592....\n",
      "Validation loss: 2.234036343336905....\n",
      "-----------------------------------\n",
      "Training loss: 2.2529724745783617....\n",
      "Validation loss: 2.2339923869484597....\n",
      "-----------------------------------\n",
      "Training loss: 2.252793169584854....\n",
      "Validation loss: 2.2339483568954406....\n",
      "-----------------------------------\n",
      "Training loss: 2.252613929816689....\n",
      "Validation loss: 2.2339036269178134....\n",
      "-----------------------------------\n",
      "Training loss: 2.252434964712693....\n",
      "Validation loss: 2.233858971690439....\n",
      "-----------------------------------\n",
      "Training loss: 2.2522560126238917....\n",
      "Validation loss: 2.2338144678381....\n",
      "-----------------------------------\n",
      "Training loss: 2.2520768456837947....\n",
      "Validation loss: 2.233770012413534....\n",
      "-----------------------------------\n",
      "Training loss: 2.2518979785218836....\n",
      "Validation loss: 2.2337250139670144....\n",
      "-----------------------------------\n",
      "Training loss: 2.251719276687357....\n",
      "Validation loss: 2.2336793975064757....\n",
      "-----------------------------------\n",
      "Training loss: 2.2515411059002894....\n",
      "Validation loss: 2.233633535907564....\n",
      "-----------------------------------\n",
      "Training loss: 2.2513629765185375....\n",
      "Validation loss: 2.233586990340303....\n",
      "-----------------------------------\n",
      "Training loss: 2.251184975826711....\n",
      "Validation loss: 2.233539949424134....\n",
      "-----------------------------------\n",
      "Training loss: 2.251007258233733....\n",
      "Validation loss: 2.233492625381086....\n",
      "-----------------------------------\n",
      "Training loss: 2.2508297960932953....\n",
      "Validation loss: 2.2334450128002072....\n",
      "-----------------------------------\n",
      "Training loss: 2.250653104533444....\n",
      "Validation loss: 2.233397234235621....\n",
      "-----------------------------------\n",
      "Training loss: 2.2504765458718254....\n",
      "Validation loss: 2.233349493918573....\n",
      "-----------------------------------\n",
      "Training loss: 2.2502997464584484....\n",
      "Validation loss: 2.2333017106267317....\n",
      "-----------------------------------\n",
      "Training loss: 2.2501230530069....\n",
      "Validation loss: 2.23325466899622....\n",
      "-----------------------------------\n",
      "Training loss: 2.249946317124611....\n",
      "Validation loss: 2.2332075586736346....\n",
      "-----------------------------------\n",
      "Training loss: 2.249769884176167....\n",
      "Validation loss: 2.233159987667268....\n",
      "-----------------------------------\n",
      "Training loss: 2.249593344035377....\n",
      "Validation loss: 2.233112542479266....\n",
      "-----------------------------------\n",
      "Training loss: 2.249416852335717....\n",
      "Validation loss: 2.2330643226435765....\n",
      "-----------------------------------\n",
      "Training loss: 2.2492405017278547....\n",
      "Validation loss: 2.233016109377801....\n",
      "-----------------------------------\n",
      "Training loss: 2.2490643778310333....\n",
      "Validation loss: 2.232968136910996....\n",
      "-----------------------------------\n",
      "Training loss: 2.2488882156194263....\n",
      "Validation loss: 2.2329197682454804....\n",
      "-----------------------------------\n",
      "Training loss: 2.2487119397207924....\n",
      "Validation loss: 2.232871179452455....\n",
      "-----------------------------------\n",
      "Training loss: 2.248535896923....\n",
      "Validation loss: 2.232822604344713....\n",
      "-----------------------------------\n",
      "Training loss: 2.2483600202037937....\n",
      "Validation loss: 2.232773899105982....\n",
      "-----------------------------------\n",
      "Training loss: 2.2481842137275434....\n",
      "Validation loss: 2.2327251265688592....\n",
      "-----------------------------------\n",
      "Training loss: 2.248008499576993....\n",
      "Validation loss: 2.232675906667756....\n",
      "-----------------------------------\n",
      "Training loss: 2.247832918322141....\n",
      "Validation loss: 2.2326261839198955....\n",
      "-----------------------------------\n",
      "Training loss: 2.247657216258607....\n",
      "Validation loss: 2.232576524400154....\n",
      "-----------------------------------\n",
      "Training loss: 2.2474817638478575....\n",
      "Validation loss: 2.2325267012601797....\n",
      "-----------------------------------\n",
      "Training loss: 2.2473069216736805....\n",
      "Validation loss: 2.2324757735092935....\n",
      "-----------------------------------\n",
      "Training loss: 2.247132289057149....\n",
      "Validation loss: 2.232425545436509....\n",
      "-----------------------------------\n",
      "Training loss: 2.2469574593082426....\n",
      "Validation loss: 2.232374926572226....\n",
      "-----------------------------------\n",
      "Training loss: 2.2467821955662606....\n",
      "Validation loss: 2.2323250247247906....\n",
      "-----------------------------------\n",
      "Training loss: 2.24660667424003....\n",
      "Validation loss: 2.2322747194544243....\n",
      "-----------------------------------\n",
      "Training loss: 2.2464316786459064....\n",
      "Validation loss: 2.2322247524709633....\n",
      "-----------------------------------\n",
      "Training loss: 2.2462570067549104....\n",
      "Validation loss: 2.2321740771703404....\n",
      "-----------------------------------\n",
      "Training loss: 2.246082758657432....\n",
      "Validation loss: 2.2321231587145802....\n",
      "-----------------------------------\n",
      "Training loss: 2.2459086114680926....\n",
      "Validation loss: 2.232071201100189....\n",
      "-----------------------------------\n",
      "Training loss: 2.2457344062232485....\n",
      "Validation loss: 2.2320198463725416....\n",
      "-----------------------------------\n",
      "Training loss: 2.2455603826091806....\n",
      "Validation loss: 2.2319677388623527....\n",
      "-----------------------------------\n",
      "Training loss: 2.2453856716096787....\n",
      "Validation loss: 2.231916565709974....\n",
      "-----------------------------------\n",
      "Training loss: 2.245210487923083....\n",
      "Validation loss: 2.2318655300650017....\n",
      "-----------------------------------\n",
      "Training loss: 2.2450353632923856....\n",
      "Validation loss: 2.231814721516509....\n",
      "-----------------------------------\n",
      "Training loss: 2.2448600611778304....\n",
      "Validation loss: 2.231763298722884....\n",
      "-----------------------------------\n",
      "Training loss: 2.24468489988729....\n",
      "Validation loss: 2.2317114891490832....\n",
      "-----------------------------------\n",
      "Training loss: 2.2445099360327587....\n",
      "Validation loss: 2.231659504006124....\n",
      "-----------------------------------\n",
      "Training loss: 2.2443349672362616....\n",
      "Validation loss: 2.231606974059282....\n",
      "-----------------------------------\n",
      "Training loss: 2.244159830431004....\n",
      "Validation loss: 2.2315548619455003....\n",
      "-----------------------------------\n",
      "Training loss: 2.2439843753609288....\n",
      "Validation loss: 2.2315025101817736....\n",
      "-----------------------------------\n",
      "Training loss: 2.2438088575424664....\n",
      "Validation loss: 2.2314501390883907....\n",
      "-----------------------------------\n",
      "Training loss: 2.2436333410415297....\n",
      "Validation loss: 2.2313976916713845....\n",
      "-----------------------------------\n",
      "Training loss: 2.243458043711815....\n",
      "Validation loss: 2.231345209782062....\n",
      "-----------------------------------\n",
      "Training loss: 2.2432826790908944....\n",
      "Validation loss: 2.2312927065583237....\n",
      "-----------------------------------\n",
      "Training loss: 2.2431068920811867....\n",
      "Validation loss: 2.231239232103747....\n",
      "-----------------------------------\n",
      "Training loss: 2.242931200875725....\n",
      "Validation loss: 2.2311863059684045....\n",
      "-----------------------------------\n",
      "Training loss: 2.2427552994648865....\n",
      "Validation loss: 2.231132665996599....\n",
      "-----------------------------------\n",
      "Training loss: 2.242579478486597....\n",
      "Validation loss: 2.231078748039252....\n",
      "-----------------------------------\n",
      "Training loss: 2.242403788989055....\n",
      "Validation loss: 2.2310250108704586....\n",
      "-----------------------------------\n",
      "Training loss: 2.2422280393574336....\n",
      "Validation loss: 2.2309713411909318....\n",
      "-----------------------------------\n",
      "Training loss: 2.24205233553269....\n",
      "Validation loss: 2.2309171032332817....\n",
      "-----------------------------------\n",
      "Training loss: 2.241876595570831....\n",
      "Validation loss: 2.230863011447691....\n",
      "-----------------------------------\n",
      "Training loss: 2.2417004719540423....\n",
      "Validation loss: 2.2308084311540184....\n",
      "-----------------------------------\n",
      "Training loss: 2.2415243772659004....\n",
      "Validation loss: 2.230753322946889....\n",
      "-----------------------------------\n",
      "Training loss: 2.241348184131183....\n",
      "Validation loss: 2.2306982325393356....\n",
      "-----------------------------------\n",
      "Training loss: 2.2411718617990943....\n",
      "Validation loss: 2.230642257775468....\n",
      "-----------------------------------\n",
      "Training loss: 2.2409954363938454....\n",
      "Validation loss: 2.230586036573152....\n",
      "-----------------------------------\n",
      "Training loss: 2.2408191671112307....\n",
      "Validation loss: 2.2305293361117666....\n",
      "-----------------------------------\n",
      "Training loss: 2.2406429322051205....\n",
      "Validation loss: 2.2304731701472904....\n",
      "-----------------------------------\n",
      "Training loss: 2.240467233462815....\n",
      "Validation loss: 2.230416400656968....\n",
      "-----------------------------------\n",
      "Training loss: 2.240291611900202....\n",
      "Validation loss: 2.2303599686908258....\n",
      "-----------------------------------\n",
      "Training loss: 2.240115855829712....\n",
      "Validation loss: 2.230303790454337....\n",
      "-----------------------------------\n",
      "Training loss: 2.239940148834035....\n",
      "Validation loss: 2.2302483595577742....\n",
      "-----------------------------------\n",
      "Training loss: 2.239764617726153....\n",
      "Validation loss: 2.230192192878634....\n",
      "-----------------------------------\n",
      "Training loss: 2.2395889161887306....\n",
      "Validation loss: 2.2301361567693556....\n",
      "-----------------------------------\n",
      "Training loss: 2.2394126855399645....\n",
      "Validation loss: 2.2300806210274806....\n",
      "-----------------------------------\n",
      "Training loss: 2.239236111664199....\n",
      "Validation loss: 2.2300254793426557....\n",
      "-----------------------------------\n",
      "Training loss: 2.2390595496935055....\n",
      "Validation loss: 2.229970407129232....\n",
      "-----------------------------------\n",
      "Training loss: 2.238883316808873....\n",
      "Validation loss: 2.2299137627154595....\n",
      "-----------------------------------\n",
      "Training loss: 2.23870731394174....\n",
      "Validation loss: 2.229857345752717....\n",
      "-----------------------------------\n",
      "Training loss: 2.2385309962921185....\n",
      "Validation loss: 2.2298000437758483....\n",
      "-----------------------------------\n",
      "Training loss: 2.2383551725367066....\n",
      "Validation loss: 2.2297434517929076....\n",
      "-----------------------------------\n",
      "Training loss: 2.238179461029514....\n",
      "Validation loss: 2.229685981908626....\n",
      "-----------------------------------\n",
      "Training loss: 2.2380037031197744....\n",
      "Validation loss: 2.229628502952603....\n",
      "-----------------------------------\n",
      "Training loss: 2.237828156482314....\n",
      "Validation loss: 2.229571292103472....\n",
      "-----------------------------------\n",
      "Training loss: 2.237652472225237....\n",
      "Validation loss: 2.229513188354141....\n",
      "-----------------------------------\n",
      "Training loss: 2.2374767170515892....\n",
      "Validation loss: 2.2294549559846493....\n",
      "-----------------------------------\n",
      "Training loss: 2.237300144762071....\n",
      "Validation loss: 2.2293968638463504....\n",
      "-----------------------------------\n",
      "Training loss: 2.2371236517650357....\n",
      "Validation loss: 2.229340201829682....\n",
      "-----------------------------------\n",
      "Training loss: 2.236946854592925....\n",
      "Validation loss: 2.2292833678531094....\n",
      "-----------------------------------\n",
      "Training loss: 2.2367701915776883....\n",
      "Validation loss: 2.2292275834397306....\n",
      "-----------------------------------\n",
      "Training loss: 2.236593995242856....\n",
      "Validation loss: 2.229170985815561....\n",
      "-----------------------------------\n",
      "Training loss: 2.236418130305192....\n",
      "Validation loss: 2.22911474139463....\n",
      "-----------------------------------\n",
      "Training loss: 2.236241964695634....\n",
      "Validation loss: 2.22905778702129....\n",
      "-----------------------------------\n",
      "Training loss: 2.2360653920664624....\n",
      "Validation loss: 2.2290007875109374....\n",
      "-----------------------------------\n",
      "Training loss: 2.2358889167214144....\n",
      "Validation loss: 2.228943835696634....\n",
      "-----------------------------------\n",
      "Training loss: 2.2357123684144127....\n",
      "Validation loss: 2.2288863848715263....\n",
      "-----------------------------------\n",
      "Training loss: 2.2355352754311433....\n",
      "Validation loss: 2.228829728038847....\n",
      "-----------------------------------\n",
      "Training loss: 2.235358063794388....\n",
      "Validation loss: 2.2287728807788807....\n",
      "-----------------------------------\n",
      "Training loss: 2.2351804789042724....\n",
      "Validation loss: 2.228716423353989....\n",
      "-----------------------------------\n",
      "Training loss: 2.2350026358812705....\n",
      "Validation loss: 2.228659234020014....\n",
      "-----------------------------------\n",
      "Training loss: 2.2348244943516447....\n",
      "Validation loss: 2.228601853355637....\n",
      "-----------------------------------\n",
      "Training loss: 2.2346460941583692....\n",
      "Validation loss: 2.228545148577691....\n",
      "-----------------------------------\n",
      "Training loss: 2.2344668881913896....\n",
      "Validation loss: 2.228487839956265....\n",
      "-----------------------------------\n",
      "Training loss: 2.2342875155805753....\n",
      "Validation loss: 2.2284305471654036....\n",
      "-----------------------------------\n",
      "Training loss: 2.234108114283216....\n",
      "Validation loss: 2.228373136426062....\n",
      "-----------------------------------\n",
      "Training loss: 2.233928242304993....\n",
      "Validation loss: 2.228315577645758....\n",
      "-----------------------------------\n",
      "Training loss: 2.233748114313719....\n",
      "Validation loss: 2.2282581578934195....\n",
      "-----------------------------------\n",
      "Training loss: 2.2335676661536894....\n",
      "Validation loss: 2.2281994067783946....\n",
      "-----------------------------------\n",
      "Training loss: 2.233387608002505....\n",
      "Validation loss: 2.2281410334479705....\n",
      "-----------------------------------\n",
      "Training loss: 2.233207526509745....\n",
      "Validation loss: 2.228081773920917....\n",
      "-----------------------------------\n",
      "Training loss: 2.2330271831190394....\n",
      "Validation loss: 2.228023162861505....\n",
      "-----------------------------------\n",
      "Training loss: 2.2328470123715958....\n",
      "Validation loss: 2.2279641180592447....\n",
      "-----------------------------------\n",
      "Training loss: 2.2326667064043337....\n",
      "Validation loss: 2.2279053831203885....\n",
      "-----------------------------------\n",
      "Training loss: 2.232486385291479....\n",
      "Validation loss: 2.2278454461877732....\n",
      "-----------------------------------\n",
      "Training loss: 2.2323056437866686....\n",
      "Validation loss: 2.227786500677479....\n",
      "-----------------------------------\n",
      "Training loss: 2.2321247868532677....\n",
      "Validation loss: 2.2277267301367574....\n",
      "-----------------------------------\n",
      "Training loss: 2.231943604706798....\n",
      "Validation loss: 2.227666934151905....\n",
      "-----------------------------------\n",
      "Training loss: 2.231761874085517....\n",
      "Validation loss: 2.2276079710372203....\n",
      "-----------------------------------\n",
      "Training loss: 2.2315799517937944....\n",
      "Validation loss: 2.227548055176259....\n",
      "-----------------------------------\n",
      "Training loss: 2.2313979909400143....\n",
      "Validation loss: 2.2274890034525723....\n",
      "-----------------------------------\n",
      "Training loss: 2.2312163142366317....\n",
      "Validation loss: 2.227429388455724....\n",
      "-----------------------------------\n",
      "Training loss: 2.2310348861417433....\n",
      "Validation loss: 2.2273702204824795....\n",
      "-----------------------------------\n",
      "Training loss: 2.230853379333917....\n",
      "Validation loss: 2.2273106151140434....\n",
      "-----------------------------------\n",
      "Training loss: 2.2306717871301864....\n",
      "Validation loss: 2.2272512052579434....\n",
      "-----------------------------------\n",
      "Training loss: 2.230490530994232....\n",
      "Validation loss: 2.227191037162193....\n",
      "-----------------------------------\n",
      "Training loss: 2.230308765519846....\n",
      "Validation loss: 2.227131955128708....\n",
      "-----------------------------------\n",
      "Training loss: 2.230126758697782....\n",
      "Validation loss: 2.227072133005316....\n",
      "-----------------------------------\n",
      "Training loss: 2.229944921517453....\n",
      "Validation loss: 2.2270126647914843....\n",
      "-----------------------------------\n",
      "Training loss: 2.229762902935228....\n",
      "Validation loss: 2.2269528833044316....\n",
      "-----------------------------------\n",
      "Training loss: 2.2295807987235245....\n",
      "Validation loss: 2.226893791511556....\n",
      "-----------------------------------\n",
      "Training loss: 2.2293984115853025....\n",
      "Validation loss: 2.2268333497986013....\n",
      "-----------------------------------\n",
      "Training loss: 2.229215995825143....\n",
      "Validation loss: 2.2267734288388144....\n",
      "-----------------------------------\n",
      "Training loss: 2.229033938878881....\n",
      "Validation loss: 2.2267123480236637....\n",
      "-----------------------------------\n",
      "Training loss: 2.228852053469822....\n",
      "Validation loss: 2.2266513504827183....\n",
      "-----------------------------------\n",
      "Training loss: 2.2286701918469802....\n",
      "Validation loss: 2.22659003895766....\n",
      "-----------------------------------\n",
      "Training loss: 2.228487471899002....\n",
      "Validation loss: 2.226530064335477....\n",
      "-----------------------------------\n",
      "Training loss: 2.2283039050020226....\n",
      "Validation loss: 2.226468930990802....\n",
      "-----------------------------------\n",
      "Training loss: 2.2281208284406215....\n",
      "Validation loss: 2.2264086209173692....\n",
      "-----------------------------------\n",
      "Training loss: 2.2279370899091386....\n",
      "Validation loss: 2.2263479863897446....\n",
      "-----------------------------------\n",
      "Training loss: 2.227752916764915....\n",
      "Validation loss: 2.2262877571530706....\n",
      "-----------------------------------\n",
      "Training loss: 2.227568274268244....\n",
      "Validation loss: 2.2262266057302105....\n",
      "-----------------------------------\n",
      "Training loss: 2.227383646431163....\n",
      "Validation loss: 2.2261656444413647....\n",
      "-----------------------------------\n",
      "Training loss: 2.2271994328466524....\n",
      "Validation loss: 2.2261040822266898....\n",
      "-----------------------------------\n",
      "Training loss: 2.2270154373314615....\n",
      "Validation loss: 2.226042683764386....\n",
      "-----------------------------------\n",
      "Training loss: 2.2268311451818286....\n",
      "Validation loss: 2.225980324026254....\n",
      "-----------------------------------\n",
      "Training loss: 2.2266470504432436....\n",
      "Validation loss: 2.2259170302153413....\n",
      "-----------------------------------\n",
      "Training loss: 2.2264629730226635....\n",
      "Validation loss: 2.2258532161314504....\n",
      "-----------------------------------\n",
      "Training loss: 2.2262795487263296....\n",
      "Validation loss: 2.225789991558809....\n",
      "-----------------------------------\n",
      "Training loss: 2.22609587870128....\n",
      "Validation loss: 2.225726283543336....\n",
      "-----------------------------------\n",
      "Training loss: 2.225912450723055....\n",
      "Validation loss: 2.2256633562723227....\n",
      "-----------------------------------\n",
      "Training loss: 2.2257298502866343....\n",
      "Validation loss: 2.2255992653927756....\n",
      "-----------------------------------\n",
      "Training loss: 2.2255476991597503....\n",
      "Validation loss: 2.225535552380361....\n",
      "-----------------------------------\n",
      "Training loss: 2.225365785161958....\n",
      "Validation loss: 2.2254716516366813....\n",
      "-----------------------------------\n",
      "Training loss: 2.22518408573087....\n",
      "Validation loss: 2.2254086565796256....\n",
      "-----------------------------------\n",
      "Training loss: 2.2250027784839235....\n",
      "Validation loss: 2.225345019992822....\n",
      "-----------------------------------\n",
      "Training loss: 2.224821193602309....\n",
      "Validation loss: 2.2252829436598915....\n",
      "-----------------------------------\n",
      "Training loss: 2.224638993527361....\n",
      "Validation loss: 2.2252196522114303....\n",
      "-----------------------------------\n",
      "Training loss: 2.2244570756582194....\n",
      "Validation loss: 2.2251565554224264....\n",
      "-----------------------------------\n",
      "Training loss: 2.224275188440791....\n",
      "Validation loss: 2.2250927456080163....\n",
      "-----------------------------------\n",
      "Training loss: 2.2240935396017325....\n",
      "Validation loss: 2.2250299126995796....\n",
      "-----------------------------------\n",
      "Training loss: 2.223911784819396....\n",
      "Validation loss: 2.224966315636557....\n",
      "-----------------------------------\n",
      "Training loss: 2.2237301162344423....\n",
      "Validation loss: 2.224904004982507....\n",
      "-----------------------------------\n",
      "Training loss: 2.2235478811835545....\n",
      "Validation loss: 2.2248412502320716....\n",
      "-----------------------------------\n",
      "Training loss: 2.2233662703051564....\n",
      "Validation loss: 2.2247789968676566....\n",
      "-----------------------------------\n",
      "Training loss: 2.2231849992037347....\n",
      "Validation loss: 2.224716966054065....\n",
      "-----------------------------------\n",
      "Training loss: 2.2230037853328644....\n",
      "Validation loss: 2.2246554701939796....\n",
      "-----------------------------------\n",
      "Training loss: 2.222821335048169....\n",
      "Validation loss: 2.224593870901616....\n",
      "-----------------------------------\n",
      "Training loss: 2.2226380213665187....\n",
      "Validation loss: 2.2245325851945434....\n",
      "-----------------------------------\n",
      "Training loss: 2.2224541620390887....\n",
      "Validation loss: 2.224470626582543....\n",
      "-----------------------------------\n",
      "Training loss: 2.222270491565185....\n",
      "Validation loss: 2.224407914030543....\n",
      "-----------------------------------\n",
      "Training loss: 2.2220876125411557....\n",
      "Validation loss: 2.2243463412614957....\n",
      "-----------------------------------\n",
      "Training loss: 2.221904428758077....\n",
      "Validation loss: 2.2242853818045516....\n",
      "-----------------------------------\n",
      "Training loss: 2.221721246809254....\n",
      "Validation loss: 2.224224701075527....\n",
      "-----------------------------------\n",
      "Training loss: 2.2215382230842113....\n",
      "Validation loss: 2.2241631903076016....\n",
      "-----------------------------------\n",
      "Training loss: 2.2213560633864664....\n",
      "Validation loss: 2.224102031650134....\n",
      "-----------------------------------\n",
      "Training loss: 2.221174460994993....\n",
      "Validation loss: 2.2240401180920633....\n",
      "-----------------------------------\n",
      "Training loss: 2.2209929206574524....\n",
      "Validation loss: 2.2239783454030193....\n",
      "-----------------------------------\n",
      "Training loss: 2.2208117877855815....\n",
      "Validation loss: 2.223916435268147....\n",
      "-----------------------------------\n",
      "Training loss: 2.2206309263185546....\n",
      "Validation loss: 2.2238545093946116....\n",
      "-----------------------------------\n",
      "Training loss: 2.2204502530208847....\n",
      "Validation loss: 2.223792051952025....\n",
      "-----------------------------------\n",
      "Training loss: 2.220270271750571....\n",
      "Validation loss: 2.2237290581015583....\n",
      "-----------------------------------\n",
      "Training loss: 2.220090754058748....\n",
      "Validation loss: 2.223666083741656....\n",
      "-----------------------------------\n",
      "Training loss: 2.2199105594873583....\n",
      "Validation loss: 2.223602904577385....\n",
      "-----------------------------------\n",
      "Training loss: 2.219730397333335....\n",
      "Validation loss: 2.2235392774119465....\n",
      "-----------------------------------\n",
      "Training loss: 2.2195504270756397....\n",
      "Validation loss: 2.223475376905967....\n",
      "-----------------------------------\n",
      "Training loss: 2.219370847351263....\n",
      "Validation loss: 2.223411467866455....\n",
      "-----------------------------------\n",
      "Training loss: 2.2191916529640534....\n",
      "Validation loss: 2.2233473964816057....\n",
      "-----------------------------------\n",
      "Training loss: 2.2190122762026903....\n",
      "Validation loss: 2.2232838906344674....\n",
      "-----------------------------------\n",
      "Training loss: 2.218832474754617....\n",
      "Validation loss: 2.2232198288917773....\n",
      "-----------------------------------\n",
      "Training loss: 2.2186528307158375....\n",
      "Validation loss: 2.2231560700827093....\n",
      "-----------------------------------\n",
      "Training loss: 2.2184729019969494....\n",
      "Validation loss: 2.223091283943747....\n",
      "-----------------------------------\n",
      "Training loss: 2.2182928931223977....\n",
      "Validation loss: 2.2230271703231494....\n",
      "-----------------------------------\n",
      "Training loss: 2.2181131542473103....\n",
      "Validation loss: 2.222962151918121....\n",
      "-----------------------------------\n",
      "Training loss: 2.21793351866761....\n",
      "Validation loss: 2.2228968665728526....\n",
      "-----------------------------------\n",
      "Training loss: 2.217753790084362....\n",
      "Validation loss: 2.222832032714747....\n",
      "-----------------------------------\n",
      "Training loss: 2.2175740807676485....\n",
      "Validation loss: 2.222765751923977....\n",
      "-----------------------------------\n",
      "Training loss: 2.2173945593847026....\n",
      "Validation loss: 2.222700143146372....\n",
      "-----------------------------------\n",
      "Training loss: 2.2172152228923583....\n",
      "Validation loss: 2.22263401124756....\n",
      "-----------------------------------\n",
      "Training loss: 2.217035575446994....\n",
      "Validation loss: 2.2225682629336667....\n",
      "-----------------------------------\n",
      "Training loss: 2.2168555152122043....\n",
      "Validation loss: 2.2225023188725146....\n",
      "-----------------------------------\n",
      "Training loss: 2.216675556097702....\n",
      "Validation loss: 2.2224366268996825....\n",
      "-----------------------------------\n",
      "Training loss: 2.2164958074408347....\n",
      "Validation loss: 2.2223720858372524....\n",
      "-----------------------------------\n",
      "Training loss: 2.216315963214522....\n",
      "Validation loss: 2.2223063663440517....\n",
      "-----------------------------------\n",
      "Training loss: 2.216136267201168....\n",
      "Validation loss: 2.2222421205576075....\n",
      "-----------------------------------\n",
      "Training loss: 2.2159562369162087....\n",
      "Validation loss: 2.222177521944931....\n",
      "-----------------------------------\n",
      "Training loss: 2.215776123624852....\n",
      "Validation loss: 2.222112198209846....\n",
      "-----------------------------------\n",
      "Training loss: 2.2155958163440337....\n",
      "Validation loss: 2.22204735088138....\n",
      "-----------------------------------\n",
      "Training loss: 2.2154152038770483....\n",
      "Validation loss: 2.221982404536037....\n",
      "-----------------------------------\n",
      "Training loss: 2.215234773065392....\n",
      "Validation loss: 2.2219165659877....\n",
      "-----------------------------------\n",
      "Training loss: 2.215054254874152....\n",
      "Validation loss: 2.2218508004877986....\n",
      "-----------------------------------\n",
      "Training loss: 2.214873537744981....\n",
      "Validation loss: 2.22178391605964....\n",
      "-----------------------------------\n",
      "Training loss: 2.214692906631714....\n",
      "Validation loss: 2.2217168779039347....\n",
      "-----------------------------------\n",
      "Training loss: 2.2145121848232687....\n",
      "Validation loss: 2.2216499573401123....\n",
      "-----------------------------------\n",
      "Training loss: 2.214331482496025....\n",
      "Validation loss: 2.221582988623465....\n",
      "-----------------------------------\n",
      "Training loss: 2.214150639057293....\n",
      "Validation loss: 2.2215159364494954....\n",
      "-----------------------------------\n",
      "Training loss: 2.2139696253815204....\n",
      "Validation loss: 2.2214474779189053....\n",
      "-----------------------------------\n",
      "Training loss: 2.2137881482951682....\n",
      "Validation loss: 2.2213789423210017....\n",
      "-----------------------------------\n",
      "Training loss: 2.2136064094805636....\n",
      "Validation loss: 2.2213107448861593....\n",
      "-----------------------------------\n",
      "Training loss: 2.213424569693665....\n",
      "Validation loss: 2.2212427730667805....\n",
      "-----------------------------------\n",
      "Training loss: 2.213242638632184....\n",
      "Validation loss: 2.2211740912634683....\n",
      "-----------------------------------\n",
      "Training loss: 2.213060960123925....\n",
      "Validation loss: 2.221106145644062....\n",
      "-----------------------------------\n",
      "Training loss: 2.212879777399994....\n",
      "Validation loss: 2.2210374702034126....\n",
      "-----------------------------------\n",
      "Training loss: 2.2126986909328332....\n",
      "Validation loss: 2.220968754398898....\n",
      "-----------------------------------\n",
      "Training loss: 2.2125177461924816....\n",
      "Validation loss: 2.2208998987072737....\n",
      "-----------------------------------\n",
      "Training loss: 2.212336399999723....\n",
      "Validation loss: 2.220831383838585....\n",
      "-----------------------------------\n",
      "Training loss: 2.2121545344436133....\n",
      "Validation loss: 2.220763098576221....\n",
      "-----------------------------------\n",
      "Training loss: 2.211972823240917....\n",
      "Validation loss: 2.2206945889829592....\n",
      "-----------------------------------\n",
      "Training loss: 2.211791280746439....\n",
      "Validation loss: 2.2206249616226006....\n",
      "-----------------------------------\n",
      "Training loss: 2.2116096169821553....\n",
      "Validation loss: 2.2205556232243584....\n",
      "-----------------------------------\n",
      "Training loss: 2.2114287408418334....\n",
      "Validation loss: 2.2204864528163664....\n",
      "-----------------------------------\n",
      "Training loss: 2.2112480693405256....\n",
      "Validation loss: 2.2204166533340253....\n",
      "-----------------------------------\n",
      "Training loss: 2.211067121082164....\n",
      "Validation loss: 2.2203461189833464....\n",
      "-----------------------------------\n",
      "Training loss: 2.210886044119994....\n",
      "Validation loss: 2.2202763514777373....\n",
      "-----------------------------------\n",
      "Training loss: 2.210704865932183....\n",
      "Validation loss: 2.2202066005949948....\n",
      "-----------------------------------\n",
      "Training loss: 2.2105237297461815....\n",
      "Validation loss: 2.220136519781883....\n",
      "-----------------------------------\n",
      "Training loss: 2.210342593260813....\n",
      "Validation loss: 2.220066522754276....\n",
      "-----------------------------------\n",
      "Training loss: 2.2101617847740567....\n",
      "Validation loss: 2.219996189540928....\n",
      "-----------------------------------\n",
      "Training loss: 2.2099812900170113....\n",
      "Validation loss: 2.2199261724654162....\n",
      "-----------------------------------\n",
      "Training loss: 2.2098008959607207....\n",
      "Validation loss: 2.2198567006648595....\n",
      "-----------------------------------\n",
      "Training loss: 2.2096205213350437....\n",
      "Validation loss: 2.219786815396146....\n",
      "-----------------------------------\n",
      "Training loss: 2.209439986119061....\n",
      "Validation loss: 2.2197172914792485....\n",
      "-----------------------------------\n",
      "Training loss: 2.2092591709822704....\n",
      "Validation loss: 2.2196477851519867....\n",
      "-----------------------------------\n",
      "Training loss: 2.209078323819267....\n",
      "Validation loss: 2.2195777406767303....\n",
      "-----------------------------------\n",
      "Training loss: 2.2088978648535744....\n",
      "Validation loss: 2.219508103045266....\n",
      "-----------------------------------\n",
      "Training loss: 2.2087172804035498....\n",
      "Validation loss: 2.219438543210763....\n",
      "-----------------------------------\n",
      "Training loss: 2.2085371424857265....\n",
      "Validation loss: 2.219369154179815....\n",
      "-----------------------------------\n",
      "Training loss: 2.208357885096997....\n",
      "Validation loss: 2.2192990991579626....\n",
      "-----------------------------------\n",
      "Training loss: 2.2081789707520323....\n",
      "Validation loss: 2.219229323974725....\n",
      "-----------------------------------\n",
      "Training loss: 2.208000199984517....\n",
      "Validation loss: 2.219160067453013....\n",
      "-----------------------------------\n",
      "Training loss: 2.207820927194212....\n",
      "Validation loss: 2.2190905627454374....\n",
      "-----------------------------------\n",
      "Training loss: 2.2076417807655457....\n",
      "Validation loss: 2.2190214779511646....\n",
      "-----------------------------------\n",
      "Training loss: 2.2074629177338214....\n",
      "Validation loss: 2.218951604242559....\n",
      "-----------------------------------\n",
      "Training loss: 2.2072842591980666....\n",
      "Validation loss: 2.218881567076325....\n",
      "-----------------------------------\n",
      "Training loss: 2.2071055332200378....\n",
      "Validation loss: 2.218811681621564....\n",
      "-----------------------------------\n",
      "Training loss: 2.2069265891299987....\n",
      "Validation loss: 2.21874138906582....\n",
      "-----------------------------------\n",
      "Training loss: 2.206747038338628....\n",
      "Validation loss: 2.218670679411367....\n",
      "-----------------------------------\n",
      "Training loss: 2.206566858821427....\n",
      "Validation loss: 2.2185996771495455....\n",
      "-----------------------------------\n",
      "Training loss: 2.2063868582437847....\n",
      "Validation loss: 2.2185285872453537....\n",
      "-----------------------------------\n",
      "Training loss: 2.2062067898763695....\n",
      "Validation loss: 2.2184577785796704....\n",
      "-----------------------------------\n",
      "Training loss: 2.206026037230544....\n",
      "Validation loss: 2.2183874265490524....\n",
      "-----------------------------------\n",
      "Training loss: 2.205844395802141....\n",
      "Validation loss: 2.2183171482222392....\n",
      "-----------------------------------\n",
      "Training loss: 2.2056627340622743....\n",
      "Validation loss: 2.2182470089268644....\n",
      "-----------------------------------\n",
      "Training loss: 2.2054809246790565....\n",
      "Validation loss: 2.218176731940375....\n",
      "-----------------------------------\n",
      "Training loss: 2.2052991991588407....\n",
      "Validation loss: 2.218106508550879....\n",
      "-----------------------------------\n",
      "Training loss: 2.2051179540559414....\n",
      "Validation loss: 2.2180347285549424....\n",
      "-----------------------------------\n",
      "Training loss: 2.2049363727397693....\n",
      "Validation loss: 2.2179627218432123....\n",
      "-----------------------------------\n",
      "Training loss: 2.2047546901473187....\n",
      "Validation loss: 2.217890168144496....\n",
      "-----------------------------------\n",
      "Training loss: 2.2045726975073725....\n",
      "Validation loss: 2.217817472239201....\n",
      "-----------------------------------\n",
      "Training loss: 2.2043901914333923....\n",
      "Validation loss: 2.217744474362914....\n",
      "-----------------------------------\n",
      "Training loss: 2.204207529701361....\n",
      "Validation loss: 2.2176717872188103....\n",
      "-----------------------------------\n",
      "Training loss: 2.2040249960775....\n",
      "Validation loss: 2.2175989120248163....\n",
      "-----------------------------------\n",
      "Training loss: 2.203842202495026....\n",
      "Validation loss: 2.2175245302260356....\n",
      "-----------------------------------\n",
      "Training loss: 2.2036595901023808....\n",
      "Validation loss: 2.2174508632929286....\n",
      "-----------------------------------\n",
      "Training loss: 2.203477143153719....\n",
      "Validation loss: 2.2173767419672825....\n",
      "-----------------------------------\n",
      "Training loss: 2.2032947997267183....\n",
      "Validation loss: 2.217302389354567....\n",
      "-----------------------------------\n",
      "Training loss: 2.203113163624818....\n",
      "Validation loss: 2.21722814783103....\n",
      "-----------------------------------\n",
      "Training loss: 2.202931584513004....\n",
      "Validation loss: 2.2171535921208005....\n",
      "-----------------------------------\n",
      "Training loss: 2.2027497841806403....\n",
      "Validation loss: 2.217079854260061....\n",
      "-----------------------------------\n",
      "Training loss: 2.202568344451188....\n",
      "Validation loss: 2.2170052905580895....\n",
      "-----------------------------------\n",
      "Training loss: 2.2023871731106257....\n",
      "Validation loss: 2.2169312055522283....\n",
      "-----------------------------------\n",
      "Training loss: 2.202206224576228....\n",
      "Validation loss: 2.2168575332293186....\n",
      "-----------------------------------\n",
      "Training loss: 2.2020251779406945....\n",
      "Validation loss: 2.216783271235514....\n",
      "-----------------------------------\n",
      "Training loss: 2.2018442346158618....\n",
      "Validation loss: 2.216709208723163....\n",
      "-----------------------------------\n",
      "Training loss: 2.201663113733061....\n",
      "Validation loss: 2.2166354855599324....\n",
      "-----------------------------------\n",
      "Training loss: 2.2014818267654026....\n",
      "Validation loss: 2.2165612075625867....\n",
      "-----------------------------------\n",
      "Training loss: 2.2013000036864563....\n",
      "Validation loss: 2.216486622957339....\n",
      "-----------------------------------\n",
      "Training loss: 2.2011179807881547....\n",
      "Validation loss: 2.2164118845986978....\n",
      "-----------------------------------\n",
      "Training loss: 2.200935948291702....\n",
      "Validation loss: 2.2163372907802055....\n",
      "-----------------------------------\n",
      "Training loss: 2.2007537920277045....\n",
      "Validation loss: 2.2162622671848315....\n",
      "-----------------------------------\n",
      "Training loss: 2.200571869723805....\n",
      "Validation loss: 2.2161881621246753....\n",
      "-----------------------------------\n",
      "Training loss: 2.200390113076558....\n",
      "Validation loss: 2.2161151251130122....\n",
      "-----------------------------------\n",
      "Training loss: 2.2002077822306014....\n",
      "Validation loss: 2.2160416200032707....\n",
      "-----------------------------------\n",
      "Training loss: 2.2000254014223013....\n",
      "Validation loss: 2.2159677094682784....\n",
      "-----------------------------------\n",
      "Training loss: 2.199842939463418....\n",
      "Validation loss: 2.2158942646197994....\n",
      "-----------------------------------\n",
      "Training loss: 2.1996601331804486....\n",
      "Validation loss: 2.215820691173226....\n",
      "-----------------------------------\n",
      "Training loss: 2.199477215746708....\n",
      "Validation loss: 2.2157475485878244....\n",
      "-----------------------------------\n",
      "Training loss: 2.1992943435464487....\n",
      "Validation loss: 2.2156738173627732....\n",
      "-----------------------------------\n",
      "Training loss: 2.1991112478898254....\n",
      "Validation loss: 2.2156008212862863....\n",
      "-----------------------------------\n",
      "Training loss: 2.1989278290109584....\n",
      "Validation loss: 2.215527679006646....\n",
      "-----------------------------------\n",
      "Training loss: 2.1987444251903017....\n",
      "Validation loss: 2.2154542600289377....\n",
      "-----------------------------------\n",
      "Training loss: 2.198561113558289....\n",
      "Validation loss: 2.215382202871228....\n",
      "-----------------------------------\n",
      "Training loss: 2.1983779665270107....\n",
      "Validation loss: 2.2153093149179997....\n",
      "-----------------------------------\n",
      "Training loss: 2.198194396179863....\n",
      "Validation loss: 2.215237394224647....\n",
      "-----------------------------------\n",
      "Training loss: 2.1980109334410076....\n",
      "Validation loss: 2.215165218141444....\n",
      "-----------------------------------\n",
      "Training loss: 2.1978274832510745....\n",
      "Validation loss: 2.2150926534770643....\n",
      "-----------------------------------\n",
      "Training loss: 2.1976439834828487....\n",
      "Validation loss: 2.215020017644496....\n",
      "-----------------------------------\n",
      "Training loss: 2.1974604197456564....\n",
      "Validation loss: 2.2149463979147095....\n",
      "-----------------------------------\n",
      "Training loss: 2.1972769028283032....\n",
      "Validation loss: 2.2148731666336907....\n",
      "-----------------------------------\n",
      "Training loss: 2.197093606023721....\n",
      "Validation loss: 2.2147998574016023....\n",
      "-----------------------------------\n",
      "Training loss: 2.1969104064312592....\n",
      "Validation loss: 2.2147257782411....\n",
      "-----------------------------------\n",
      "Training loss: 2.196727000494829....\n",
      "Validation loss: 2.2146524531422505....\n",
      "-----------------------------------\n",
      "Training loss: 2.196543694394664....\n",
      "Validation loss: 2.214578512810878....\n",
      "-----------------------------------\n",
      "Training loss: 2.1963605438470806....\n",
      "Validation loss: 2.2145046529766157....\n",
      "-----------------------------------\n",
      "Training loss: 2.196177995379365....\n",
      "Validation loss: 2.2144306986670435....\n",
      "-----------------------------------\n",
      "Training loss: 2.195995746691519....\n",
      "Validation loss: 2.214355679344633....\n",
      "-----------------------------------\n",
      "Training loss: 2.1958137019412285....\n",
      "Validation loss: 2.21428169916656....\n",
      "-----------------------------------\n",
      "Training loss: 2.195631254986445....\n",
      "Validation loss: 2.21420729535155....\n",
      "-----------------------------------\n",
      "Training loss: 2.1954488871364393....\n",
      "Validation loss: 2.214133203612694....\n",
      "-----------------------------------\n",
      "Training loss: 2.1952659134548953....\n",
      "Validation loss: 2.2140585845606906....\n",
      "-----------------------------------\n",
      "Training loss: 2.195083100978782....\n",
      "Validation loss: 2.2139844204185684....\n",
      "-----------------------------------\n",
      "Training loss: 2.194900298978976....\n",
      "Validation loss: 2.2139095879185464....\n",
      "-----------------------------------\n",
      "Training loss: 2.1947175432726973....\n",
      "Validation loss: 2.213834750632541....\n",
      "-----------------------------------\n",
      "Training loss: 2.1945348501050645....\n",
      "Validation loss: 2.2137594855829885....\n",
      "-----------------------------------\n",
      "Training loss: 2.1943520159372696....\n",
      "Validation loss: 2.2136840000500575....\n",
      "-----------------------------------\n",
      "Training loss: 2.1941690778988963....\n",
      "Validation loss: 2.2136089951591003....\n",
      "-----------------------------------\n",
      "Training loss: 2.1939860894328675....\n",
      "Validation loss: 2.213532864749175....\n",
      "-----------------------------------\n",
      "Training loss: 2.1938032954650892....\n",
      "Validation loss: 2.2134575241722962....\n",
      "-----------------------------------\n",
      "Training loss: 2.193620479955087....\n",
      "Validation loss: 2.2133813000417693....\n",
      "-----------------------------------\n",
      "Training loss: 2.1934376949572063....\n",
      "Validation loss: 2.2133052919415364....\n",
      "-----------------------------------\n",
      "Training loss: 2.1932550398749497....\n",
      "Validation loss: 2.2132283765839613....\n",
      "-----------------------------------\n",
      "Training loss: 2.1930723694295824....\n",
      "Validation loss: 2.2131508642923863....\n",
      "-----------------------------------\n",
      "Training loss: 2.1928895512200093....\n",
      "Validation loss: 2.213074486389302....\n",
      "-----------------------------------\n",
      "Training loss: 2.1927065749376267....\n",
      "Validation loss: 2.2129971310446908....\n",
      "-----------------------------------\n",
      "Training loss: 2.19252384162646....\n",
      "Validation loss: 2.2129203331758407....\n",
      "-----------------------------------\n",
      "Training loss: 2.1923412020359008....\n",
      "Validation loss: 2.2128429645641665....\n",
      "-----------------------------------\n",
      "Training loss: 2.1921586637551704....\n",
      "Validation loss: 2.2127663405213527....\n",
      "-----------------------------------\n",
      "Training loss: 2.191976128155895....\n",
      "Validation loss: 2.2126895871870844....\n",
      "-----------------------------------\n",
      "Training loss: 2.1917935276795535....\n",
      "Validation loss: 2.212612956167572....\n",
      "-----------------------------------\n",
      "Training loss: 2.1916105579073513....\n",
      "Validation loss: 2.212536613305758....\n",
      "-----------------------------------\n",
      "Training loss: 2.1914276393577694....\n",
      "Validation loss: 2.212460040187154....\n",
      "-----------------------------------\n",
      "Training loss: 2.1912449287610283....\n",
      "Validation loss: 2.212383763001629....\n",
      "-----------------------------------\n",
      "Training loss: 2.1910622133169144....\n",
      "Validation loss: 2.2123080217656743....\n",
      "-----------------------------------\n",
      "Training loss: 2.1908793984556807....\n",
      "Validation loss: 2.2122323589893242....\n",
      "-----------------------------------\n",
      "Training loss: 2.1906966212834793....\n",
      "Validation loss: 2.2121560563045337....\n",
      "-----------------------------------\n",
      "Training loss: 2.190513625111664....\n",
      "Validation loss: 2.2120803797966637....\n",
      "-----------------------------------\n",
      "Training loss: 2.1903303652552624....\n",
      "Validation loss: 2.212004783207671....\n",
      "-----------------------------------\n",
      "Training loss: 2.190147028663865....\n",
      "Validation loss: 2.211928799942956....\n",
      "-----------------------------------\n",
      "Training loss: 2.189963617654503....\n",
      "Validation loss: 2.2118533523982222....\n",
      "-----------------------------------\n",
      "Training loss: 2.189780009793266....\n",
      "Validation loss: 2.211776812610329....\n",
      "-----------------------------------\n",
      "Training loss: 2.189595981040372....\n",
      "Validation loss: 2.211700786588891....\n",
      "-----------------------------------\n",
      "Training loss: 2.1894114540070797....\n",
      "Validation loss: 2.2116244242433805....\n",
      "-----------------------------------\n",
      "Training loss: 2.18922642380428....\n",
      "Validation loss: 2.211548776664604....\n",
      "-----------------------------------\n",
      "Training loss: 2.189041017313111....\n",
      "Validation loss: 2.2114725845124754....\n",
      "-----------------------------------\n",
      "Training loss: 2.1888556413783045....\n",
      "Validation loss: 2.211396581163428....\n",
      "-----------------------------------\n",
      "Training loss: 2.188670381927863....\n",
      "Validation loss: 2.2113200574478817....\n",
      "-----------------------------------\n",
      "Training loss: 2.18848520005369....\n",
      "Validation loss: 2.211244206199374....\n",
      "-----------------------------------\n",
      "Training loss: 2.1883001102478157....\n",
      "Validation loss: 2.21116819574643....\n",
      "-----------------------------------\n",
      "Training loss: 2.188114591093036....\n",
      "Validation loss: 2.2110924737025925....\n",
      "-----------------------------------\n",
      "Training loss: 2.18792875705436....\n",
      "Validation loss: 2.2110165835626536....\n",
      "-----------------------------------\n",
      "Training loss: 2.187742898315988....\n",
      "Validation loss: 2.210940613423599....\n",
      "-----------------------------------\n",
      "Training loss: 2.1875570783106726....\n",
      "Validation loss: 2.210864535471772....\n",
      "-----------------------------------\n",
      "Training loss: 2.187371162547627....\n",
      "Validation loss: 2.2107890520833777....\n",
      "-----------------------------------\n",
      "Training loss: 2.187184755706031....\n",
      "Validation loss: 2.2107131779909737....\n",
      "-----------------------------------\n",
      "Training loss: 2.1869978062356714....\n",
      "Validation loss: 2.2106375073623044....\n",
      "-----------------------------------\n",
      "Training loss: 2.1868111009319255....\n",
      "Validation loss: 2.210561843090519....\n",
      "-----------------------------------\n",
      "Training loss: 2.1866245633955725....\n",
      "Validation loss: 2.210485826568859....\n",
      "-----------------------------------\n",
      "Training loss: 2.1864376052032615....\n",
      "Validation loss: 2.2104100818261423....\n",
      "-----------------------------------\n",
      "Training loss: 2.186249978272295....\n",
      "Validation loss: 2.210334273996304....\n",
      "-----------------------------------\n",
      "Training loss: 2.1860621960165973....\n",
      "Validation loss: 2.2102589214921182....\n",
      "-----------------------------------\n",
      "Training loss: 2.1858743167941763....\n",
      "Validation loss: 2.2101839417681384....\n",
      "-----------------------------------\n",
      "Training loss: 2.185686425982478....\n",
      "Validation loss: 2.210108955945834....\n",
      "-----------------------------------\n",
      "Training loss: 2.1854986871562208....\n",
      "Validation loss: 2.2100336841772803....\n",
      "-----------------------------------\n",
      "Training loss: 2.1853111230858357....\n",
      "Validation loss: 2.2099584471040425....\n",
      "-----------------------------------\n",
      "Training loss: 2.1851236527802325....\n",
      "Validation loss: 2.209883601385649....\n",
      "-----------------------------------\n",
      "Training loss: 2.1849362284682816....\n",
      "Validation loss: 2.2098081033467345....\n",
      "-----------------------------------\n",
      "Training loss: 2.1847489615015836....\n",
      "Validation loss: 2.209732874600317....\n",
      "-----------------------------------\n",
      "Training loss: 2.1845615795147584....\n",
      "Validation loss: 2.2096575534774336....\n",
      "-----------------------------------\n",
      "Training loss: 2.18437357140105....\n",
      "Validation loss: 2.2095825945270127....\n",
      "-----------------------------------\n",
      "Training loss: 2.1841849469016137....\n",
      "Validation loss: 2.2095076467479626....\n",
      "-----------------------------------\n",
      "Training loss: 2.1839960326044503....\n",
      "Validation loss: 2.209432397469607....\n",
      "-----------------------------------\n",
      "Training loss: 2.1838070814994737....\n",
      "Validation loss: 2.20935657487861....\n",
      "-----------------------------------\n",
      "Training loss: 2.183618278547902....\n",
      "Validation loss: 2.2092816881485424....\n",
      "-----------------------------------\n",
      "Training loss: 2.1834295536437143....\n",
      "Validation loss: 2.2092060682549466....\n",
      "-----------------------------------\n",
      "Training loss: 2.183240783296499....\n",
      "Validation loss: 2.2091303557355033....\n",
      "-----------------------------------\n",
      "Training loss: 2.183052189137197....\n",
      "Validation loss: 2.2090546216576454....\n",
      "-----------------------------------\n",
      "Training loss: 2.1828635818815147....\n",
      "Validation loss: 2.2089793369782074....\n",
      "-----------------------------------\n",
      "Training loss: 2.182674616433319....\n",
      "Validation loss: 2.2089032068900223....\n",
      "-----------------------------------\n",
      "Training loss: 2.1824855108234247....\n",
      "Validation loss: 2.2088271070861074....\n",
      "-----------------------------------\n",
      "Training loss: 2.1822963086359453....\n",
      "Validation loss: 2.208750621713271....\n",
      "-----------------------------------\n",
      "Training loss: 2.1821067624789947....\n",
      "Validation loss: 2.208674178541633....\n",
      "-----------------------------------\n",
      "Training loss: 2.181916644248633....\n",
      "Validation loss: 2.2085979691211586....\n",
      "-----------------------------------\n",
      "Training loss: 2.181726468156295....\n",
      "Validation loss: 2.2085212845327526....\n",
      "-----------------------------------\n",
      "Training loss: 2.181536222960531....\n",
      "Validation loss: 2.208444027395748....\n",
      "-----------------------------------\n",
      "Training loss: 2.1813455287509176....\n",
      "Validation loss: 2.2083668143364896....\n",
      "-----------------------------------\n",
      "Training loss: 2.181154524679802....\n",
      "Validation loss: 2.208290002138547....\n",
      "-----------------------------------\n",
      "Training loss: 2.1809630365823045....\n",
      "Validation loss: 2.2082133395208303....\n",
      "-----------------------------------\n",
      "Training loss: 2.18077151594182....\n",
      "Validation loss: 2.2081371231544007....\n",
      "-----------------------------------\n",
      "Training loss: 2.180580175772937....\n",
      "Validation loss: 2.2080610518763533....\n",
      "-----------------------------------\n",
      "Training loss: 2.1803885506288436....\n",
      "Validation loss: 2.2079846781533945....\n",
      "-----------------------------------\n",
      "Training loss: 2.180196255134665....\n",
      "Validation loss: 2.207908800733646....\n",
      "-----------------------------------\n",
      "Training loss: 2.1800039286680644....\n",
      "Validation loss: 2.2078312854432136....\n",
      "-----------------------------------\n",
      "Training loss: 2.179811461853825....\n",
      "Validation loss: 2.207754137168902....\n",
      "-----------------------------------\n",
      "Training loss: 2.17961870662671....\n",
      "Validation loss: 2.2076765767028466....\n",
      "-----------------------------------\n",
      "Training loss: 2.179425779075067....\n",
      "Validation loss: 2.207598704783424....\n",
      "-----------------------------------\n",
      "Training loss: 2.1792326701079787....\n",
      "Validation loss: 2.2075216970230893....\n",
      "-----------------------------------\n",
      "Training loss: 2.179039773564594....\n",
      "Validation loss: 2.2074438869914452....\n",
      "-----------------------------------\n",
      "Training loss: 2.1788471003496266....\n",
      "Validation loss: 2.2073661488494456....\n",
      "-----------------------------------\n",
      "Training loss: 2.178654304351865....\n",
      "Validation loss: 2.207288723999555....\n",
      "-----------------------------------\n",
      "Training loss: 2.1784620698852772....\n",
      "Validation loss: 2.2072111946449775....\n",
      "-----------------------------------\n",
      "Training loss: 2.1782700827006773....\n",
      "Validation loss: 2.207133288968694....\n",
      "-----------------------------------\n",
      "Training loss: 2.1780781710556205....\n",
      "Validation loss: 2.207054905483414....\n",
      "-----------------------------------\n",
      "Training loss: 2.1778865179009927....\n",
      "Validation loss: 2.2069756958566797....\n",
      "-----------------------------------\n",
      "Training loss: 2.177694504794396....\n",
      "Validation loss: 2.206896741697965....\n",
      "-----------------------------------\n",
      "Training loss: 2.177502324487789....\n",
      "Validation loss: 2.206818031464083....\n",
      "-----------------------------------\n",
      "Training loss: 2.177310275653566....\n",
      "Validation loss: 2.2067391810481785....\n",
      "-----------------------------------\n",
      "Training loss: 2.1771183913807786....\n",
      "Validation loss: 2.2066605350913115....\n",
      "-----------------------------------\n",
      "Training loss: 2.176926471141916....\n",
      "Validation loss: 2.2065816800290423....\n",
      "-----------------------------------\n",
      "Training loss: 2.176733752901667....\n",
      "Validation loss: 2.20650295134117....\n",
      "-----------------------------------\n",
      "Training loss: 2.176541053811548....\n",
      "Validation loss: 2.2064247369730645....\n",
      "-----------------------------------\n",
      "Training loss: 2.1763483935612142....\n",
      "Validation loss: 2.2063468178573404....\n",
      "-----------------------------------\n",
      "Training loss: 2.176155253742134....\n",
      "Validation loss: 2.206269039442957....\n",
      "-----------------------------------\n",
      "Training loss: 2.175962232532771....\n",
      "Validation loss: 2.2061908032323925....\n",
      "-----------------------------------\n",
      "Training loss: 2.1757690423549105....\n",
      "Validation loss: 2.2061129659800165....\n",
      "-----------------------------------\n",
      "Training loss: 2.1755758486233554....\n",
      "Validation loss: 2.206035003673892....\n",
      "-----------------------------------\n",
      "Training loss: 2.175382428753539....\n",
      "Validation loss: 2.205956420734183....\n",
      "-----------------------------------\n",
      "Training loss: 2.175188805303192....\n",
      "Validation loss: 2.205877627601779....\n",
      "-----------------------------------\n",
      "Training loss: 2.174994951271762....\n",
      "Validation loss: 2.2057992167680016....\n",
      "-----------------------------------\n",
      "Training loss: 2.1748006264282926....\n",
      "Validation loss: 2.2057207836578363....\n",
      "-----------------------------------\n",
      "Training loss: 2.174606298960627....\n",
      "Validation loss: 2.2056423776972727....\n",
      "-----------------------------------\n",
      "Training loss: 2.174411524503995....\n",
      "Validation loss: 2.2055644295087977....\n",
      "-----------------------------------\n",
      "Training loss: 2.1742165913546403....\n",
      "Validation loss: 2.205486027675063....\n",
      "-----------------------------------\n",
      "Training loss: 2.174021629078696....\n",
      "Validation loss: 2.2054082619137696....\n",
      "-----------------------------------\n",
      "Training loss: 2.1738258803057198....\n",
      "Validation loss: 2.205330616087636....\n",
      "-----------------------------------\n",
      "Training loss: 2.173630083067467....\n",
      "Validation loss: 2.205253457975905....\n",
      "-----------------------------------\n",
      "Training loss: 2.1734341568998277....\n",
      "Validation loss: 2.2051754189951667....\n",
      "-----------------------------------\n",
      "Training loss: 2.1732377914879044....\n",
      "Validation loss: 2.2050982878498964....\n",
      "-----------------------------------\n",
      "Training loss: 2.173040636088954....\n",
      "Validation loss: 2.2050198933172718....\n",
      "-----------------------------------\n",
      "Training loss: 2.172842885629577....\n",
      "Validation loss: 2.204941956413033....\n",
      "-----------------------------------\n",
      "Training loss: 2.1726449290311565....\n",
      "Validation loss: 2.2048632487163045....\n",
      "-----------------------------------\n",
      "Training loss: 2.172446899610863....\n",
      "Validation loss: 2.204785669876486....\n",
      "-----------------------------------\n",
      "Training loss: 2.1722476876145955....\n",
      "Validation loss: 2.2047071533634846....\n",
      "-----------------------------------\n",
      "Training loss: 2.1720477975040997....\n",
      "Validation loss: 2.2046289087791755....\n",
      "-----------------------------------\n",
      "Training loss: 2.1718473024690965....\n",
      "Validation loss: 2.2045503998912093....\n",
      "-----------------------------------\n",
      "Training loss: 2.171646531722205....\n",
      "Validation loss: 2.2044713693939375....\n",
      "-----------------------------------\n",
      "Training loss: 2.171445583188431....\n",
      "Validation loss: 2.2043918934271303....\n",
      "-----------------------------------\n",
      "Training loss: 2.171244593798255....\n",
      "Validation loss: 2.2043132519033533....\n",
      "-----------------------------------\n",
      "Training loss: 2.1710437080780074....\n",
      "Validation loss: 2.2042345048710232....\n",
      "-----------------------------------\n",
      "Training loss: 2.1708424283109844....\n",
      "Validation loss: 2.2041563541626155....\n",
      "-----------------------------------\n",
      "Training loss: 2.1706411182714715....\n",
      "Validation loss: 2.204077553768295....\n",
      "-----------------------------------\n",
      "Training loss: 2.170439543396623....\n",
      "Validation loss: 2.203998375115366....\n",
      "-----------------------------------\n",
      "Training loss: 2.170237606616717....\n",
      "Validation loss: 2.2039188239799765....\n",
      "-----------------------------------\n",
      "Training loss: 2.170035313124543....\n",
      "Validation loss: 2.2038399868060514....\n",
      "-----------------------------------\n",
      "Training loss: 2.1698331960461266....\n",
      "Validation loss: 2.20376042361035....\n",
      "-----------------------------------\n",
      "Training loss: 2.1696313870169295....\n",
      "Validation loss: 2.2036822145396027....\n",
      "-----------------------------------\n",
      "Training loss: 2.169429687728419....\n",
      "Validation loss: 2.2036034117963217....\n",
      "-----------------------------------\n",
      "Training loss: 2.1692279812857143....\n",
      "Validation loss: 2.2035242875538326....\n",
      "-----------------------------------\n",
      "Training loss: 2.1690259764166813....\n",
      "Validation loss: 2.203445942896412....\n",
      "-----------------------------------\n",
      "Training loss: 2.1688238160972357....\n",
      "Validation loss: 2.2033665537817386....\n",
      "-----------------------------------\n",
      "Training loss: 2.168621684133754....\n",
      "Validation loss: 2.2032870928300325....\n",
      "-----------------------------------\n",
      "Training loss: 2.168419817793175....\n",
      "Validation loss: 2.2032070026743744....\n",
      "-----------------------------------\n",
      "Training loss: 2.1682184285619477....\n",
      "Validation loss: 2.2031260965652266....\n",
      "-----------------------------------\n",
      "Training loss: 2.1680172392951103....\n",
      "Validation loss: 2.2030457478751244....\n",
      "-----------------------------------\n",
      "Training loss: 2.167816255583536....\n",
      "Validation loss: 2.2029643570086....\n",
      "-----------------------------------\n",
      "Training loss: 2.16761543494639....\n",
      "Validation loss: 2.202883796662681....\n",
      "-----------------------------------\n",
      "Training loss: 2.1674143547689435....\n",
      "Validation loss: 2.202803288901197....\n",
      "-----------------------------------\n",
      "Training loss: 2.1672131507543466....\n",
      "Validation loss: 2.202722590667069....\n",
      "-----------------------------------\n",
      "Training loss: 2.1670119803511634....\n",
      "Validation loss: 2.202641293685806....\n",
      "-----------------------------------\n",
      "Training loss: 2.1668103865018167....\n",
      "Validation loss: 2.202559423590919....\n",
      "-----------------------------------\n",
      "Training loss: 2.1666088757322166....\n",
      "Validation loss: 2.202478575641276....\n",
      "-----------------------------------\n",
      "Training loss: 2.1664069723691153....\n",
      "Validation loss: 2.202398437593217....\n",
      "-----------------------------------\n",
      "Training loss: 2.166205084130273....\n",
      "Validation loss: 2.202318255326614....\n",
      "-----------------------------------\n",
      "Training loss: 2.1660029013425364....\n",
      "Validation loss: 2.2022391411799958....\n",
      "-----------------------------------\n",
      "Training loss: 2.165800436721866....\n",
      "Validation loss: 2.202159490494444....\n",
      "-----------------------------------\n",
      "Training loss: 2.165598421624587....\n",
      "Validation loss: 2.202080651463613....\n",
      "-----------------------------------\n",
      "Training loss: 2.165396731247466....\n",
      "Validation loss: 2.2020013174471464....\n",
      "-----------------------------------\n",
      "Training loss: 2.1651956251606888....\n",
      "Validation loss: 2.2019221866067826....\n",
      "-----------------------------------\n",
      "Training loss: 2.164994817377937....\n",
      "Validation loss: 2.201842760777167....\n",
      "-----------------------------------\n",
      "Training loss: 2.1647937565701483....\n",
      "Validation loss: 2.2017633186846997....\n",
      "-----------------------------------\n",
      "Training loss: 2.1645922646164824....\n",
      "Validation loss: 2.201683838370625....\n",
      "-----------------------------------\n",
      "Training loss: 2.1643907151594544....\n",
      "Validation loss: 2.201603066489859....\n",
      "-----------------------------------\n",
      "Training loss: 2.164188918858167....\n",
      "Validation loss: 2.2015227546971574....\n",
      "-----------------------------------\n",
      "Training loss: 2.163987121162661....\n",
      "Validation loss: 2.201441482672499....\n",
      "-----------------------------------\n",
      "Training loss: 2.1637854034561115....\n",
      "Validation loss: 2.2013601928088318....\n",
      "-----------------------------------\n",
      "Training loss: 2.1635838250217128....\n",
      "Validation loss: 2.2012792885498165....\n",
      "-----------------------------------\n",
      "Training loss: 2.163382187677981....\n",
      "Validation loss: 2.201198340536154....\n",
      "-----------------------------------\n",
      "Training loss: 2.1631803353233168....\n",
      "Validation loss: 2.2011178101213282....\n",
      "-----------------------------------\n",
      "Training loss: 2.1629786182497543....\n",
      "Validation loss: 2.2010371389161043....\n",
      "-----------------------------------\n",
      "Training loss: 2.1627768974861157....\n",
      "Validation loss: 2.200956051570338....\n",
      "-----------------------------------\n",
      "Training loss: 2.1625750140468516....\n",
      "Validation loss: 2.2008755090522065....\n",
      "-----------------------------------\n",
      "Training loss: 2.1623728870420944....\n",
      "Validation loss: 2.2007948090649627....\n",
      "-----------------------------------\n",
      "Training loss: 2.1621707925290745....\n",
      "Validation loss: 2.2007141302135547....\n",
      "-----------------------------------\n",
      "Training loss: 2.16196886518823....\n",
      "Validation loss: 2.200633460797078....\n",
      "-----------------------------------\n",
      "Training loss: 2.161766777958364....\n",
      "Validation loss: 2.2005525284611034....\n",
      "-----------------------------------\n",
      "Training loss: 2.1615645748121204....\n",
      "Validation loss: 2.2004711403053516....\n",
      "-----------------------------------\n",
      "Training loss: 2.1613624430473735....\n",
      "Validation loss: 2.2003904574073316....\n",
      "-----------------------------------\n",
      "Training loss: 2.1611600495549865....\n",
      "Validation loss: 2.2003096785515166....\n",
      "-----------------------------------\n",
      "Training loss: 2.1609577651969976....\n",
      "Validation loss: 2.2002283671041196....\n",
      "-----------------------------------\n",
      "Training loss: 2.160755724009132....\n",
      "Validation loss: 2.2001462759647965....\n",
      "-----------------------------------\n",
      "Training loss: 2.1605543624243246....\n",
      "Validation loss: 2.200063909361385....\n",
      "-----------------------------------\n",
      "Training loss: 2.1603530982344163....\n",
      "Validation loss: 2.1999817174267946....\n",
      "-----------------------------------\n",
      "Training loss: 2.1601516682413693....\n",
      "Validation loss: 2.1999000455719937....\n",
      "-----------------------------------\n",
      "Training loss: 2.159950181783444....\n",
      "Validation loss: 2.1998169585300347....\n",
      "-----------------------------------\n",
      "Training loss: 2.1597483852714907....\n",
      "Validation loss: 2.1997344415644013....\n",
      "-----------------------------------\n",
      "Training loss: 2.1595468215848515....\n",
      "Validation loss: 2.199651709291693....\n",
      "-----------------------------------\n",
      "Training loss: 2.1593455661467846....\n",
      "Validation loss: 2.1995694012777856....\n",
      "-----------------------------------\n",
      "Training loss: 2.159144402920091....\n",
      "Validation loss: 2.1994872898859295....\n",
      "-----------------------------------\n",
      "Training loss: 2.1589432132307294....\n",
      "Validation loss: 2.1994049515137193....\n",
      "-----------------------------------\n",
      "Training loss: 2.1587419586686423....\n",
      "Validation loss: 2.1993218883957417....\n",
      "-----------------------------------\n",
      "Training loss: 2.158540701678644....\n",
      "Validation loss: 2.199238886835071....\n",
      "-----------------------------------\n",
      "Training loss: 2.1583391808088273....\n",
      "Validation loss: 2.1991564687628036....\n",
      "-----------------------------------\n",
      "Training loss: 2.158137695844185....\n",
      "Validation loss: 2.199072695064345....\n",
      "-----------------------------------\n",
      "Training loss: 2.157936278616882....\n",
      "Validation loss: 2.198988068515889....\n",
      "-----------------------------------\n",
      "Training loss: 2.1577347129122537....\n",
      "Validation loss: 2.19890274104347....\n",
      "-----------------------------------\n",
      "Training loss: 2.157532985979109....\n",
      "Validation loss: 2.1988175598705038....\n",
      "-----------------------------------\n",
      "Training loss: 2.1573311914223985....\n",
      "Validation loss: 2.1987319831792798....\n",
      "-----------------------------------\n",
      "Training loss: 2.157129311653879....\n",
      "Validation loss: 2.1986462981462025....\n",
      "-----------------------------------\n",
      "Training loss: 2.156927400889137....\n",
      "Validation loss: 2.1985611633013726....\n",
      "-----------------------------------\n",
      "Training loss: 2.15672540847721....\n",
      "Validation loss: 2.198475628009928....\n",
      "-----------------------------------\n",
      "Training loss: 2.1565233939126216....\n",
      "Validation loss: 2.1983909737131095....\n",
      "-----------------------------------\n",
      "Training loss: 2.156321224571097....\n",
      "Validation loss: 2.1983055029809955....\n",
      "-----------------------------------\n",
      "Training loss: 2.156119294249484....\n",
      "Validation loss: 2.198220496653494....\n",
      "-----------------------------------\n",
      "Training loss: 2.1559172386453005....\n",
      "Validation loss: 2.198135583171832....\n",
      "-----------------------------------\n",
      "Training loss: 2.1557153859214977....\n",
      "Validation loss: 2.1980506328169076....\n",
      "-----------------------------------\n",
      "Training loss: 2.155513923319649....\n",
      "Validation loss: 2.197966154046366....\n",
      "-----------------------------------\n",
      "Training loss: 2.1553122298225085....\n",
      "Validation loss: 2.1978811362528696....\n",
      "-----------------------------------\n",
      "Training loss: 2.155110221220801....\n",
      "Validation loss: 2.197796648340207....\n",
      "-----------------------------------\n",
      "Training loss: 2.154907896995336....\n",
      "Validation loss: 2.197712183468373....\n",
      "-----------------------------------\n",
      "Training loss: 2.1547057941077434....\n",
      "Validation loss: 2.197627316084976....\n",
      "-----------------------------------\n",
      "Training loss: 2.154504161941719....\n",
      "Validation loss: 2.197542405912224....\n",
      "-----------------------------------\n",
      "Training loss: 2.1543025052607088....\n",
      "Validation loss: 2.197457887084793....\n",
      "-----------------------------------\n",
      "Training loss: 2.154100457744313....\n",
      "Validation loss: 2.197373150519728....\n",
      "-----------------------------------\n",
      "Training loss: 2.1538981909371144....\n",
      "Validation loss: 2.1972883781057737....\n",
      "-----------------------------------\n",
      "Training loss: 2.1536961911269064....\n",
      "Validation loss: 2.1972035859199552....\n",
      "-----------------------------------\n",
      "Training loss: 2.1534944463084678....\n",
      "Validation loss: 2.197118563610573....\n",
      "-----------------------------------\n",
      "Training loss: 2.153292918602115....\n",
      "Validation loss: 2.1970332362737173....\n",
      "-----------------------------------\n",
      "Training loss: 2.153091815687744....\n",
      "Validation loss: 2.196947060701751....\n",
      "-----------------------------------\n",
      "Training loss: 2.1528903514221605....\n",
      "Validation loss: 2.196861564712863....\n",
      "-----------------------------------\n",
      "Training loss: 2.1526887034400497....\n",
      "Validation loss: 2.196776131556082....\n",
      "-----------------------------------\n",
      "Training loss: 2.152487045780474....\n",
      "Validation loss: 2.19669042051396....\n",
      "-----------------------------------\n",
      "Training loss: 2.152285644667446....\n",
      "Validation loss: 2.196604722911147....\n",
      "-----------------------------------\n",
      "Training loss: 2.152083987069655....\n",
      "Validation loss: 2.196518326189412....\n",
      "-----------------------------------\n",
      "Training loss: 2.151882240609478....\n",
      "Validation loss: 2.196432011362463....\n",
      "-----------------------------------\n",
      "Training loss: 2.1516803930248045....\n",
      "Validation loss: 2.1963460416350946....\n",
      "-----------------------------------\n",
      "Training loss: 2.151478730310024....\n",
      "Validation loss: 2.196259614573208....\n",
      "-----------------------------------\n",
      "Training loss: 2.151276992847146....\n",
      "Validation loss: 2.1961732151418767....\n",
      "-----------------------------------\n",
      "Training loss: 2.151075190592621....\n",
      "Validation loss: 2.1960872348397293....\n",
      "-----------------------------------\n",
      "Training loss: 2.1508732664107693....\n",
      "Validation loss: 2.1960012080277105....\n",
      "-----------------------------------\n",
      "Training loss: 2.150671099363755....\n",
      "Validation loss: 2.1959149235363213....\n",
      "-----------------------------------\n",
      "Training loss: 2.1504684505537854....\n",
      "Validation loss: 2.1958283627927906....\n",
      "-----------------------------------\n",
      "Training loss: 2.1502658749477916....\n",
      "Validation loss: 2.1957414238257544....\n",
      "-----------------------------------\n",
      "Training loss: 2.1500634410928403....\n",
      "Validation loss: 2.1956537962529064....\n",
      "-----------------------------------\n",
      "Training loss: 2.149860880986896....\n",
      "Validation loss: 2.1955662607161845....\n",
      "-----------------------------------\n",
      "Training loss: 2.149658161554114....\n",
      "Validation loss: 2.195478136807558....\n",
      "-----------------------------------\n",
      "Training loss: 2.1494550499443013....\n",
      "Validation loss: 2.1953911657014524....\n",
      "-----------------------------------\n",
      "Training loss: 2.1492513979143273....\n",
      "Validation loss: 2.1953040564425788....\n",
      "-----------------------------------\n",
      "Training loss: 2.1490477933476098....\n",
      "Validation loss: 2.1952167079936347....\n",
      "-----------------------------------\n",
      "Training loss: 2.1488438596614983....\n",
      "Validation loss: 2.195129118478826....\n",
      "-----------------------------------\n",
      "Training loss: 2.1486400535982515....\n",
      "Validation loss: 2.195041207128097....\n",
      "-----------------------------------\n",
      "Training loss: 2.1484363233500035....\n",
      "Validation loss: 2.1949525179775313....\n",
      "-----------------------------------\n",
      "Training loss: 2.1482329586154827....\n",
      "Validation loss: 2.1948642302288484....\n",
      "-----------------------------------\n",
      "Training loss: 2.148029536104737....\n",
      "Validation loss: 2.194774896089316....\n",
      "-----------------------------------\n",
      "Training loss: 2.1478258299257416....\n",
      "Validation loss: 2.1946861728803606....\n",
      "-----------------------------------\n",
      "Training loss: 2.1476223132760195....\n",
      "Validation loss: 2.1945967920134457....\n",
      "-----------------------------------\n",
      "Training loss: 2.147418966835292....\n",
      "Validation loss: 2.1945081712380734....\n",
      "-----------------------------------\n",
      "Training loss: 2.147215324676134....\n",
      "Validation loss: 2.1944186898305102....\n",
      "-----------------------------------\n",
      "Training loss: 2.147011296936261....\n",
      "Validation loss: 2.1943289643741957....\n",
      "-----------------------------------\n",
      "Training loss: 2.1468067498058163....\n",
      "Validation loss: 2.194239175190143....\n",
      "-----------------------------------\n",
      "Training loss: 2.146601336468796....\n",
      "Validation loss: 2.1941498335065432....\n",
      "-----------------------------------\n",
      "Training loss: 2.146395395474998....\n",
      "Validation loss: 2.194059642656277....\n",
      "-----------------------------------\n",
      "Training loss: 2.1461893301654547....\n",
      "Validation loss: 2.1939707208952135....\n",
      "-----------------------------------\n",
      "Training loss: 2.145982802863828....\n",
      "Validation loss: 2.1938810989244995....\n",
      "-----------------------------------\n",
      "Training loss: 2.1457759162481254....\n",
      "Validation loss: 2.193791982104455....\n",
      "-----------------------------------\n",
      "Training loss: 2.1455689856424764....\n",
      "Validation loss: 2.1937016456170557....\n",
      "-----------------------------------\n",
      "Training loss: 2.14536150178348....\n",
      "Validation loss: 2.1936120680122757....\n",
      "-----------------------------------\n",
      "Training loss: 2.145153875564195....\n",
      "Validation loss: 2.193521107126325....\n",
      "-----------------------------------\n",
      "Training loss: 2.1449464314592865....\n",
      "Validation loss: 2.1934310270674744....\n",
      "-----------------------------------\n",
      "Training loss: 2.1447390173162915....\n",
      "Validation loss: 2.19334002427133....\n",
      "-----------------------------------\n",
      "Training loss: 2.144531732789754....\n",
      "Validation loss: 2.1932489758241918....\n",
      "-----------------------------------\n",
      "Training loss: 2.1443245670252153....\n",
      "Validation loss: 2.1931585265830003....\n",
      "-----------------------------------\n",
      "Training loss: 2.1441170856524128....\n",
      "Validation loss: 2.193068053568481....\n",
      "-----------------------------------\n",
      "Training loss: 2.1439090212678034....\n",
      "Validation loss: 2.1929772097334395....\n",
      "-----------------------------------\n",
      "Training loss: 2.1437010943573536....\n",
      "Validation loss: 2.1928872085616873....\n",
      "-----------------------------------\n",
      "Training loss: 2.1434931969051014....\n",
      "Validation loss: 2.1927965739474065....\n",
      "-----------------------------------\n",
      "Training loss: 2.143285195457187....\n",
      "Validation loss: 2.1927058427035946....\n",
      "-----------------------------------\n",
      "Training loss: 2.1430772540796843....\n",
      "Validation loss: 2.192614682006877....\n",
      "-----------------------------------\n",
      "Training loss: 2.1428693669258365....\n",
      "Validation loss: 2.1925241309736783....\n",
      "-----------------------------------\n",
      "Training loss: 2.142661992815329....\n",
      "Validation loss: 2.192431296894969....\n",
      "-----------------------------------\n",
      "Training loss: 2.142454919111273....\n",
      "Validation loss: 2.192339570426352....\n",
      "-----------------------------------\n",
      "Training loss: 2.1422479316732392....\n",
      "Validation loss: 2.192246570427951....\n",
      "-----------------------------------\n",
      "Training loss: 2.1420409098866466....\n",
      "Validation loss: 2.192153719049187....\n",
      "-----------------------------------\n",
      "Training loss: 2.141833928889594....\n",
      "Validation loss: 2.192060951098984....\n",
      "-----------------------------------\n",
      "Training loss: 2.1416271241106646....\n",
      "Validation loss: 2.1919678650425256....\n",
      "-----------------------------------\n",
      "Training loss: 2.14142051308288....\n",
      "Validation loss: 2.1918747279253927....\n",
      "-----------------------------------\n",
      "Training loss: 2.141214168317458....\n",
      "Validation loss: 2.1917815138810455....\n",
      "-----------------------------------\n",
      "Training loss: 2.141008079123112....\n",
      "Validation loss: 2.191687735868369....\n",
      "-----------------------------------\n",
      "Training loss: 2.140801777756253....\n",
      "Validation loss: 2.1915931790796153....\n",
      "-----------------------------------\n",
      "Training loss: 2.1405955185543064....\n",
      "Validation loss: 2.191499116155044....\n",
      "-----------------------------------\n",
      "Training loss: 2.140388877601515....\n",
      "Validation loss: 2.1914046709961554....\n",
      "-----------------------------------\n",
      "Training loss: 2.140182295188898....\n",
      "Validation loss: 2.1913094612528887....\n",
      "-----------------------------------\n",
      "Training loss: 2.1399758365772925....\n",
      "Validation loss: 2.191214678123266....\n",
      "-----------------------------------\n",
      "Training loss: 2.139769417859256....\n",
      "Validation loss: 2.1911192059150024....\n",
      "-----------------------------------\n",
      "Training loss: 2.139563285974396....\n",
      "Validation loss: 2.191023881183871....\n",
      "-----------------------------------\n",
      "Training loss: 2.139357068512257....\n",
      "Validation loss: 2.1909283999998754....\n",
      "-----------------------------------\n",
      "Training loss: 2.1391509396056767....\n",
      "Validation loss: 2.1908322686135464....\n",
      "-----------------------------------\n",
      "Training loss: 2.1389450126682013....\n",
      "Validation loss: 2.190737054497476....\n",
      "-----------------------------------\n",
      "Training loss: 2.1387395158690294....\n",
      "Validation loss: 2.190642376360283....\n",
      "-----------------------------------\n",
      "Training loss: 2.1385342624182258....\n",
      "Validation loss: 2.1905469339696957....\n",
      "-----------------------------------\n",
      "Training loss: 2.138328823118374....\n",
      "Validation loss: 2.190451882169769....\n",
      "-----------------------------------\n",
      "Training loss: 2.1381233364047043....\n",
      "Validation loss: 2.1903572104553897....\n",
      "-----------------------------------\n",
      "Training loss: 2.1379176098256947....\n",
      "Validation loss: 2.1902615248012824....\n",
      "-----------------------------------\n",
      "Training loss: 2.1377120237074947....\n",
      "Validation loss: 2.1901659903121864....\n",
      "-----------------------------------\n",
      "Training loss: 2.137506588361391....\n",
      "Validation loss: 2.1900705671436405....\n",
      "-----------------------------------\n",
      "Training loss: 2.137301025700409....\n",
      "Validation loss: 2.18997498141504....\n",
      "-----------------------------------\n",
      "Training loss: 2.1370955700963656....\n",
      "Validation loss: 2.189879173190543....\n",
      "-----------------------------------\n",
      "Training loss: 2.136890147294779....\n",
      "Validation loss: 2.189783754184355....\n",
      "-----------------------------------\n",
      "Training loss: 2.136684727820079....\n",
      "Validation loss: 2.18968788632601....\n",
      "-----------------------------------\n",
      "Training loss: 2.136479605622223....\n",
      "Validation loss: 2.1895923166733064....\n",
      "-----------------------------------\n",
      "Training loss: 2.1362746005084152....\n",
      "Validation loss: 2.1894961952991445....\n",
      "-----------------------------------\n",
      "Training loss: 2.1360697181014867....\n",
      "Validation loss: 2.1894005963585235....\n",
      "-----------------------------------\n",
      "Training loss: 2.135864898509682....\n",
      "Validation loss: 2.189304739790089....\n",
      "-----------------------------------\n",
      "Training loss: 2.1356602469302604....\n",
      "Validation loss: 2.1892101168017506....\n",
      "-----------------------------------\n",
      "Training loss: 2.135455641581829....\n",
      "Validation loss: 2.189114343172658....\n",
      "-----------------------------------\n",
      "Training loss: 2.1352508172502436....\n",
      "Validation loss: 2.189018599650659....\n",
      "-----------------------------------\n",
      "Training loss: 2.1350461192777086....\n",
      "Validation loss: 2.188923025005087....\n",
      "-----------------------------------\n",
      "Training loss: 2.134841426514534....\n",
      "Validation loss: 2.188826890920749....\n",
      "-----------------------------------\n",
      "Training loss: 2.134636934187146....\n",
      "Validation loss: 2.188730930066979....\n",
      "-----------------------------------\n",
      "Training loss: 2.134432461610807....\n",
      "Validation loss: 2.1886352929467523....\n",
      "-----------------------------------\n",
      "Training loss: 2.1342278484529835....\n",
      "Validation loss: 2.1885406654533908....\n",
      "-----------------------------------\n",
      "Training loss: 2.1340228316870253....\n",
      "Validation loss: 2.188444564507456....\n",
      "-----------------------------------\n",
      "Training loss: 2.133817912005594....\n",
      "Validation loss: 2.1883490928840663....\n",
      "-----------------------------------\n",
      "Training loss: 2.1336128953844655....\n",
      "Validation loss: 2.18825294906905....\n",
      "-----------------------------------\n",
      "Training loss: 2.1334080382074125....\n",
      "Validation loss: 2.188157739956853....\n",
      "-----------------------------------\n",
      "Training loss: 2.133203003483079....\n",
      "Validation loss: 2.1880618078135274....\n",
      "-----------------------------------\n",
      "Training loss: 2.132997815829458....\n",
      "Validation loss: 2.1879661864901396....\n",
      "-----------------------------------\n",
      "Training loss: 2.132792386470584....\n",
      "Validation loss: 2.187870244929451....\n",
      "-----------------------------------\n",
      "Training loss: 2.132586970021081....\n",
      "Validation loss: 2.187774968606057....\n",
      "-----------------------------------\n",
      "Training loss: 2.1323813516187924....\n",
      "Validation loss: 2.1876784647711864....\n",
      "-----------------------------------\n",
      "Training loss: 2.132175644530287....\n",
      "Validation loss: 2.187583412613019....\n",
      "-----------------------------------\n",
      "Training loss: 2.1757995430097483....\n",
      "Validation loss: 2.099832649525026....\n",
      "-----------------------------------\n",
      "Training loss: 2.175549826747838....\n",
      "Validation loss: 2.09975676809689....\n",
      "-----------------------------------\n",
      "Training loss: 2.1753027888237666....\n",
      "Validation loss: 2.0996813031753474....\n",
      "-----------------------------------\n",
      "Training loss: 2.1750574408186214....\n",
      "Validation loss: 2.0996061442272276....\n",
      "-----------------------------------\n",
      "Training loss: 2.174813276999815....\n",
      "Validation loss: 2.099530874253894....\n",
      "-----------------------------------\n",
      "Training loss: 2.174570051742403....\n",
      "Validation loss: 2.0994557344349367....\n",
      "-----------------------------------\n",
      "Training loss: 2.1743281071518408....\n",
      "Validation loss: 2.0993802791543916....\n",
      "-----------------------------------\n",
      "Training loss: 2.1740870990672176....\n",
      "Validation loss: 2.0993050218685267....\n",
      "-----------------------------------\n",
      "Training loss: 2.1738466778278043....\n",
      "Validation loss: 2.099229176676108....\n",
      "-----------------------------------\n",
      "Training loss: 2.173607013536218....\n",
      "Validation loss: 2.0991528170715776....\n",
      "-----------------------------------\n",
      "Training loss: 2.1733685040916253....\n",
      "Validation loss: 2.0990768935469704....\n",
      "-----------------------------------\n",
      "Training loss: 2.173130810049048....\n",
      "Validation loss: 2.099001223538432....\n",
      "-----------------------------------\n",
      "Training loss: 2.1728935056340064....\n",
      "Validation loss: 2.098925493698625....\n",
      "-----------------------------------\n",
      "Training loss: 2.172656650681589....\n",
      "Validation loss: 2.0988500100954934....\n",
      "-----------------------------------\n",
      "Training loss: 2.1724201813419515....\n",
      "Validation loss: 2.0987739883163066....\n",
      "-----------------------------------\n",
      "Training loss: 2.1721838530388977....\n",
      "Validation loss: 2.098697731598939....\n",
      "-----------------------------------\n",
      "Training loss: 2.1719476857645064....\n",
      "Validation loss: 2.098621861317174....\n",
      "-----------------------------------\n",
      "Training loss: 2.1717123496243422....\n",
      "Validation loss: 2.098546071506182....\n",
      "-----------------------------------\n",
      "Training loss: 2.171477384834986....\n",
      "Validation loss: 2.0984698025330295....\n",
      "-----------------------------------\n",
      "Training loss: 2.1712441593260285....\n",
      "Validation loss: 2.098393509807862....\n",
      "-----------------------------------\n",
      "Training loss: 2.1710114341483258....\n",
      "Validation loss: 2.0983167887486216....\n",
      "-----------------------------------\n",
      "Training loss: 2.1707789547001735....\n",
      "Validation loss: 2.0982397837524758....\n",
      "-----------------------------------\n",
      "Training loss: 2.1705468452638605....\n",
      "Validation loss: 2.098162442043272....\n",
      "-----------------------------------\n",
      "Training loss: 2.1703154072744555....\n",
      "Validation loss: 2.0980845557735903....\n",
      "-----------------------------------\n",
      "Training loss: 2.1700842417979977....\n",
      "Validation loss: 2.098006363513122....\n",
      "-----------------------------------\n",
      "Training loss: 2.1698522876124833....\n",
      "Validation loss: 2.0979286082415998....\n",
      "-----------------------------------\n",
      "Training loss: 2.1696201057233666....\n",
      "Validation loss: 2.097850210207726....\n",
      "-----------------------------------\n",
      "Training loss: 2.1693880566532973....\n",
      "Validation loss: 2.097771351670092....\n",
      "-----------------------------------\n",
      "Training loss: 2.169155590892038....\n",
      "Validation loss: 2.09769269776767....\n",
      "-----------------------------------\n",
      "Training loss: 2.1689232577899955....\n",
      "Validation loss: 2.0976147646783674....\n",
      "-----------------------------------\n",
      "Training loss: 2.168690558090488....\n",
      "Validation loss: 2.0975366731680443....\n",
      "-----------------------------------\n",
      "Training loss: 2.1684584527420934....\n",
      "Validation loss: 2.0974589625671256....\n",
      "-----------------------------------\n",
      "Training loss: 2.168227068427947....\n",
      "Validation loss: 2.097380969148343....\n",
      "-----------------------------------\n",
      "Training loss: 2.1679950929390177....\n",
      "Validation loss: 2.0973027184347552....\n",
      "-----------------------------------\n",
      "Training loss: 2.1677626440331914....\n",
      "Validation loss: 2.0972251178628274....\n",
      "-----------------------------------\n",
      "Training loss: 2.1675298503686564....\n",
      "Validation loss: 2.097147235946038....\n",
      "-----------------------------------\n",
      "Training loss: 2.167297259667812....\n",
      "Validation loss: 2.0970683244259747....\n",
      "-----------------------------------\n",
      "Training loss: 2.167065034348035....\n",
      "Validation loss: 2.096989281236861....\n",
      "-----------------------------------\n",
      "Training loss: 2.1668332507044044....\n",
      "Validation loss: 2.096910441760674....\n",
      "-----------------------------------\n",
      "Training loss: 2.166601185628785....\n",
      "Validation loss: 2.096830711082075....\n",
      "-----------------------------------\n",
      "Training loss: 2.1663690747701607....\n",
      "Validation loss: 2.0967509101917052....\n",
      "-----------------------------------\n",
      "Training loss: 2.16613746086564....\n",
      "Validation loss: 2.0966704734184067....\n",
      "-----------------------------------\n",
      "Training loss: 2.1659064885099926....\n",
      "Validation loss: 2.0965891975170736....\n",
      "-----------------------------------\n",
      "Training loss: 2.165675688393316....\n",
      "Validation loss: 2.0965077691540963....\n",
      "-----------------------------------\n",
      "Training loss: 2.165445756676053....\n",
      "Validation loss: 2.0964268368582215....\n",
      "-----------------------------------\n",
      "Training loss: 2.1652160254518793....\n",
      "Validation loss: 2.0963465606877567....\n",
      "-----------------------------------\n",
      "Training loss: 2.1649863199194694....\n",
      "Validation loss: 2.096266588027519....\n",
      "-----------------------------------\n",
      "Training loss: 2.164756877314382....\n",
      "Validation loss: 2.09618666710586....\n",
      "-----------------------------------\n",
      "Training loss: 2.164527825836134....\n",
      "Validation loss: 2.0961066085233404....\n",
      "-----------------------------------\n",
      "Training loss: 2.1642991060187105....\n",
      "Validation loss: 2.0960265444536073....\n",
      "-----------------------------------\n",
      "Training loss: 2.164070648085878....\n",
      "Validation loss: 2.0959460272548514....\n",
      "-----------------------------------\n",
      "Training loss: 2.163842162103532....\n",
      "Validation loss: 2.0958655133915802....\n",
      "-----------------------------------\n",
      "Training loss: 2.1636137340806907....\n",
      "Validation loss: 2.095784691703161....\n",
      "-----------------------------------\n",
      "Training loss: 2.1633853870654307....\n",
      "Validation loss: 2.0957041981273057....\n",
      "-----------------------------------\n",
      "Training loss: 2.163156649304625....\n",
      "Validation loss: 2.0956243502550773....\n",
      "-----------------------------------\n",
      "Training loss: 2.1629280219683045....\n",
      "Validation loss: 2.0955446993013447....\n",
      "-----------------------------------\n",
      "Training loss: 2.1626996341649725....\n",
      "Validation loss: 2.0954647515282354....\n",
      "-----------------------------------\n",
      "Training loss: 2.1624713956752535....\n",
      "Validation loss: 2.0953851764476634....\n",
      "-----------------------------------\n",
      "Training loss: 2.162243219342957....\n",
      "Validation loss: 2.0953056061526505....\n",
      "-----------------------------------\n",
      "Training loss: 2.162014504487148....\n",
      "Validation loss: 2.0952256807539977....\n",
      "-----------------------------------\n",
      "Training loss: 2.161785508732021....\n",
      "Validation loss: 2.0951462093401623....\n",
      "-----------------------------------\n",
      "Training loss: 2.161555705997871....\n",
      "Validation loss: 2.095066138294542....\n",
      "-----------------------------------\n",
      "Training loss: 2.1613250806462583....\n",
      "Validation loss: 2.094985729359178....\n",
      "-----------------------------------\n",
      "Training loss: 2.1610936478930163....\n",
      "Validation loss: 2.0949049953533443....\n",
      "-----------------------------------\n",
      "Training loss: 2.1608621868639064....\n",
      "Validation loss: 2.0948240559990943....\n",
      "-----------------------------------\n",
      "Training loss: 2.160630587164829....\n",
      "Validation loss: 2.0947440216579998....\n",
      "-----------------------------------\n",
      "Training loss: 2.1603977414236026....\n",
      "Validation loss: 2.0946637161527106....\n",
      "-----------------------------------\n",
      "Training loss: 2.1601645249140398....\n",
      "Validation loss: 2.0945826792336564....\n",
      "-----------------------------------\n",
      "Training loss: 2.159931357783651....\n",
      "Validation loss: 2.0945015666327653....\n",
      "-----------------------------------\n",
      "Training loss: 2.1596983784783....\n",
      "Validation loss: 2.094420659684431....\n",
      "-----------------------------------\n",
      "Training loss: 2.15946591738639....\n",
      "Validation loss: 2.0943393529076277....\n",
      "-----------------------------------\n",
      "Training loss: 2.159234193724924....\n",
      "Validation loss: 2.0942580272711275....\n",
      "-----------------------------------\n",
      "Training loss: 2.1590027242880896....\n",
      "Validation loss: 2.094176470985072....\n",
      "-----------------------------------\n",
      "Training loss: 2.1587712849030787....\n",
      "Validation loss: 2.094094323252064....\n",
      "-----------------------------------\n",
      "Training loss: 2.1585395556782525....\n",
      "Validation loss: 2.0940126940737835....\n",
      "-----------------------------------\n",
      "Training loss: 2.1583073567610827....\n",
      "Validation loss: 2.0939318170544166....\n",
      "-----------------------------------\n",
      "Training loss: 2.1580748135305283....\n",
      "Validation loss: 2.0938513051191747....\n",
      "-----------------------------------\n",
      "Training loss: 2.1578423965132028....\n",
      "Validation loss: 2.0937709463285743....\n",
      "-----------------------------------\n",
      "Training loss: 2.1576105624871262....\n",
      "Validation loss: 2.0936897573047277....\n",
      "-----------------------------------\n",
      "Training loss: 2.157378582757873....\n",
      "Validation loss: 2.09360714128087....\n",
      "-----------------------------------\n",
      "Training loss: 2.1571462845829066....\n",
      "Validation loss: 2.0935247254032987....\n",
      "-----------------------------------\n",
      "Training loss: 2.1569137771796667....\n",
      "Validation loss: 2.093442706232034....\n",
      "-----------------------------------\n",
      "Training loss: 2.156681322153062....\n",
      "Validation loss: 2.093360089952031....\n",
      "-----------------------------------\n",
      "Training loss: 2.156449361850019....\n",
      "Validation loss: 2.093277155585948....\n",
      "-----------------------------------\n",
      "Training loss: 2.1562174737991815....\n",
      "Validation loss: 2.0931942024646073....\n",
      "-----------------------------------\n",
      "Training loss: 2.1559854298743524....\n",
      "Validation loss: 2.093111671450094....\n",
      "-----------------------------------\n",
      "Training loss: 2.155753322483675....\n",
      "Validation loss: 2.0930289169310945....\n",
      "-----------------------------------\n",
      "Training loss: 2.1555209422687884....\n",
      "Validation loss: 2.092945951723858....\n",
      "-----------------------------------\n",
      "Training loss: 2.1552880514238097....\n",
      "Validation loss: 2.0928625964483283....\n",
      "-----------------------------------\n",
      "Training loss: 2.1550549550230493....\n",
      "Validation loss: 2.092779354037636....\n",
      "-----------------------------------\n",
      "Training loss: 2.15482184829153....\n",
      "Validation loss: 2.0926947657370896....\n",
      "-----------------------------------\n",
      "Training loss: 2.1545885033674006....\n",
      "Validation loss: 2.092610374621806....\n",
      "-----------------------------------\n",
      "Training loss: 2.1543552118466267....\n",
      "Validation loss: 2.092526841280834....\n",
      "-----------------------------------\n",
      "Training loss: 2.1541224896283824....\n",
      "Validation loss: 2.0924445851167612....\n",
      "-----------------------------------\n",
      "Training loss: 2.1538898048673554....\n",
      "Validation loss: 2.092361576317883....\n",
      "-----------------------------------\n",
      "Training loss: 2.153657599124179....\n",
      "Validation loss: 2.0922788795378953....\n",
      "-----------------------------------\n",
      "Training loss: 2.153425319665726....\n",
      "Validation loss: 2.0921955955628593....\n",
      "-----------------------------------\n",
      "Training loss: 2.1531940675787404....\n",
      "Validation loss: 2.092111170520008....\n",
      "-----------------------------------\n",
      "Training loss: 2.1529627455177422....\n",
      "Validation loss: 2.0920264921504086....\n",
      "-----------------------------------\n",
      "Training loss: 2.152730900225389....\n",
      "Validation loss: 2.091941809678615....\n",
      "-----------------------------------\n",
      "Training loss: 2.152499020944326....\n",
      "Validation loss: 2.0918570015104097....\n",
      "-----------------------------------\n",
      "Training loss: 2.1522667438334193....\n",
      "Validation loss: 2.0917720024212567....\n",
      "-----------------------------------\n",
      "Training loss: 2.15203405383377....\n",
      "Validation loss: 2.091686914230806....\n",
      "-----------------------------------\n",
      "Training loss: 2.1518016458488285....\n",
      "Validation loss: 2.0916018745271097....\n",
      "-----------------------------------\n",
      "Training loss: 2.1515686683061106....\n",
      "Validation loss: 2.0915172467849943....\n",
      "-----------------------------------\n",
      "Training loss: 2.1513352878218583....\n",
      "Validation loss: 2.091432403145205....\n",
      "-----------------------------------\n",
      "Training loss: 2.1511017853898537....\n",
      "Validation loss: 2.091348325891974....\n",
      "-----------------------------------\n",
      "Training loss: 2.150867699323848....\n",
      "Validation loss: 2.091264513221711....\n",
      "-----------------------------------\n",
      "Training loss: 2.150633382996007....\n",
      "Validation loss: 2.0911814868098086....\n",
      "-----------------------------------\n",
      "Training loss: 2.150399211147628....\n",
      "Validation loss: 2.091098644316687....\n",
      "-----------------------------------\n",
      "Training loss: 2.1501650113882125....\n",
      "Validation loss: 2.0910148221125637....\n",
      "-----------------------------------\n",
      "Training loss: 2.149931348157563....\n",
      "Validation loss: 2.090931650946358....\n",
      "-----------------------------------\n",
      "Training loss: 2.1496977271684674....\n",
      "Validation loss: 2.0908484748345346....\n",
      "-----------------------------------\n",
      "Training loss: 2.1494633758686397....\n",
      "Validation loss: 2.0907650504375987....\n",
      "-----------------------------------\n",
      "Training loss: 2.1492287365501053....\n",
      "Validation loss: 2.0906812406684483....\n",
      "-----------------------------------\n",
      "Training loss: 2.1489942371739907....\n",
      "Validation loss: 2.090597360635374....\n",
      "-----------------------------------\n",
      "Training loss: 2.1487601704270314....\n",
      "Validation loss: 2.090512844284832....\n",
      "-----------------------------------\n",
      "Training loss: 2.1485261388370085....\n",
      "Validation loss: 2.0904282952037856....\n",
      "-----------------------------------\n",
      "Training loss: 2.1482920623640016....\n",
      "Validation loss: 2.090344439076079....\n",
      "-----------------------------------\n",
      "Training loss: 2.148058109975416....\n",
      "Validation loss: 2.0902603885101856....\n",
      "-----------------------------------\n",
      "Training loss: 2.1478239279054288....\n",
      "Validation loss: 2.0901752141058356....\n",
      "-----------------------------------\n",
      "Training loss: 2.1475901662273014....\n",
      "Validation loss: 2.0900894292153933....\n",
      "-----------------------------------\n",
      "Training loss: 2.1473571408232575....\n",
      "Validation loss: 2.0900038495045603....\n",
      "-----------------------------------\n",
      "Training loss: 2.1471239332057723....\n",
      "Validation loss: 2.089919204316697....\n",
      "-----------------------------------\n",
      "Training loss: 2.1468907671345434....\n",
      "Validation loss: 2.089834295113705....\n",
      "-----------------------------------\n",
      "Training loss: 2.1466579293441423....\n",
      "Validation loss: 2.0897492249563006....\n",
      "-----------------------------------\n",
      "Training loss: 2.1464251520504214....\n",
      "Validation loss: 2.089664700450905....\n",
      "-----------------------------------\n",
      "Training loss: 2.1461923587730456....\n",
      "Validation loss: 2.0895794594854475....\n",
      "-----------------------------------\n",
      "Training loss: 2.14596019549552....\n",
      "Validation loss: 2.0894939447139795....\n",
      "-----------------------------------\n",
      "Training loss: 2.145728187539709....\n",
      "Validation loss: 2.089408597064073....\n",
      "-----------------------------------\n",
      "Training loss: 2.1454964566464914....\n",
      "Validation loss: 2.089322882355994....\n",
      "-----------------------------------\n",
      "Training loss: 2.145264843216887....\n",
      "Validation loss: 2.089237364788915....\n",
      "-----------------------------------\n",
      "Training loss: 2.1450332822841522....\n",
      "Validation loss: 2.0891527835186188....\n",
      "-----------------------------------\n",
      "Training loss: 2.1448012974904045....\n",
      "Validation loss: 2.0890685106472167....\n",
      "-----------------------------------\n",
      "Training loss: 2.1445698722692295....\n",
      "Validation loss: 2.08898357696775....\n",
      "-----------------------------------\n",
      "Training loss: 2.14433873232372....\n",
      "Validation loss: 2.0888986486203573....\n",
      "-----------------------------------\n",
      "Training loss: 2.1441075189618926....\n",
      "Validation loss: 2.088813676454529....\n",
      "-----------------------------------\n",
      "Training loss: 2.143875891163867....\n",
      "Validation loss: 2.08872804448545....\n",
      "-----------------------------------\n",
      "Training loss: 2.143644559471459....\n",
      "Validation loss: 2.0886422861827962....\n",
      "-----------------------------------\n",
      "Training loss: 2.1434132046478083....\n",
      "Validation loss: 2.088556444462172....\n",
      "-----------------------------------\n",
      "Training loss: 2.1431817382130585....\n",
      "Validation loss: 2.0884703761163323....\n",
      "-----------------------------------\n",
      "Training loss: 2.1429504872445833....\n",
      "Validation loss: 2.088384267844898....\n",
      "-----------------------------------\n",
      "Training loss: 2.1427191135455432....\n",
      "Validation loss: 2.088298010637441....\n",
      "-----------------------------------\n",
      "Training loss: 2.142487884884268....\n",
      "Validation loss: 2.0882118806696535....\n",
      "-----------------------------------\n",
      "Training loss: 2.1422566986226483....\n",
      "Validation loss: 2.0881262427536273....\n",
      "-----------------------------------\n",
      "Training loss: 2.142025200084624....\n",
      "Validation loss: 2.0880406643330827....\n",
      "-----------------------------------\n",
      "Training loss: 2.141793481324873....\n",
      "Validation loss: 2.087954969463265....\n",
      "-----------------------------------\n",
      "Training loss: 2.141561481754776....\n",
      "Validation loss: 2.0878695975937416....\n",
      "-----------------------------------\n",
      "Training loss: 2.1413293069365626....\n",
      "Validation loss: 2.0877847068279487....\n",
      "-----------------------------------\n",
      "Training loss: 2.141096657559205....\n",
      "Validation loss: 2.087699570449906....\n",
      "-----------------------------------\n",
      "Training loss: 2.1408640176984837....\n",
      "Validation loss: 2.087614841508836....\n",
      "-----------------------------------\n",
      "Training loss: 2.1406314105296618....\n",
      "Validation loss: 2.0875306144462393....\n",
      "-----------------------------------\n",
      "Training loss: 2.1403978799147736....\n",
      "Validation loss: 2.087447527380347....\n",
      "-----------------------------------\n",
      "Training loss: 2.140164491360044....\n",
      "Validation loss: 2.0873643669980138....\n",
      "-----------------------------------\n",
      "Training loss: 2.139930986331624....\n",
      "Validation loss: 2.087281100185603....\n",
      "-----------------------------------\n",
      "Training loss: 2.1396976513248323....\n",
      "Validation loss: 2.0871973896341087....\n",
      "-----------------------------------\n",
      "Training loss: 2.1394647253859014....\n",
      "Validation loss: 2.0871138347544083....\n",
      "-----------------------------------\n",
      "Training loss: 2.1392317729185373....\n",
      "Validation loss: 2.087030276854833....\n",
      "-----------------------------------\n",
      "Training loss: 2.1389987942560715....\n",
      "Validation loss: 2.086946883249027....\n",
      "-----------------------------------\n",
      "Training loss: 2.1387654982547644....\n",
      "Validation loss: 2.0868633717710643....\n",
      "-----------------------------------\n",
      "Training loss: 2.1385324897148754....\n",
      "Validation loss: 2.0867804980971494....\n",
      "-----------------------------------\n",
      "Training loss: 2.138298844994404....\n",
      "Validation loss: 2.086697343711471....\n",
      "-----------------------------------\n",
      "Training loss: 2.1380656453636875....\n",
      "Validation loss: 2.086614318377145....\n",
      "-----------------------------------\n",
      "Training loss: 2.1378323220230904....\n",
      "Validation loss: 2.0865314497751593....\n",
      "-----------------------------------\n",
      "Training loss: 2.137599048878422....\n",
      "Validation loss: 2.0864482302200775....\n",
      "-----------------------------------\n",
      "Training loss: 2.137365511225279....\n",
      "Validation loss: 2.0863652537087227....\n",
      "-----------------------------------\n",
      "Training loss: 2.1371315420432024....\n",
      "Validation loss: 2.086282637593325....\n",
      "-----------------------------------\n",
      "Training loss: 2.1368977026466034....\n",
      "Validation loss: 2.08619895356009....\n",
      "-----------------------------------\n",
      "Training loss: 2.1366631611958278....\n",
      "Validation loss: 2.086115599583436....\n",
      "-----------------------------------\n",
      "Training loss: 2.1364283501290946....\n",
      "Validation loss: 2.0860317300226003....\n",
      "-----------------------------------\n",
      "Training loss: 2.1361936313485193....\n",
      "Validation loss: 2.085947520440377....\n",
      "-----------------------------------\n",
      "Training loss: 2.135958635024324....\n",
      "Validation loss: 2.0858633260454185....\n",
      "-----------------------------------\n",
      "Training loss: 2.1357231978297073....\n",
      "Validation loss: 2.085779569683179....\n",
      "-----------------------------------\n",
      "Training loss: 2.1354873842909967....\n",
      "Validation loss: 2.0856966560353922....\n",
      "-----------------------------------\n",
      "Training loss: 2.1352519311345084....\n",
      "Validation loss: 2.085613117929985....\n",
      "-----------------------------------\n",
      "Training loss: 2.1350170213500634....\n",
      "Validation loss: 2.0855297489341598....\n",
      "-----------------------------------\n",
      "Training loss: 2.1347816699179902....\n",
      "Validation loss: 2.085446726210393....\n",
      "-----------------------------------\n",
      "Training loss: 2.1345458939909148....\n",
      "Validation loss: 2.085362961334811....\n",
      "-----------------------------------\n",
      "Training loss: 2.1343098094842543....\n",
      "Validation loss: 2.0852795357459017....\n",
      "-----------------------------------\n",
      "Training loss: 2.1340735586401878....\n",
      "Validation loss: 2.085195824087666....\n",
      "-----------------------------------\n",
      "Training loss: 2.133837249246855....\n",
      "Validation loss: 2.0851117470836553....\n",
      "-----------------------------------\n",
      "Training loss: 2.133601554281824....\n",
      "Validation loss: 2.0850277049685553....\n",
      "-----------------------------------\n",
      "Training loss: 2.1333659424866043....\n",
      "Validation loss: 2.084944030192592....\n",
      "-----------------------------------\n",
      "Training loss: 2.133129681121788....\n",
      "Validation loss: 2.0848593467833596....\n",
      "-----------------------------------\n",
      "Training loss: 2.1328932901504767....\n",
      "Validation loss: 2.084775774234099....\n",
      "-----------------------------------\n",
      "Training loss: 2.1326567763283624....\n",
      "Validation loss: 2.0846915821192016....\n",
      "-----------------------------------\n",
      "Training loss: 2.132420557775029....\n",
      "Validation loss: 2.084606459781836....\n",
      "-----------------------------------\n",
      "Training loss: 2.132184938223684....\n",
      "Validation loss: 2.0845219603255476....\n",
      "-----------------------------------\n",
      "Training loss: 2.131948810817852....\n",
      "Validation loss: 2.0844382901989658....\n",
      "-----------------------------------\n",
      "Training loss: 2.131712264269503....\n",
      "Validation loss: 2.0843545690768917....\n",
      "-----------------------------------\n",
      "Training loss: 2.1314752196679647....\n",
      "Validation loss: 2.0842719090429136....\n",
      "-----------------------------------\n",
      "Training loss: 2.1312380023627493....\n",
      "Validation loss: 2.0841899032796465....\n",
      "-----------------------------------\n",
      "Training loss: 2.1310005418369258....\n",
      "Validation loss: 2.084108275547551....\n",
      "-----------------------------------\n",
      "Training loss: 2.1307630216900306....\n",
      "Validation loss: 2.0840253354521727....\n",
      "-----------------------------------\n",
      "Training loss: 2.13052555034982....\n",
      "Validation loss: 2.0839421126215614....\n",
      "-----------------------------------\n",
      "Training loss: 2.1302885444894692....\n",
      "Validation loss: 2.0838591992985402....\n",
      "-----------------------------------\n",
      "Training loss: 2.1300516818867....\n",
      "Validation loss: 2.0837765868258358....\n",
      "-----------------------------------\n",
      "Training loss: 2.129814549224513....\n",
      "Validation loss: 2.0836943908158605....\n",
      "-----------------------------------\n",
      "Training loss: 2.1295773260158652....\n",
      "Validation loss: 2.083612188338008....\n",
      "-----------------------------------\n",
      "Training loss: 2.12934001008245....\n",
      "Validation loss: 2.0835294181800874....\n",
      "-----------------------------------\n",
      "Training loss: 2.1291020261221134....\n",
      "Validation loss: 2.0834477328555354....\n",
      "-----------------------------------\n",
      "Training loss: 2.1288638907729758....\n",
      "Validation loss: 2.0833653482862675....\n",
      "-----------------------------------\n",
      "Training loss: 2.128626572373172....\n",
      "Validation loss: 2.0832831396278233....\n",
      "-----------------------------------\n",
      "Training loss: 2.128389595595836....\n",
      "Validation loss: 2.083201621166901....\n",
      "-----------------------------------\n",
      "Training loss: 2.128152238087159....\n",
      "Validation loss: 2.08312002501464....\n",
      "-----------------------------------\n",
      "Training loss: 2.1279144379836312....\n",
      "Validation loss: 2.0830382655413127....\n",
      "-----------------------------------\n",
      "Training loss: 2.127676777957399....\n",
      "Validation loss: 2.0829565074962817....\n",
      "-----------------------------------\n",
      "Training loss: 2.1274392070109913....\n",
      "Validation loss: 2.0828752638603016....\n",
      "-----------------------------------\n",
      "Training loss: 2.1272016007093897....\n",
      "Validation loss: 2.082793955367848....\n",
      "-----------------------------------\n",
      "Training loss: 2.1269640947833413....\n",
      "Validation loss: 2.082712752176125....\n",
      "-----------------------------------\n",
      "Training loss: 2.1267265156101014....\n",
      "Validation loss: 2.0826314255519662....\n",
      "-----------------------------------\n",
      "Training loss: 2.12648918555153....\n",
      "Validation loss: 2.08254960416178....\n",
      "-----------------------------------\n",
      "Training loss: 2.126251662628831....\n",
      "Validation loss: 2.0824681608388618....\n",
      "-----------------------------------\n",
      "Training loss: 2.126013849153483....\n",
      "Validation loss: 2.082385536927439....\n",
      "-----------------------------------\n",
      "Training loss: 2.125776146547205....\n",
      "Validation loss: 2.082303736725084....\n",
      "-----------------------------------\n",
      "Training loss: 2.1255383061101902....\n",
      "Validation loss: 2.0822220174400408....\n",
      "-----------------------------------\n",
      "Training loss: 2.1253002144183806....\n",
      "Validation loss: 2.0821402438155157....\n",
      "-----------------------------------\n",
      "Training loss: 2.125061868236949....\n",
      "Validation loss: 2.0820583810712487....\n",
      "-----------------------------------\n",
      "Training loss: 2.124823530405399....\n",
      "Validation loss: 2.081977031901187....\n",
      "-----------------------------------\n",
      "Training loss: 2.1245848564770005....\n",
      "Validation loss: 2.0818956996571263....\n",
      "-----------------------------------\n",
      "Training loss: 2.124346110325061....\n",
      "Validation loss: 2.081815348409411....\n",
      "-----------------------------------\n",
      "Training loss: 2.1241073251821097....\n",
      "Validation loss: 2.081735172954506....\n",
      "-----------------------------------\n",
      "Training loss: 2.123867954225682....\n",
      "Validation loss: 2.0816548368318473....\n",
      "-----------------------------------\n",
      "Training loss: 2.1236278437170544....\n",
      "Validation loss: 2.081574342898647....\n",
      "-----------------------------------\n",
      "Training loss: 2.123387565277384....\n",
      "Validation loss: 2.0814938872752826....\n",
      "-----------------------------------\n",
      "Training loss: 2.1231473849926252....\n",
      "Validation loss: 2.0814132648896018....\n",
      "-----------------------------------\n",
      "Training loss: 2.1229070301671427....\n",
      "Validation loss: 2.0813338945552586....\n",
      "-----------------------------------\n",
      "Training loss: 2.122666408890353....\n",
      "Validation loss: 2.0812539234235237....\n",
      "-----------------------------------\n",
      "Training loss: 2.1224261200951515....\n",
      "Validation loss: 2.0811745504412023....\n",
      "-----------------------------------\n",
      "Training loss: 2.122185973693632....\n",
      "Validation loss: 2.081094142911257....\n",
      "-----------------------------------\n",
      "Training loss: 2.121946092492256....\n",
      "Validation loss: 2.08101412603337....\n",
      "-----------------------------------\n",
      "Training loss: 2.1217054781508797....\n",
      "Validation loss: 2.080934140400142....\n",
      "-----------------------------------\n",
      "Training loss: 2.1214651562283096....\n",
      "Validation loss: 2.080853581606231....\n",
      "-----------------------------------\n",
      "Training loss: 2.1212259600254293....\n",
      "Validation loss: 2.080772976346374....\n",
      "-----------------------------------\n",
      "Training loss: 2.120986972836634....\n",
      "Validation loss: 2.0806928669761553....\n",
      "-----------------------------------\n",
      "Training loss: 2.12074711434364....\n",
      "Validation loss: 2.080613205584845....\n",
      "-----------------------------------\n",
      "Training loss: 2.12050693624152....\n",
      "Validation loss: 2.0805330039295176....\n",
      "-----------------------------------\n",
      "Training loss: 2.1202670604800744....\n",
      "Validation loss: 2.0804517324354....\n",
      "-----------------------------------\n",
      "Training loss: 2.1200277228105944....\n",
      "Validation loss: 2.0803705353948363....\n",
      "-----------------------------------\n",
      "Training loss: 2.119788795400484....\n",
      "Validation loss: 2.0802899181936696....\n",
      "-----------------------------------\n",
      "Training loss: 2.1195496909880287....\n",
      "Validation loss: 2.080208879605584....\n",
      "-----------------------------------\n",
      "Training loss: 2.119310533069479....\n",
      "Validation loss: 2.0801285398311573....\n",
      "-----------------------------------\n",
      "Training loss: 2.1190711248926073....\n",
      "Validation loss: 2.0800481805204187....\n",
      "-----------------------------------\n",
      "Training loss: 2.118831301600115....\n",
      "Validation loss: 2.079967863976528....\n",
      "-----------------------------------\n",
      "Training loss: 2.1185915796312607....\n",
      "Validation loss: 2.0798862690469604....\n",
      "-----------------------------------\n",
      "Training loss: 2.118352349256143....\n",
      "Validation loss: 2.0798046784021813....\n",
      "-----------------------------------\n",
      "Training loss: 2.118113139577937....\n",
      "Validation loss: 2.0797224850253526....\n",
      "-----------------------------------\n",
      "Training loss: 2.117873857552089....\n",
      "Validation loss: 2.079640494970582....\n",
      "-----------------------------------\n",
      "Training loss: 2.1176349698089227....\n",
      "Validation loss: 2.0795583957100834....\n",
      "-----------------------------------\n",
      "Training loss: 2.1173963821884327....\n",
      "Validation loss: 2.07947655702092....\n",
      "-----------------------------------\n",
      "Training loss: 2.1171578623588547....\n",
      "Validation loss: 2.0793939409299633....\n",
      "-----------------------------------\n",
      "Training loss: 2.1169195525078828....\n",
      "Validation loss: 2.079311642972109....\n",
      "-----------------------------------\n",
      "Training loss: 2.116680708124846....\n",
      "Validation loss: 2.0792286892283736....\n",
      "-----------------------------------\n",
      "Training loss: 2.1164413068235386....\n",
      "Validation loss: 2.0791458413168096....\n",
      "-----------------------------------\n",
      "Training loss: 2.1162017291822703....\n",
      "Validation loss: 2.0790634159941623....\n",
      "-----------------------------------\n",
      "Training loss: 2.1159623772917886....\n",
      "Validation loss: 2.0789815665849036....\n",
      "-----------------------------------\n",
      "Training loss: 2.115722928055392....\n",
      "Validation loss: 2.078898840924483....\n",
      "-----------------------------------\n",
      "Training loss: 2.115483839733449....\n",
      "Validation loss: 2.0788159501132197....\n",
      "-----------------------------------\n",
      "Training loss: 2.1152445252157235....\n",
      "Validation loss: 2.0787326421634433....\n",
      "-----------------------------------\n",
      "Training loss: 2.1150051482120045....\n",
      "Validation loss: 2.078649093569663....\n",
      "-----------------------------------\n",
      "Training loss: 2.1147664818730028....\n",
      "Validation loss: 2.0785649294503523....\n",
      "-----------------------------------\n",
      "Training loss: 2.1145284327256024....\n",
      "Validation loss: 2.0784803317777305....\n",
      "-----------------------------------\n",
      "Training loss: 2.1142904001720493....\n",
      "Validation loss: 2.078395451761232....\n",
      "-----------------------------------\n",
      "Training loss: 2.1140527179430215....\n",
      "Validation loss: 2.078310706710037....\n",
      "-----------------------------------\n",
      "Training loss: 2.113815301975562....\n",
      "Validation loss: 2.0782262041274056....\n",
      "-----------------------------------\n",
      "Training loss: 2.113577736385423....\n",
      "Validation loss: 2.0781420247953757....\n",
      "-----------------------------------\n",
      "Training loss: 2.113340177775666....\n",
      "Validation loss: 2.0780573028986873....\n",
      "-----------------------------------\n",
      "Training loss: 2.1131025054136927....\n",
      "Validation loss: 2.0779727670177497....\n",
      "-----------------------------------\n",
      "Training loss: 2.112864901930529....\n",
      "Validation loss: 2.077887622426569....\n",
      "-----------------------------------\n",
      "Training loss: 2.1126271184387644....\n",
      "Validation loss: 2.0778027623063573....\n",
      "-----------------------------------\n",
      "Training loss: 2.1123893582354585....\n",
      "Validation loss: 2.0777173762945775....\n",
      "-----------------------------------\n",
      "Training loss: 2.11215234312134....\n",
      "Validation loss: 2.077632667344767....\n",
      "-----------------------------------\n",
      "Training loss: 2.111915173935571....\n",
      "Validation loss: 2.077546939170406....\n",
      "-----------------------------------\n",
      "Training loss: 2.111677858841479....\n",
      "Validation loss: 2.0774609985264245....\n",
      "-----------------------------------\n",
      "Training loss: 2.1114406385378874....\n",
      "Validation loss: 2.07737438536368....\n",
      "-----------------------------------\n",
      "Training loss: 2.1112032157913547....\n",
      "Validation loss: 2.077287493602935....\n",
      "-----------------------------------\n",
      "Training loss: 2.1109653739151955....\n",
      "Validation loss: 2.077200229073634....\n",
      "-----------------------------------\n",
      "Training loss: 2.1107275764206515....\n",
      "Validation loss: 2.0771129214574775....\n",
      "-----------------------------------\n",
      "Training loss: 2.110490052158098....\n",
      "Validation loss: 2.0770252786706718....\n",
      "-----------------------------------\n",
      "Training loss: 2.1102526416838163....\n",
      "Validation loss: 2.0769379358930453....\n",
      "-----------------------------------\n",
      "Training loss: 2.110014856080936....\n",
      "Validation loss: 2.0768503178228417....\n",
      "-----------------------------------\n",
      "Training loss: 2.1097770381454692....\n",
      "Validation loss: 2.076762805999213....\n",
      "-----------------------------------\n",
      "Training loss: 2.1095391373732215....\n",
      "Validation loss: 2.076675660648175....\n",
      "-----------------------------------\n",
      "Training loss: 2.109300658326433....\n",
      "Validation loss: 2.0765892446098397....\n",
      "-----------------------------------\n",
      "Training loss: 2.109062105255299....\n",
      "Validation loss: 2.076503574022813....\n",
      "-----------------------------------\n",
      "Training loss: 2.108823517204893....\n",
      "Validation loss: 2.0764179383636368....\n",
      "-----------------------------------\n",
      "Training loss: 2.1085841737653035....\n",
      "Validation loss: 2.0763314467808374....\n",
      "-----------------------------------\n",
      "Training loss: 2.108344162349629....\n",
      "Validation loss: 2.0762439570513145....\n",
      "-----------------------------------\n",
      "Training loss: 2.108103573752805....\n",
      "Validation loss: 2.0761568873177967....\n",
      "-----------------------------------\n",
      "Training loss: 2.107863402229639....\n",
      "Validation loss: 2.076070103785242....\n",
      "-----------------------------------\n",
      "Training loss: 2.1076237310038253....\n",
      "Validation loss: 2.0759833353413644....\n",
      "-----------------------------------\n",
      "Training loss: 2.1073839760294897....\n",
      "Validation loss: 2.07589703814358....\n",
      "-----------------------------------\n",
      "Training loss: 2.107144298496927....\n",
      "Validation loss: 2.0758109125353648....\n",
      "-----------------------------------\n",
      "Training loss: 2.106904582151602....\n",
      "Validation loss: 2.075725234884037....\n",
      "-----------------------------------\n",
      "Training loss: 2.106664587595018....\n",
      "Validation loss: 2.075639522733885....\n",
      "-----------------------------------\n",
      "Training loss: 2.106424979708815....\n",
      "Validation loss: 2.0755541567716196....\n",
      "-----------------------------------\n",
      "Training loss: 2.1061854365319395....\n",
      "Validation loss: 2.0754673711915643....\n",
      "-----------------------------------\n",
      "Training loss: 2.105946041128159....\n",
      "Validation loss: 2.075380515937124....\n",
      "-----------------------------------\n",
      "Training loss: 2.105706544140758....\n",
      "Validation loss: 2.0752935382027173....\n",
      "-----------------------------------\n",
      "Training loss: 2.105466353769663....\n",
      "Validation loss: 2.07520693222549....\n",
      "-----------------------------------\n",
      "Training loss: 2.105225816081428....\n",
      "Validation loss: 2.075120920160787....\n",
      "-----------------------------------\n",
      "Training loss: 2.1049849224899697....\n",
      "Validation loss: 2.0750355800982327....\n",
      "-----------------------------------\n",
      "Training loss: 2.1047439677521553....\n",
      "Validation loss: 2.0749498818885233....\n",
      "-----------------------------------\n",
      "Training loss: 2.104503404013798....\n",
      "Validation loss: 2.0748642995051245....\n",
      "-----------------------------------\n",
      "Training loss: 2.1042627932908626....\n",
      "Validation loss: 2.07477878966817....\n",
      "-----------------------------------\n",
      "Training loss: 2.1040221559759793....\n",
      "Validation loss: 2.074693260263824....\n",
      "-----------------------------------\n",
      "Training loss: 2.1037815252084853....\n",
      "Validation loss: 2.0746077816909114....\n",
      "-----------------------------------\n",
      "Training loss: 2.1035412155118403....\n",
      "Validation loss: 2.0745217315010986....\n",
      "-----------------------------------\n",
      "Training loss: 2.103301008067988....\n",
      "Validation loss: 2.074435944572069....\n",
      "-----------------------------------\n",
      "Training loss: 2.1030608422183814....\n",
      "Validation loss: 2.074350084624873....\n",
      "-----------------------------------\n",
      "Training loss: 2.1028207112325146....\n",
      "Validation loss: 2.0742646878791002....\n",
      "-----------------------------------\n",
      "Training loss: 2.102580108192368....\n",
      "Validation loss: 2.074179327038976....\n",
      "-----------------------------------\n",
      "Training loss: 2.1023386802101585....\n",
      "Validation loss: 2.0740942881989177....\n",
      "-----------------------------------\n",
      "Training loss: 2.102097330985078....\n",
      "Validation loss: 2.0740089866024833....\n",
      "-----------------------------------\n",
      "Training loss: 2.1018561643706297....\n",
      "Validation loss: 2.073924074822298....\n",
      "-----------------------------------\n",
      "Training loss: 2.1016147653053987....\n",
      "Validation loss: 2.073839303261786....\n",
      "-----------------------------------\n",
      "Training loss: 2.1013738166048355....\n",
      "Validation loss: 2.0737537023516968....\n",
      "-----------------------------------\n",
      "Training loss: 2.101133299054969....\n",
      "Validation loss: 2.073668323742066....\n",
      "-----------------------------------\n",
      "Training loss: 2.1008928202110027....\n",
      "Validation loss: 2.073582328334024....\n",
      "-----------------------------------\n",
      "Training loss: 2.1006526806457275....\n",
      "Validation loss: 2.0734957782874623....\n",
      "-----------------------------------\n",
      "Training loss: 2.100412911912642....\n",
      "Validation loss: 2.0734082092638686....\n",
      "-----------------------------------\n",
      "Training loss: 2.1001736446944133....\n",
      "Validation loss: 2.073319980080011....\n",
      "-----------------------------------\n",
      "Training loss: 2.099934138955624....\n",
      "Validation loss: 2.0732317117134134....\n",
      "-----------------------------------\n",
      "Training loss: 2.0996944021964006....\n",
      "Validation loss: 2.0731432281255207....\n",
      "-----------------------------------\n",
      "Training loss: 2.09945472457779....\n",
      "Validation loss: 2.0730552424631274....\n",
      "-----------------------------------\n",
      "Training loss: 2.0992152468499072....\n",
      "Validation loss: 2.0729669857341277....\n",
      "-----------------------------------\n",
      "Training loss: 2.098975954883504....\n",
      "Validation loss: 2.072878816756396....\n",
      "-----------------------------------\n",
      "Training loss: 2.098736725288516....\n",
      "Validation loss: 2.0727908950481106....\n",
      "-----------------------------------\n",
      "Training loss: 2.098497683553888....\n",
      "Validation loss: 2.072702652163257....\n",
      "-----------------------------------\n",
      "Training loss: 2.0982583093368987....\n",
      "Validation loss: 2.0726148579943082....\n",
      "-----------------------------------\n",
      "Training loss: 2.0980191074019476....\n",
      "Validation loss: 2.072526089171609....\n",
      "-----------------------------------\n",
      "Training loss: 2.097780392988195....\n",
      "Validation loss: 2.072437132800246....\n",
      "-----------------------------------\n",
      "Training loss: 2.0975420861427927....\n",
      "Validation loss: 2.0723489535911863....\n",
      "-----------------------------------\n",
      "Training loss: 2.0973039882025923....\n",
      "Validation loss: 2.0722604342571627....\n",
      "-----------------------------------\n",
      "Training loss: 2.097066053137594....\n",
      "Validation loss: 2.072172593368507....\n",
      "-----------------------------------\n",
      "Training loss: 2.096828149329701....\n",
      "Validation loss: 2.072084510639589....\n",
      "-----------------------------------\n",
      "Training loss: 2.096590454764186....\n",
      "Validation loss: 2.071996958254441....\n",
      "-----------------------------------\n",
      "Training loss: 2.096352956687471....\n",
      "Validation loss: 2.071908101374361....\n",
      "-----------------------------------\n",
      "Training loss: 2.096115559993259....\n",
      "Validation loss: 2.071819289297845....\n",
      "-----------------------------------\n",
      "Training loss: 2.0958783899812397....\n",
      "Validation loss: 2.071730756084006....\n",
      "-----------------------------------\n",
      "Training loss: 2.0956412599556966....\n",
      "Validation loss: 2.0716426153308056....\n",
      "-----------------------------------\n",
      "Training loss: 2.0954042847948013....\n",
      "Validation loss: 2.071553676371133....\n",
      "-----------------------------------\n",
      "Training loss: 2.095167695921823....\n",
      "Validation loss: 2.071464433642865....\n",
      "-----------------------------------\n",
      "Training loss: 2.094931261786928....\n",
      "Validation loss: 2.0713748389928743....\n",
      "-----------------------------------\n",
      "Training loss: 2.0946951021233953....\n",
      "Validation loss: 2.071285397557178....\n",
      "-----------------------------------\n",
      "Training loss: 2.0944587587851493....\n",
      "Validation loss: 2.071196481267591....\n",
      "-----------------------------------\n",
      "Training loss: 2.09422249181196....\n",
      "Validation loss: 2.0711063228849866....\n",
      "-----------------------------------\n",
      "Training loss: 2.0939868544477673....\n",
      "Validation loss: 2.0710157727078715....\n",
      "-----------------------------------\n",
      "Training loss: 2.0937515532203546....\n",
      "Validation loss: 2.0709248150666064....\n",
      "-----------------------------------\n",
      "Training loss: 2.093516434148752....\n",
      "Validation loss: 2.0708345547598537....\n",
      "-----------------------------------\n",
      "Training loss: 2.0932809415557663....\n",
      "Validation loss: 2.0707449234589332....\n",
      "-----------------------------------\n",
      "Training loss: 2.0930449819135926....\n",
      "Validation loss: 2.0706549064095574....\n",
      "-----------------------------------\n",
      "Training loss: 2.0928093717752962....\n",
      "Validation loss: 2.0705643164312386....\n",
      "-----------------------------------\n",
      "Training loss: 2.0925741435190632....\n",
      "Validation loss: 2.0704734907783506....\n",
      "-----------------------------------\n",
      "Training loss: 2.0923391147143358....\n",
      "Validation loss: 2.0703822213491323....\n",
      "-----------------------------------\n",
      "Training loss: 2.0921040754367115....\n",
      "Validation loss: 2.0702913424657594....\n",
      "-----------------------------------\n",
      "Training loss: 2.09186882502777....\n",
      "Validation loss: 2.070200618682609....\n",
      "-----------------------------------\n",
      "Training loss: 2.091633508699582....\n",
      "Validation loss: 2.0701104290715424....\n",
      "-----------------------------------\n",
      "Training loss: 2.0913978307449446....\n",
      "Validation loss: 2.0700205666409635....\n",
      "-----------------------------------\n",
      "Training loss: 2.0911620366540355....\n",
      "Validation loss: 2.0699308069339835....\n",
      "-----------------------------------\n",
      "Training loss: 2.0909261021665215....\n",
      "Validation loss: 2.069840764528773....\n",
      "-----------------------------------\n",
      "Training loss: 2.0906903974354565....\n",
      "Validation loss: 2.069750652229751....\n",
      "-----------------------------------\n",
      "Training loss: 2.0904548713077267....\n",
      "Validation loss: 2.0696601010936675....\n",
      "-----------------------------------\n",
      "Training loss: 2.0902193197571806....\n",
      "Validation loss: 2.069570063393851....\n",
      "-----------------------------------\n",
      "Training loss: 2.0899836190572114....\n",
      "Validation loss: 2.0694805299950385....\n",
      "-----------------------------------\n",
      "Training loss: 2.0897477296174065....\n",
      "Validation loss: 2.0693909190907296....\n",
      "-----------------------------------\n",
      "Training loss: 2.0895117855050063....\n",
      "Validation loss: 2.069302123517169....\n",
      "-----------------------------------\n",
      "Training loss: 2.089275497434931....\n",
      "Validation loss: 2.069213150352192....\n",
      "-----------------------------------\n",
      "Training loss: 2.089038988563594....\n",
      "Validation loss: 2.0691246003350323....\n",
      "-----------------------------------\n",
      "Training loss: 2.0888021872297475....\n",
      "Validation loss: 2.069036292852032....\n",
      "-----------------------------------\n",
      "Training loss: 2.08856479126233....\n",
      "Validation loss: 2.068948436205656....\n",
      "-----------------------------------\n",
      "Training loss: 2.0883277544785015....\n",
      "Validation loss: 2.068860589538168....\n",
      "-----------------------------------\n",
      "Training loss: 2.0880909131817775....\n",
      "Validation loss: 2.068772715198956....\n",
      "-----------------------------------\n",
      "Training loss: 2.0878538593983818....\n",
      "Validation loss: 2.0686851188993427....\n",
      "-----------------------------------\n",
      "Training loss: 2.0876167626914217....\n",
      "Validation loss: 2.068596809103901....\n",
      "-----------------------------------\n",
      "Training loss: 2.0873796790542065....\n",
      "Validation loss: 2.0685092530932927....\n",
      "-----------------------------------\n",
      "Training loss: 2.087142465525254....\n",
      "Validation loss: 2.0684213913071328....\n",
      "-----------------------------------\n",
      "Training loss: 2.08690497397966....\n",
      "Validation loss: 2.068334310499954....\n",
      "-----------------------------------\n",
      "Training loss: 2.086667050928858....\n",
      "Validation loss: 2.068247176232929....\n",
      "-----------------------------------\n",
      "Training loss: 2.0864291575162843....\n",
      "Validation loss: 2.0681598179761904....\n",
      "-----------------------------------\n",
      "Training loss: 2.0861918866521885....\n",
      "Validation loss: 2.068071898994515....\n",
      "-----------------------------------\n",
      "Training loss: 2.0859547788772344....\n",
      "Validation loss: 2.0679849415431137....\n",
      "-----------------------------------\n",
      "Training loss: 2.0857173773866142....\n",
      "Validation loss: 2.06789798031308....\n",
      "-----------------------------------\n",
      "Training loss: 2.085479622888459....\n",
      "Validation loss: 2.0678108934305204....\n",
      "-----------------------------------\n",
      "Training loss: 2.085241569012906....\n",
      "Validation loss: 2.067724368820588....\n",
      "-----------------------------------\n",
      "Training loss: 2.0850038109368043....\n",
      "Validation loss: 2.0676374327214....\n",
      "-----------------------------------\n",
      "Training loss: 2.0847660977948843....\n",
      "Validation loss: 2.067549999760325....\n",
      "-----------------------------------\n",
      "Training loss: 2.084528704612715....\n",
      "Validation loss: 2.067461426734487....\n",
      "-----------------------------------\n",
      "Training loss: 2.084292026545976....\n",
      "Validation loss: 2.0673735124686203....\n",
      "-----------------------------------\n",
      "Training loss: 2.0840553142234306....\n",
      "Validation loss: 2.067285947167412....\n",
      "-----------------------------------\n",
      "Training loss: 2.083818588032944....\n",
      "Validation loss: 2.0671983927251754....\n",
      "-----------------------------------\n",
      "Training loss: 2.083581750448974....\n",
      "Validation loss: 2.0671105347240326....\n",
      "-----------------------------------\n",
      "Training loss: 2.0833445771498917....\n",
      "Validation loss: 2.067022632731582....\n",
      "-----------------------------------\n",
      "Training loss: 2.0831068699441397....\n",
      "Validation loss: 2.0669345488699964....\n",
      "-----------------------------------\n",
      "Training loss: 2.08286919161255....\n",
      "Validation loss: 2.066846725583747....\n",
      "-----------------------------------\n",
      "Training loss: 2.08263186379519....\n",
      "Validation loss: 2.0667594247651615....\n",
      "-----------------------------------\n",
      "Training loss: 2.08239468929302....\n",
      "Validation loss: 2.066671460468899....\n",
      "-----------------------------------\n",
      "Training loss: 2.0821577311908137....\n",
      "Validation loss: 2.0665837104273743....\n",
      "-----------------------------------\n",
      "Training loss: 2.081920837674205....\n",
      "Validation loss: 2.066495534436577....\n",
      "-----------------------------------\n",
      "Training loss: 2.0816837433334303....\n",
      "Validation loss: 2.0664078313834646....\n",
      "-----------------------------------\n",
      "Training loss: 2.0814469376117453....\n",
      "Validation loss: 2.066319677376006....\n",
      "-----------------------------------\n",
      "Training loss: 2.0812104734087615....\n",
      "Validation loss: 2.0662315921207384....\n",
      "-----------------------------------\n",
      "Training loss: 2.0809740715015597....\n",
      "Validation loss: 2.0661427320978665....\n",
      "-----------------------------------\n",
      "Training loss: 2.080738050969656....\n",
      "Validation loss: 2.0660533884295553....\n",
      "-----------------------------------\n",
      "Training loss: 2.0805019814675156....\n",
      "Validation loss: 2.0659631253929738....\n",
      "-----------------------------------\n",
      "Training loss: 2.080265982922054....\n",
      "Validation loss: 2.06587437574967....\n",
      "-----------------------------------\n",
      "Training loss: 2.08002936310722....\n",
      "Validation loss: 2.065786391863679....\n",
      "-----------------------------------\n",
      "Training loss: 2.0797926620605227....\n",
      "Validation loss: 2.0656981582565757....\n",
      "-----------------------------------\n",
      "Training loss: 2.0795562284474056....\n",
      "Validation loss: 2.0656104090954974....\n",
      "-----------------------------------\n",
      "Training loss: 2.0793197768856224....\n",
      "Validation loss: 2.0655231270247056....\n",
      "-----------------------------------\n",
      "Training loss: 2.079083182093598....\n",
      "Validation loss: 2.0654354900446075....\n",
      "-----------------------------------\n",
      "Training loss: 2.07884659676243....\n",
      "Validation loss: 2.065347666052928....\n",
      "-----------------------------------\n",
      "Training loss: 2.078610081433592....\n",
      "Validation loss: 2.0652607101999996....\n",
      "-----------------------------------\n",
      "Training loss: 2.078373526589612....\n",
      "Validation loss: 2.0651740443784....\n",
      "-----------------------------------\n",
      "Training loss: 2.0781374364663785....\n",
      "Validation loss: 2.06508765778909....\n",
      "-----------------------------------\n",
      "Training loss: 2.077901501514337....\n",
      "Validation loss: 2.0650006498367475....\n",
      "-----------------------------------\n",
      "Training loss: 2.0776659878135817....\n",
      "Validation loss: 2.064913267115069....\n",
      "-----------------------------------\n",
      "Training loss: 2.077430552715734....\n",
      "Validation loss: 2.0648260157555276....\n",
      "-----------------------------------\n",
      "Training loss: 2.077195167678793....\n",
      "Validation loss: 2.0647382815270596....\n",
      "-----------------------------------\n",
      "Training loss: 2.076959980216176....\n",
      "Validation loss: 2.0646502762475545....\n",
      "-----------------------------------\n",
      "Training loss: 2.0767249920128514....\n",
      "Validation loss: 2.064561813685914....\n",
      "-----------------------------------\n",
      "Training loss: 2.076490170540383....\n",
      "Validation loss: 2.064473774048417....\n",
      "-----------------------------------\n",
      "Training loss: 2.076255288320903....\n",
      "Validation loss: 2.06438467961054....\n",
      "-----------------------------------\n",
      "Training loss: 2.0760198995545096....\n",
      "Validation loss: 2.064295750830029....\n",
      "-----------------------------------\n",
      "Training loss: 2.0757844755833594....\n",
      "Validation loss: 2.0642061221411723....\n",
      "-----------------------------------\n",
      "Training loss: 2.075549381710982....\n",
      "Validation loss: 2.064116643611393....\n",
      "-----------------------------------\n",
      "Training loss: 2.0753143175268107....\n",
      "Validation loss: 2.0640282504155536....\n",
      "-----------------------------------\n",
      "Training loss: 2.075079133537519....\n",
      "Validation loss: 2.0639398545908287....\n",
      "-----------------------------------\n",
      "Training loss: 2.0748440630685567....\n",
      "Validation loss: 2.063851931545223....\n",
      "-----------------------------------\n",
      "Training loss: 2.074609148176144....\n",
      "Validation loss: 2.0637637914859845....\n",
      "-----------------------------------\n",
      "Training loss: 2.074373971615177....\n",
      "Validation loss: 2.063675740788048....\n",
      "-----------------------------------\n",
      "Training loss: 2.074138965915932....\n",
      "Validation loss: 2.0635878326300654....\n",
      "-----------------------------------\n",
      "Training loss: 2.0739040520255014....\n",
      "Validation loss: 2.0634994852830553....\n",
      "-----------------------------------\n",
      "Training loss: 2.0736691440234005....\n",
      "Validation loss: 2.0634104814424648....\n",
      "-----------------------------------\n",
      "Training loss: 2.073434357924943....\n",
      "Validation loss: 2.0633230586900506....\n",
      "-----------------------------------\n",
      "Training loss: 2.073199123287386....\n",
      "Validation loss: 2.0632345101194907....\n",
      "-----------------------------------\n",
      "Training loss: 2.0729637523604447....\n",
      "Validation loss: 2.0631461234121913....\n",
      "-----------------------------------\n",
      "Training loss: 2.0727280067442773....\n",
      "Validation loss: 2.063057516185294....\n",
      "-----------------------------------\n",
      "Training loss: 2.072492364102206....\n",
      "Validation loss: 2.062968701472141....\n",
      "-----------------------------------\n",
      "Training loss: 2.0722570339240605....\n",
      "Validation loss: 2.062879660195455....\n",
      "-----------------------------------\n",
      "Training loss: 2.072022053986808....\n",
      "Validation loss: 2.0627909721970825....\n",
      "-----------------------------------\n",
      "Training loss: 2.071787005024835....\n",
      "Validation loss: 2.0627019970814042....\n",
      "-----------------------------------\n",
      "Training loss: 2.0715518449548007....\n",
      "Validation loss: 2.062612380480008....\n",
      "-----------------------------------\n",
      "Training loss: 2.0713167220058266....\n",
      "Validation loss: 2.0625234725931665....\n",
      "-----------------------------------\n",
      "Training loss: 2.0710813422232204....\n",
      "Validation loss: 2.0624343867658763....\n",
      "-----------------------------------\n",
      "Training loss: 2.070845867499499....\n",
      "Validation loss: 2.0623457707707256....\n",
      "-----------------------------------\n",
      "Training loss: 2.0706102067863124....\n",
      "Validation loss: 2.0622571285101907....\n",
      "-----------------------------------\n",
      "Training loss: 2.07037416630211....\n",
      "Validation loss: 2.062169109404501....\n",
      "-----------------------------------\n",
      "Training loss: 2.0701383089183567....\n",
      "Validation loss: 2.0620796160957715....\n",
      "-----------------------------------\n",
      "Training loss: 2.0699025903115067....\n",
      "Validation loss: 2.061991420133259....\n",
      "-----------------------------------\n",
      "Training loss: 2.0696668900915913....\n",
      "Validation loss: 2.0619014749838533....\n",
      "-----------------------------------\n",
      "Training loss: 2.0694313713860297....\n",
      "Validation loss: 2.0618115400147836....\n",
      "-----------------------------------\n",
      "Training loss: 2.0691959940007636....\n",
      "Validation loss: 2.061722056683298....\n",
      "-----------------------------------\n",
      "Training loss: 2.068960806445635....\n",
      "Validation loss: 2.0616318864656873....\n",
      "-----------------------------------\n",
      "Training loss: 2.0687257460640804....\n",
      "Validation loss: 2.061541426957501....\n",
      "-----------------------------------\n",
      "Training loss: 2.068490538249906....\n",
      "Validation loss: 2.0614518184258284....\n",
      "-----------------------------------\n",
      "Training loss: 2.0682552476650464....\n",
      "Validation loss: 2.061361979726772....\n",
      "-----------------------------------\n",
      "Training loss: 2.068019742359722....\n",
      "Validation loss: 2.061271947428361....\n",
      "-----------------------------------\n",
      "Training loss: 2.067784250491837....\n",
      "Validation loss: 2.0611815086715284....\n",
      "-----------------------------------\n",
      "Training loss: 2.067548517578555....\n",
      "Validation loss: 2.0610918347004965....\n",
      "-----------------------------------\n",
      "Training loss: 2.067312423797186....\n",
      "Validation loss: 2.0610023510460973....\n",
      "-----------------------------------\n",
      "Training loss: 2.0670764290387864....\n",
      "Validation loss: 2.060913734740878....\n",
      "-----------------------------------\n",
      "Training loss: 2.0668401395133635....\n",
      "Validation loss: 2.0608255331994156....\n",
      "-----------------------------------\n",
      "Training loss: 2.066603985934039....\n",
      "Validation loss: 2.0607374101214715....\n",
      "-----------------------------------\n",
      "Training loss: 2.0663680385393683....\n",
      "Validation loss: 2.0606483822519785....\n",
      "-----------------------------------\n",
      "Training loss: 2.066132308735456....\n",
      "Validation loss: 2.060560541938015....\n",
      "-----------------------------------\n",
      "Training loss: 2.0658963479218984....\n",
      "Validation loss: 2.0604725223326312....\n",
      "-----------------------------------\n",
      "Training loss: 2.0656601059484503....\n",
      "Validation loss: 2.0603857601037836....\n",
      "-----------------------------------\n",
      "Training loss: 2.065423598548034....\n",
      "Validation loss: 2.060298825468003....\n",
      "-----------------------------------\n",
      "Training loss: 2.065186941198918....\n",
      "Validation loss: 2.060211983968313....\n",
      "-----------------------------------\n",
      "Training loss: 2.0649505971630076....\n",
      "Validation loss: 2.0601256473845173....\n",
      "-----------------------------------\n",
      "Training loss: 2.064714277211096....\n",
      "Validation loss: 2.0600402411755363....\n",
      "-----------------------------------\n",
      "Training loss: 2.0644775814201566....\n",
      "Validation loss: 2.0599550172334786....\n",
      "-----------------------------------\n",
      "Training loss: 2.0642407604065562....\n",
      "Validation loss: 2.059869619162188....\n",
      "-----------------------------------\n",
      "Training loss: 2.0640034941658554....\n",
      "Validation loss: 2.0597848222585013....\n",
      "-----------------------------------\n",
      "Training loss: 2.0637660047951085....\n",
      "Validation loss: 2.0596987739974617....\n",
      "-----------------------------------\n",
      "Training loss: 2.0635287197157224....\n",
      "Validation loss: 2.0596124543507295....\n",
      "-----------------------------------\n",
      "Training loss: 2.0632917172676297....\n",
      "Validation loss: 2.0595262403998142....\n",
      "-----------------------------------\n",
      "Training loss: 2.063054752312684....\n",
      "Validation loss: 2.059440889707919....\n",
      "-----------------------------------\n",
      "Training loss: 2.062817897101765....\n",
      "Validation loss: 2.059354609579482....\n",
      "-----------------------------------\n",
      "Training loss: 2.062581457757467....\n",
      "Validation loss: 2.059268881262609....\n",
      "-----------------------------------\n",
      "Training loss: 2.0623454584257193....\n",
      "Validation loss: 2.0591823207738154....\n",
      "-----------------------------------\n",
      "Training loss: 2.062109573590899....\n",
      "Validation loss: 2.0590960100811104....\n",
      "-----------------------------------\n",
      "Training loss: 2.0618739996929465....\n",
      "Validation loss: 2.059009354370918....\n",
      "-----------------------------------\n",
      "Training loss: 2.061638710104571....\n",
      "Validation loss: 2.0589228785040286....\n",
      "-----------------------------------\n",
      "Training loss: 2.0614038209633234....\n",
      "Validation loss: 2.058836375152184....\n",
      "-----------------------------------\n",
      "Training loss: 2.061169242822345....\n",
      "Validation loss: 2.0587493554510123....\n",
      "-----------------------------------\n",
      "Training loss: 2.0609348212096785....\n",
      "Validation loss: 2.058663327072684....\n",
      "-----------------------------------\n",
      "Training loss: 2.0607003290711945....\n",
      "Validation loss: 2.0585767853054593....\n",
      "-----------------------------------\n",
      "Training loss: 2.0604661668961937....\n",
      "Validation loss: 2.0584898584371567....\n",
      "-----------------------------------\n",
      "Training loss: 2.060232145108728....\n",
      "Validation loss: 2.0584031887143457....\n",
      "-----------------------------------\n",
      "Training loss: 2.0599978206787464....\n",
      "Validation loss: 2.058316649604984....\n",
      "-----------------------------------\n",
      "Training loss: 2.0597635152438007....\n",
      "Validation loss: 2.058230092804718....\n",
      "-----------------------------------\n",
      "Training loss: 2.059529093811045....\n",
      "Validation loss: 2.0581438464442723....\n",
      "-----------------------------------\n",
      "Training loss: 2.059294669036695....\n",
      "Validation loss: 2.0580572832442123....\n",
      "-----------------------------------\n",
      "Training loss: 2.05906018014981....\n",
      "Validation loss: 2.057971339309837....\n",
      "-----------------------------------\n",
      "Training loss: 2.0588258368304295....\n",
      "Validation loss: 2.0578849305642124....\n",
      "-----------------------------------\n",
      "Training loss: 2.058591338110155....\n",
      "Validation loss: 2.0577991622321976....\n",
      "-----------------------------------\n",
      "Training loss: 2.058356979985362....\n",
      "Validation loss: 2.0577121590040752....\n",
      "-----------------------------------\n",
      "Training loss: 2.0581229406736865....\n",
      "Validation loss: 2.0576252439076343....\n",
      "-----------------------------------\n",
      "Training loss: 2.057888960494444....\n",
      "Validation loss: 2.0575370772101453....\n",
      "-----------------------------------\n",
      "Training loss: 2.0576551553188707....\n",
      "Validation loss: 2.057449689336177....\n",
      "-----------------------------------\n",
      "Training loss: 2.057421263386386....\n",
      "Validation loss: 2.057362085666024....\n",
      "-----------------------------------\n",
      "Training loss: 2.0571873303335497....\n",
      "Validation loss: 2.0572748107447163....\n",
      "-----------------------------------\n",
      "Training loss: 2.0569531187279733....\n",
      "Validation loss: 2.0571876166418614....\n",
      "-----------------------------------\n",
      "Training loss: 2.0567186258365133....\n",
      "Validation loss: 2.057100854660418....\n",
      "-----------------------------------\n",
      "Training loss: 2.0564838579717737....\n",
      "Validation loss: 2.057014256754369....\n",
      "-----------------------------------\n",
      "Training loss: 2.056248936373173....\n",
      "Validation loss: 2.0569274403971454....\n",
      "-----------------------------------\n",
      "Training loss: 2.0560139040988643....\n",
      "Validation loss: 2.0568403423341146....\n",
      "-----------------------------------\n",
      "Training loss: 2.055778910796614....\n",
      "Validation loss: 2.056753433704487....\n",
      "-----------------------------------\n",
      "Training loss: 2.055543890295104....\n",
      "Validation loss: 2.056667618968389....\n",
      "-----------------------------------\n",
      "Training loss: 2.0553084559157413....\n",
      "Validation loss: 2.0565814030717653....\n",
      "-----------------------------------\n",
      "Training loss: 2.055073271951437....\n",
      "Validation loss: 2.056494785483486....\n",
      "-----------------------------------\n",
      "Training loss: 2.054838212026295....\n",
      "Validation loss: 2.0564074569802346....\n",
      "-----------------------------------\n",
      "Training loss: 2.0546034215886935....\n",
      "Validation loss: 2.0563199496538953....\n",
      "-----------------------------------\n",
      "Training loss: 2.054368685467406....\n",
      "Validation loss: 2.056232821044725....\n",
      "-----------------------------------\n",
      "Training loss: 2.054133961808146....\n",
      "Validation loss: 2.056145284219977....\n",
      "-----------------------------------\n",
      "Training loss: 2.0538994504670147....\n",
      "Validation loss: 2.05605744156277....\n",
      "-----------------------------------\n",
      "Training loss: 2.0536650614545406....\n",
      "Validation loss: 2.055970071099743....\n",
      "-----------------------------------\n",
      "Training loss: 2.0534308869919635....\n",
      "Validation loss: 2.0558826426847605....\n",
      "-----------------------------------\n",
      "Training loss: 2.053196537084914....\n",
      "Validation loss: 2.055795634988201....\n",
      "-----------------------------------\n",
      "Training loss: 2.052961919524578....\n",
      "Validation loss: 2.0557080869488527....\n",
      "-----------------------------------\n",
      "Training loss: 2.052727327951786....\n",
      "Validation loss: 2.0556206343788515....\n",
      "-----------------------------------\n",
      "Training loss: 2.0524927238484114....\n",
      "Validation loss: 2.0555328324363926....\n",
      "-----------------------------------\n",
      "Training loss: 2.052257671791667....\n",
      "Validation loss: 2.055445434914599....\n",
      "-----------------------------------\n",
      "Training loss: 2.052022418568562....\n",
      "Validation loss: 2.05535770162539....\n",
      "-----------------------------------\n",
      "Training loss: 2.051787214922209....\n",
      "Validation loss: 2.055269761051673....\n",
      "-----------------------------------\n",
      "Training loss: 2.0515517733528332....\n",
      "Validation loss: 2.0551812353204544....\n",
      "-----------------------------------\n",
      "Training loss: 2.0513160134031674....\n",
      "Validation loss: 2.055093473937623....\n",
      "-----------------------------------\n",
      "Training loss: 2.0510799929440826....\n",
      "Validation loss: 2.0550051584909133....\n",
      "-----------------------------------\n",
      "Training loss: 2.050844144685521....\n",
      "Validation loss: 2.0549169968234886....\n",
      "-----------------------------------\n",
      "Training loss: 2.0506085434621535....\n",
      "Validation loss: 2.054827391953806....\n",
      "-----------------------------------\n",
      "Training loss: 2.0503732448041156....\n",
      "Validation loss: 2.0547387984225707....\n",
      "-----------------------------------\n",
      "Training loss: 2.0501380167997394....\n",
      "Validation loss: 2.0546495681537205....\n",
      "-----------------------------------\n",
      "Training loss: 2.049902656749634....\n",
      "Validation loss: 2.0545608916139066....\n",
      "-----------------------------------\n",
      "Training loss: 2.0496671980183283....\n",
      "Validation loss: 2.054471686965933....\n",
      "-----------------------------------\n",
      "Training loss: 2.0494320777842248....\n",
      "Validation loss: 2.0543829090959114....\n",
      "-----------------------------------\n",
      "Training loss: 2.0491968219011305....\n",
      "Validation loss: 2.054293417749337....\n",
      "-----------------------------------\n",
      "Training loss: 2.0489614916106453....\n",
      "Validation loss: 2.0542035574213555....\n",
      "-----------------------------------\n",
      "Training loss: 2.048726460606129....\n",
      "Validation loss: 2.054114204743249....\n",
      "-----------------------------------\n",
      "Training loss: 2.0484916006292018....\n",
      "Validation loss: 2.0540253206035355....\n",
      "-----------------------------------\n",
      "Training loss: 2.0482564360339572....\n",
      "Validation loss: 2.053935377190176....\n",
      "-----------------------------------\n",
      "Training loss: 2.0480213512347394....\n",
      "Validation loss: 2.0538458008182054....\n",
      "-----------------------------------\n",
      "Training loss: 2.0477861809697266....\n",
      "Validation loss: 2.0537555929144085....\n",
      "-----------------------------------\n",
      "Training loss: 2.0475507049754604....\n",
      "Validation loss: 2.0536659465823117....\n",
      "-----------------------------------\n",
      "Training loss: 2.0473151241215364....\n",
      "Validation loss: 2.05357695813609....\n",
      "-----------------------------------\n",
      "Training loss: 2.0470796332936025....\n",
      "Validation loss: 2.0534877282610213....\n",
      "-----------------------------------\n",
      "Training loss: 2.0468442735834036....\n",
      "Validation loss: 2.0533988265207292....\n",
      "-----------------------------------\n",
      "Training loss: 2.0466086275647153....\n",
      "Validation loss: 2.0533096860947015....\n",
      "-----------------------------------\n",
      "Training loss: 2.046373021892038....\n",
      "Validation loss: 2.0532205568011164....\n",
      "-----------------------------------\n",
      "Training loss: 2.046137622363211....\n",
      "Validation loss: 2.0531313551741754....\n",
      "-----------------------------------\n",
      "Training loss: 2.045902306975652....\n",
      "Validation loss: 2.053042218607085....\n",
      "-----------------------------------\n",
      "Training loss: 2.0456670610631424....\n",
      "Validation loss: 2.0529524153013754....\n",
      "-----------------------------------\n",
      "Training loss: 2.045431568813843....\n",
      "Validation loss: 2.0528634888764286....\n",
      "-----------------------------------\n",
      "Training loss: 2.045195710570073....\n",
      "Validation loss: 2.0527747209218825....\n",
      "-----------------------------------\n",
      "Training loss: 2.0449599209496587....\n",
      "Validation loss: 2.052685577052338....\n",
      "-----------------------------------\n",
      "Training loss: 2.0447241699511873....\n",
      "Validation loss: 2.052596463733193....\n",
      "-----------------------------------\n",
      "Training loss: 2.044488712674216....\n",
      "Validation loss: 2.052508201657567....\n",
      "-----------------------------------\n",
      "Training loss: 2.044253127403875....\n",
      "Validation loss: 2.052419522046115....\n",
      "-----------------------------------\n",
      "Training loss: 2.0440177847877807....\n",
      "Validation loss: 2.0523303636834385....\n",
      "-----------------------------------\n",
      "Training loss: 2.043782660696195....\n",
      "Validation loss: 2.052242148100072....\n",
      "-----------------------------------\n",
      "Training loss: 2.0435477090339966....\n",
      "Validation loss: 2.052153179880085....\n",
      "-----------------------------------\n",
      "Training loss: 2.043312629124236....\n",
      "Validation loss: 2.052064493674972....\n",
      "-----------------------------------\n",
      "Training loss: 2.043077846312932....\n",
      "Validation loss: 2.0519758296166932....\n",
      "-----------------------------------\n",
      "Training loss: 2.04284291090578....\n",
      "Validation loss: 2.051886763836678....\n",
      "-----------------------------------\n",
      "Training loss: 2.0426081617221725....\n",
      "Validation loss: 2.0517979236733166....\n",
      "-----------------------------------\n",
      "Training loss: 2.0423735274605175....\n",
      "Validation loss: 2.051709454824834....\n",
      "-----------------------------------\n",
      "Training loss: 2.042138779132818....\n",
      "Validation loss: 2.051620797375716....\n",
      "-----------------------------------\n",
      "Training loss: 2.04190413050061....\n",
      "Validation loss: 2.0515319487514865....\n",
      "-----------------------------------\n",
      "Training loss: 2.0416699453015728....\n",
      "Validation loss: 2.0514425756915884....\n",
      "-----------------------------------\n",
      "Training loss: 2.0414360955865645....\n",
      "Validation loss: 2.05135409554195....\n",
      "-----------------------------------\n",
      "Training loss: 2.0412021887524623....\n",
      "Validation loss: 2.0512662455304564....\n",
      "-----------------------------------\n",
      "Training loss: 2.040968053106171....\n",
      "Validation loss: 2.05117834450459....\n",
      "-----------------------------------\n",
      "Training loss: 2.0407340715044056....\n",
      "Validation loss: 2.051090833401439....\n",
      "-----------------------------------\n",
      "Training loss: 2.040500345732093....\n",
      "Validation loss: 2.051002848671153....\n",
      "-----------------------------------\n",
      "Training loss: 2.0402665728169747....\n",
      "Validation loss: 2.0509144620232638....\n",
      "-----------------------------------\n",
      "Training loss: 2.0400326321388813....\n",
      "Validation loss: 2.0508267131686435....\n",
      "-----------------------------------\n",
      "Training loss: 2.039798504524155....\n",
      "Validation loss: 2.0507382270211174....\n",
      "-----------------------------------\n",
      "Training loss: 2.0395645060100653....\n",
      "Validation loss: 2.0506502927567225....\n",
      "-----------------------------------\n",
      "Training loss: 2.0393304867029114....\n",
      "Validation loss: 2.0505617425297578....\n",
      "-----------------------------------\n",
      "Training loss: 2.039096603941236....\n",
      "Validation loss: 2.05047326752115....\n",
      "-----------------------------------\n",
      "Training loss: 2.038862845763657....\n",
      "Validation loss: 2.05038526035537....\n",
      "-----------------------------------\n",
      "Training loss: 2.038628941888532....\n",
      "Validation loss: 2.0502977754578975....\n",
      "-----------------------------------\n",
      "Training loss: 2.0383950074808803....\n",
      "Validation loss: 2.0502107784654546....\n",
      "-----------------------------------\n",
      "Training loss: 2.038160869131514....\n",
      "Validation loss: 2.0501227628387464....\n",
      "-----------------------------------\n",
      "Training loss: 2.0379268577114247....\n",
      "Validation loss: 2.0500353090174492....\n",
      "-----------------------------------\n",
      "Training loss: 2.037692887359106....\n",
      "Validation loss: 2.049947541170736....\n",
      "-----------------------------------\n",
      "Training loss: 2.0374593095300044....\n",
      "Validation loss: 2.049859495257565....\n",
      "-----------------------------------\n",
      "Training loss: 2.037225955644466....\n",
      "Validation loss: 2.0497718454823923....\n",
      "-----------------------------------\n",
      "Training loss: 2.03699277573575....\n",
      "Validation loss: 2.0496837880596717....\n",
      "-----------------------------------\n",
      "Training loss: 2.036759843439848....\n",
      "Validation loss: 2.0495961242638225....\n",
      "-----------------------------------\n",
      "Training loss: 2.0365271157912113....\n",
      "Validation loss: 2.0495073874531045....\n",
      "-----------------------------------\n",
      "Training loss: 2.036294565774948....\n",
      "Validation loss: 2.0494186421698357....\n",
      "-----------------------------------\n",
      "Training loss: 2.0360620162380365....\n",
      "Validation loss: 2.0493301647626914....\n",
      "-----------------------------------\n",
      "Training loss: 2.035829470006552....\n",
      "Validation loss: 2.049241051215307....\n",
      "-----------------------------------\n",
      "Training loss: 2.0355968337661183....\n",
      "Validation loss: 2.0491537883908006....\n",
      "-----------------------------------\n",
      "Training loss: 2.035363834071029....\n",
      "Validation loss: 2.04906571165214....\n",
      "-----------------------------------\n",
      "Training loss: 2.0351311197386512....\n",
      "Validation loss: 2.048978640566843....\n",
      "-----------------------------------\n",
      "Training loss: 2.0348985611116017....\n",
      "Validation loss: 2.048890291126867....\n",
      "-----------------------------------\n",
      "Training loss: 2.034666271437159....\n",
      "Validation loss: 2.0488021608823597....\n",
      "-----------------------------------\n",
      "Training loss: 2.0344340290892555....\n",
      "Validation loss: 2.0487141503292703....\n",
      "-----------------------------------\n",
      "Training loss: 2.034201761477187....\n",
      "Validation loss: 2.0486270438939336....\n",
      "-----------------------------------\n",
      "Training loss: 2.03396941104372....\n",
      "Validation loss: 2.0485395434651226....\n",
      "-----------------------------------\n",
      "Training loss: 2.0337371810990255....\n",
      "Validation loss: 2.048452270691224....\n",
      "-----------------------------------\n",
      "Training loss: 2.0335050436387228....\n",
      "Validation loss: 2.0483638391847623....\n",
      "-----------------------------------\n",
      "Training loss: 2.033272957088847....\n",
      "Validation loss: 2.0482765551464603....\n",
      "-----------------------------------\n",
      "Training loss: 2.033040940226724....\n",
      "Validation loss: 2.0481887722908154....\n",
      "-----------------------------------\n",
      "Training loss: 2.032808994407943....\n",
      "Validation loss: 2.048101158024611....\n",
      "-----------------------------------\n",
      "Training loss: 2.032577119373789....\n",
      "Validation loss: 2.0480132451280877....\n",
      "-----------------------------------\n",
      "Training loss: 2.0323453389759534....\n",
      "Validation loss: 2.0479262409902885....\n",
      "-----------------------------------\n",
      "Training loss: 2.032113702410968....\n",
      "Validation loss: 2.0478394106977573....\n",
      "-----------------------------------\n",
      "Training loss: 2.0318823975711537....\n",
      "Validation loss: 2.047753058399596....\n",
      "-----------------------------------\n",
      "Training loss: 2.0316512106214564....\n",
      "Validation loss: 2.0476656625305174....\n",
      "-----------------------------------\n",
      "Training loss: 2.0314200330775414....\n",
      "Validation loss: 2.04757897577178....\n",
      "-----------------------------------\n",
      "Training loss: 2.0311890695282204....\n",
      "Validation loss: 2.0474913866360303....\n",
      "-----------------------------------\n",
      "Training loss: 2.0309582922763627....\n",
      "Validation loss: 2.0474043943326494....\n",
      "-----------------------------------\n",
      "Training loss: 2.030727547513352....\n",
      "Validation loss: 2.04731728763327....\n",
      "-----------------------------------\n",
      "Training loss: 2.0304968150967784....\n",
      "Validation loss: 2.047230498595129....\n",
      "-----------------------------------\n",
      "Training loss: 2.0302662173872665....\n",
      "Validation loss: 2.0471425971361956....\n",
      "-----------------------------------\n",
      "Training loss: 2.0300358445173132....\n",
      "Validation loss: 2.0470558092888496....\n",
      "-----------------------------------\n",
      "Training loss: 2.029805724507826....\n",
      "Validation loss: 2.046967093853515....\n",
      "-----------------------------------\n",
      "Training loss: 2.0295757809221153....\n",
      "Validation loss: 2.046879020083502....\n",
      "-----------------------------------\n",
      "Training loss: 2.029345854543527....\n",
      "Validation loss: 2.046790769127198....\n",
      "-----------------------------------\n",
      "Training loss: 2.029115988847372....\n",
      "Validation loss: 2.046702987986017....\n",
      "-----------------------------------\n",
      "Training loss: 2.028886106846744....\n",
      "Validation loss: 2.0466148361235548....\n",
      "-----------------------------------\n",
      "Training loss: 2.0286561071995317....\n",
      "Validation loss: 2.046526810985779....\n",
      "-----------------------------------\n",
      "Training loss: 2.0284260063816015....\n",
      "Validation loss: 2.0464393813891895....\n",
      "-----------------------------------\n",
      "Training loss: 2.0281957674146533....\n",
      "Validation loss: 2.046351953025338....\n",
      "-----------------------------------\n",
      "Training loss: 2.0279655695001995....\n",
      "Validation loss: 2.046265560065294....\n",
      "-----------------------------------\n",
      "Training loss: 2.0277355544781375....\n",
      "Validation loss: 2.0461789756627713....\n",
      "-----------------------------------\n",
      "Training loss: 2.0275053379181376....\n",
      "Validation loss: 2.046092290565856....\n",
      "-----------------------------------\n",
      "Training loss: 2.027275225890961....\n",
      "Validation loss: 2.0460056772189614....\n",
      "-----------------------------------\n",
      "Training loss: 2.0270451114797527....\n",
      "Validation loss: 2.045920103135873....\n",
      "-----------------------------------\n",
      "Training loss: 2.026815144852812....\n",
      "Validation loss: 2.0458338508405056....\n",
      "-----------------------------------\n",
      "Training loss: 2.0265852560014608....\n",
      "Validation loss: 2.0457470018657857....\n",
      "-----------------------------------\n",
      "Training loss: 2.0263555354668723....\n",
      "Validation loss: 2.045661205655192....\n",
      "-----------------------------------\n",
      "Training loss: 2.026125936950914....\n",
      "Validation loss: 2.045574766738565....\n",
      "-----------------------------------\n",
      "Training loss: 2.0258966418533713....\n",
      "Validation loss: 2.045487863941571....\n",
      "-----------------------------------\n",
      "Training loss: 2.025667739918286....\n",
      "Validation loss: 2.0454012634636003....\n",
      "-----------------------------------\n",
      "Training loss: 2.025439169382415....\n",
      "Validation loss: 2.0453156670136474....\n",
      "-----------------------------------\n",
      "Training loss: 2.0252107147896363....\n",
      "Validation loss: 2.0452287556314386....\n",
      "-----------------------------------\n",
      "Training loss: 2.02498228266078....\n",
      "Validation loss: 2.045142631527251....\n",
      "-----------------------------------\n",
      "Training loss: 2.024753895186249....\n",
      "Validation loss: 2.0450560733980123....\n",
      "-----------------------------------\n",
      "Training loss: 2.0245256239980804....\n",
      "Validation loss: 2.044969669752367....\n",
      "-----------------------------------\n",
      "Training loss: 2.0242977272280824....\n",
      "Validation loss: 2.044881975594171....\n",
      "-----------------------------------\n",
      "Training loss: 2.024070062151493....\n",
      "Validation loss: 2.044795109789055....\n",
      "-----------------------------------\n",
      "Training loss: 2.0238423992515013....\n",
      "Validation loss: 2.0447075230364034....\n",
      "-----------------------------------\n",
      "Training loss: 2.0236146040987166....\n",
      "Validation loss: 2.0446208444327008....\n",
      "-----------------------------------\n",
      "Training loss: 2.0233867739149343....\n",
      "Validation loss: 2.044534208591777....\n",
      "-----------------------------------\n",
      "Training loss: 2.023158771636422....\n",
      "Validation loss: 2.0444481343086576....\n",
      "-----------------------------------\n",
      "Training loss: 2.0229308123538416....\n",
      "Validation loss: 2.0443619011026977....\n",
      "-----------------------------------\n",
      "Training loss: 2.0227026864369493....\n",
      "Validation loss: 2.044275848213364....\n",
      "-----------------------------------\n",
      "Training loss: 2.022474648536381....\n",
      "Validation loss: 2.044189293949843....\n",
      "-----------------------------------\n",
      "Training loss: 2.022246641641874....\n",
      "Validation loss: 2.044102617243274....\n",
      "-----------------------------------\n",
      "Training loss: 2.022018602028899....\n",
      "Validation loss: 2.0440173037383103....\n",
      "-----------------------------------\n",
      "Training loss: 2.021790486559622....\n",
      "Validation loss: 2.0439308021684313....\n",
      "-----------------------------------\n",
      "Training loss: 2.0215624044336793....\n",
      "Validation loss: 2.043845341417799....\n",
      "-----------------------------------\n",
      "Training loss: 2.021334608366322....\n",
      "Validation loss: 2.0437587881651313....\n",
      "-----------------------------------\n",
      "Training loss: 2.0211068738405817....\n",
      "Validation loss: 2.043673460708165....\n",
      "-----------------------------------\n",
      "Training loss: 2.0208790013207625....\n",
      "Validation loss: 2.043587340827979....\n",
      "-----------------------------------\n",
      "Training loss: 2.020651361647387....\n",
      "Validation loss: 2.0435020062032168....\n",
      "-----------------------------------\n",
      "Training loss: 2.0204236296570905....\n",
      "Validation loss: 2.0434160867256153....\n",
      "-----------------------------------\n",
      "Training loss: 2.0201958544135206....\n",
      "Validation loss: 2.0433315859722874....\n",
      "-----------------------------------\n",
      "Training loss: 2.0199679315412267....\n",
      "Validation loss: 2.0432464547110563....\n",
      "-----------------------------------\n",
      "Training loss: 2.019740171685765....\n",
      "Validation loss: 2.0431618158843827....\n",
      "-----------------------------------\n",
      "Training loss: 2.019512233748113....\n",
      "Validation loss: 2.043076663849496....\n",
      "-----------------------------------\n",
      "Training loss: 2.0192844008574946....\n",
      "Validation loss: 2.0429915009540096....\n",
      "-----------------------------------\n",
      "Training loss: 2.019056722872992....\n",
      "Validation loss: 2.042906164968454....\n",
      "-----------------------------------\n",
      "Training loss: 2.01882861617052....\n",
      "Validation loss: 2.0428213186375404....\n",
      "-----------------------------------\n",
      "Training loss: 2.018600485850449....\n",
      "Validation loss: 2.0427356248389548....\n",
      "-----------------------------------\n",
      "Training loss: 2.018372637625128....\n",
      "Validation loss: 2.0426501543955218....\n",
      "-----------------------------------\n",
      "Training loss: 2.0181449634145308....\n",
      "Validation loss: 2.0425652283243108....\n",
      "-----------------------------------\n",
      "Training loss: 2.0179173881455883....\n",
      "Validation loss: 2.042479221048207....\n",
      "-----------------------------------\n",
      "Training loss: 2.0176899156571992....\n",
      "Validation loss: 2.042393702633997....\n",
      "-----------------------------------\n",
      "Training loss: 2.0174625523989262....\n",
      "Validation loss: 2.0423074598559996....\n",
      "-----------------------------------\n",
      "Training loss: 2.0172353412762476....\n",
      "Validation loss: 2.0422217307772677....\n",
      "-----------------------------------\n",
      "Training loss: 2.0170082897057147....\n",
      "Validation loss: 2.042135995172048....\n",
      "-----------------------------------\n",
      "Training loss: 2.016781360853636....\n",
      "Validation loss: 2.0420498705180723....\n",
      "-----------------------------------\n",
      "Training loss: 2.01655435698461....\n",
      "Validation loss: 2.0419641171713305....\n",
      "-----------------------------------\n",
      "Training loss: 2.0163274998513567....\n",
      "Validation loss: 2.0418782045009785....\n",
      "-----------------------------------\n",
      "Training loss: 2.0161008134073284....\n",
      "Validation loss: 2.0417921894023494....\n",
      "-----------------------------------\n",
      "Training loss: 2.0158743486340347....\n",
      "Validation loss: 2.041706729339892....\n",
      "-----------------------------------\n",
      "Training loss: 2.0156477186461363....\n",
      "Validation loss: 2.0416202445525258....\n",
      "-----------------------------------\n",
      "Training loss: 2.015421128832306....\n",
      "Validation loss: 2.04153480916693....\n",
      "-----------------------------------\n",
      "Training loss: 2.0151943851190137....\n",
      "Validation loss: 2.0414498333416797....\n",
      "-----------------------------------\n",
      "Training loss: 2.0149676057624877....\n",
      "Validation loss: 2.0413643928607588....\n",
      "-----------------------------------\n",
      "Training loss: 2.0147409424645226....\n",
      "Validation loss: 2.041278733254681....\n",
      "-----------------------------------\n",
      "Training loss: 2.0145144166096594....\n",
      "Validation loss: 2.0411928428136923....\n",
      "-----------------------------------\n",
      "Training loss: 2.0142877890724553....\n",
      "Validation loss: 2.0411076559045416....\n",
      "-----------------------------------\n",
      "Training loss: 2.0140611572705565....\n",
      "Validation loss: 2.041021566751555....\n",
      "-----------------------------------\n",
      "Training loss: 2.0138345536402635....\n",
      "Validation loss: 2.040936229906171....\n",
      "-----------------------------------\n",
      "Training loss: 2.01360819067037....\n",
      "Validation loss: 2.04085054452226....\n",
      "-----------------------------------\n",
      "Training loss: 2.013381942029207....\n",
      "Validation loss: 2.0407645099434126....\n",
      "-----------------------------------\n",
      "Training loss: 2.013155921140278....\n",
      "Validation loss: 2.0406784683992703....\n",
      "-----------------------------------\n",
      "Training loss: 2.0129301129975365....\n",
      "Validation loss: 2.0405918911286407....\n",
      "-----------------------------------\n",
      "Training loss: 2.012704439805432....\n",
      "Validation loss: 2.040506262227638....\n",
      "-----------------------------------\n",
      "Training loss: 2.0124788569854486....\n",
      "Validation loss: 2.0404205971889997....\n",
      "-----------------------------------\n",
      "Training loss: 2.0122533606599307....\n",
      "Validation loss: 2.040334343838216....\n",
      "-----------------------------------\n",
      "Training loss: 2.012027841539793....\n",
      "Validation loss: 2.0402496974734046....\n",
      "-----------------------------------\n",
      "Training loss: 2.011802343343043....\n",
      "Validation loss: 2.0401636901753437....\n",
      "-----------------------------------\n",
      "Training loss: 2.0115768881798437....\n",
      "Validation loss: 2.04007952351659....\n",
      "-----------------------------------\n",
      "Training loss: 2.01135128174147....\n",
      "Validation loss: 2.03999469970037....\n",
      "-----------------------------------\n",
      "Training loss: 2.0111256389116785....\n",
      "Validation loss: 2.039909833420516....\n",
      "-----------------------------------\n",
      "Training loss: 2.0109000031178206....\n",
      "Validation loss: 2.03982562775936....\n",
      "-----------------------------------\n",
      "Training loss: 2.0106742457969773....\n",
      "Validation loss: 2.039740914101097....\n",
      "-----------------------------------\n",
      "Training loss: 2.010448067828349....\n",
      "Validation loss: 2.039657293667373....\n",
      "-----------------------------------\n",
      "Training loss: 2.0102216818994427....\n",
      "Validation loss: 2.039572879353901....\n",
      "-----------------------------------\n",
      "Training loss: 2.0099953344457013....\n",
      "Validation loss: 2.039489687442132....\n",
      "-----------------------------------\n",
      "Training loss: 2.0097689806420003....\n",
      "Validation loss: 2.039405838808007....\n",
      "-----------------------------------\n",
      "Training loss: 2.009542503934063....\n",
      "Validation loss: 2.0393223273742076....\n",
      "-----------------------------------\n",
      "Training loss: 2.0093161140154536....\n",
      "Validation loss: 2.0392390560202074....\n",
      "-----------------------------------\n",
      "Training loss: 2.009089824471015....\n",
      "Validation loss: 2.039156212816188....\n",
      "-----------------------------------\n",
      "Training loss: 2.008863465143197....\n",
      "Validation loss: 2.03907275046444....\n",
      "-----------------------------------\n",
      "Training loss: 2.008637227115888....\n",
      "Validation loss: 2.0389895320083653....\n",
      "-----------------------------------\n",
      "Training loss: 2.008410927347005....\n",
      "Validation loss: 2.0389071789304962....\n",
      "-----------------------------------\n",
      "Training loss: 2.0081848653213585....\n",
      "Validation loss: 2.038824474837413....\n",
      "-----------------------------------\n",
      "Training loss: 2.0079587022606344....\n",
      "Validation loss: 2.0387426378783706....\n",
      "-----------------------------------\n",
      "Training loss: 2.0077324306639364....\n",
      "Validation loss: 2.0386612459562556....\n",
      "-----------------------------------\n",
      "Training loss: 2.007506092667485....\n",
      "Validation loss: 2.0385794907567867....\n",
      "-----------------------------------\n",
      "Training loss: 2.007279825717652....\n",
      "Validation loss: 2.038498307016281....\n",
      "-----------------------------------\n",
      "Training loss: 2.0070535003171837....\n",
      "Validation loss: 2.0384166499274534....\n",
      "-----------------------------------\n",
      "Training loss: 2.0068272951114383....\n",
      "Validation loss: 2.0383353675606837....\n",
      "-----------------------------------\n",
      "Training loss: 2.0066011030245914....\n",
      "Validation loss: 2.0382543221847453....\n",
      "-----------------------------------\n",
      "Training loss: 2.0063754219279244....\n",
      "Validation loss: 2.038172625948882....\n",
      "-----------------------------------\n",
      "Training loss: 2.0061501159099664....\n",
      "Validation loss: 2.038089786513157....\n",
      "-----------------------------------\n",
      "Training loss: 2.0059249179383656....\n",
      "Validation loss: 2.0380077624374358....\n",
      "-----------------------------------\n",
      "Training loss: 2.0056996567855068....\n",
      "Validation loss: 2.0379260511195634....\n",
      "-----------------------------------\n",
      "Training loss: 2.005474537548749....\n",
      "Validation loss: 2.0378435755864364....\n",
      "-----------------------------------\n",
      "Training loss: 2.0052496227763386....\n",
      "Validation loss: 2.037760884723228....\n",
      "-----------------------------------\n",
      "Training loss: 2.0050249035346046....\n",
      "Validation loss: 2.0376781558189294....\n",
      "-----------------------------------\n",
      "Training loss: 2.004800134290575....\n",
      "Validation loss: 2.0375948729686852....\n",
      "-----------------------------------\n",
      "Training loss: 2.00457530496517....\n",
      "Validation loss: 2.0375118348052434....\n",
      "-----------------------------------\n",
      "Training loss: 2.0043503964390488....\n",
      "Validation loss: 2.037429905748541....\n",
      "-----------------------------------\n",
      "Training loss: 2.0041253866289392....\n",
      "Validation loss: 2.0373470492886585....\n",
      "-----------------------------------\n",
      "Training loss: 2.0039006119730978....\n",
      "Validation loss: 2.0372647309865872....\n",
      "-----------------------------------\n",
      "Training loss: 2.0036760334109798....\n",
      "Validation loss: 2.037181943907688....\n",
      "-----------------------------------\n",
      "Training loss: 2.003451470991814....\n",
      "Validation loss: 2.037100161534949....\n",
      "-----------------------------------\n",
      "Training loss: 2.0032269589644947....\n",
      "Validation loss: 2.037017605966715....\n",
      "-----------------------------------\n",
      "Training loss: 2.0030026707495114....\n",
      "Validation loss: 2.0369356928387745....\n",
      "-----------------------------------\n",
      "Training loss: 2.0027782679746267....\n",
      "Validation loss: 2.036853201636766....\n",
      "-----------------------------------\n",
      "Training loss: 2.002553954074581....\n",
      "Validation loss: 2.036770278142117....\n",
      "-----------------------------------\n",
      "Training loss: 2.00232958523785....\n",
      "Validation loss: 2.0366879374202593....\n",
      "-----------------------------------\n",
      "Training loss: 2.002105284606821....\n",
      "Validation loss: 2.036605527662466....\n",
      "-----------------------------------\n",
      "Training loss: 2.001880879140882....\n",
      "Validation loss: 2.0365235322003064....\n",
      "-----------------------------------\n",
      "Training loss: 2.0016562206261255....\n",
      "Validation loss: 2.036441708600948....\n",
      "-----------------------------------\n",
      "Training loss: 2.0014312568897883....\n",
      "Validation loss: 2.0363605830507465....\n",
      "-----------------------------------\n",
      "Training loss: 2.0012063988540554....\n",
      "Validation loss: 2.036278785331663....\n",
      "-----------------------------------\n",
      "Training loss: 2.000981480965573....\n",
      "Validation loss: 2.0361978847078084....\n",
      "-----------------------------------\n",
      "Training loss: 2.000756429902444....\n",
      "Validation loss: 2.036116110478115....\n",
      "-----------------------------------\n",
      "Training loss: 2.0005310861376895....\n",
      "Validation loss: 2.0360358932704408....\n",
      "-----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29       499\n",
      "           2       0.17      0.13      0.15       500\n",
      "           3       0.19      0.20      0.19       500\n",
      "           4       0.22      0.25      0.24       500\n",
      "           5       0.13      0.10      0.12       500\n",
      "           6       0.25      0.26      0.26       500\n",
      "           7       0.20      0.22      0.21       500\n",
      "           8       0.15      0.11      0.13       500\n",
      "           9       0.20      0.19      0.19       500\n",
      "          10       0.27      0.32      0.29       500\n",
      "\n",
      "    accuracy                           0.21      4999\n",
      "   macro avg       0.20      0.21      0.21      4999\n",
      "weighted avg       0.20      0.21      0.21      4999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distortion 0.9\n",
    "xtr3= distortion(0.9,X_train_arr,X_train_arr.shape[0])\n",
    "xtest3=distortion(0.9,X_test_arr,X_test_arr.shape[0])\n",
    "C3 = NN(0.01)\n",
    "C3.add_layer(50,256,'relu')\n",
    "C3.add_layer(10,50,'softmax')\n",
    "t_loss,v_loss= train_and_validate(C3,xtr3,Y_train_arr,n=3)\n",
    "report3 = test(C3,xtest3)\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8a604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
